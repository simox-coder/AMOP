{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":180858,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":154124,"modelId":176602}],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"378f5acf-1baa-4f95-92cb-4bf054d59de9","cell_type":"markdown","source":"# AIMO3 Baseline Notebook\n## AI Mathematical Olympiad – Progress Prize 3\n\n### Quick Start Guide\n1. **RUN_MODE Selection**:\n   - `\"local_ref\"`: Debug mode - runs evaluation on `reference.csv` (10 problems)\n   - `\"submit_auto\"`: Kaggle submission mode - uses `kaggle_evaluation` API\n\n2. **Model Setup (Kaggle)**:\n   - Add your model as a Kaggle Dataset/Model input\n   - Update `MODEL_PATH` in CONFIG to point to `/kaggle/input/your-model-name`\n   - Or use Kaggle's built-in models\n\n3. **Telemetry**:\n   - Logs saved to `/kaggle/working/aimo3_telemetry.jsonl`\n   - Download after submission for analysis","metadata":{}},{"id":"273e591d-bc38-426f-907b-1a1859c8c58e","cell_type":"markdown","source":"## CELL A — CONFIG (Constants)","metadata":{}},{"id":"d9d33871-7a04-400b-84b6-f8d672c03c48","cell_type":"code","source":"# ================================\n# CELL A — CONFIG (Constants)\n# ================================\n\nimport os\n\n# ----- RUN MODE AUTO DETECTION -----\n# Automatically set to \"submit_auto\" if KAGGLE_IS_COMPETITION_RERUN is set\n_is_kaggle_rerun = os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\") == \"1\"\nif _is_kaggle_rerun:\n    RUN_MODE = \"submit_auto\"\nelse:\n    # \"local_ref\": Run evaluation on reference.csv (10 problems) - for debugging\n    # \"submit_auto\": Kaggle submission mode - uses kaggle_evaluation API\n    RUN_MODE = \"local_ref\"  # Change to \"submit_auto\" for manual Kaggle submission\n\n# ----- TIME BUDGET -----\nTIME_BUDGET_SEC_PER_PROBLEM = 120  # seconds per problem\n\n# ----- GENERATION PARAMS -----\nK_BASE = 4              # Base number of candidates for easy problems\nK_MAX_HARD = 8          # Max candidates for hard problems\nTEMPERATURE_BASE = 0.3  # Temperature for stable generation\nTEMPERATURE_HARD = 0.7  # Temperature for exploration\nMAX_NEW_TOKENS = 2048   # Max tokens per generation\n\n# ----- VOTING & EARLY STOP -----\nVOTING_THRESHOLD = 0.6  # Stop early if top answer >= this fraction\n\n# ----- PATHS -----\nLOG_PATH = \"/kaggle/working/aimo3_telemetry.jsonl\"\nCACHE_DIR = \"/kaggle/working/cache\"\n\n# ----- MODEL CONFIG -----\nMODEL_PATH = None  # Set to model path when available (e.g., \"/kaggle/input/your-model\")\nMODEL_ID = \"/kaggle/input/qwq-32b-preview/transformers/default/1\"  # Model ID (only used locally with internet)\n\n# ----- PROMPT STYLE -----\nPROMPT_STYLE = \"tir\"  # \"tir\", \"concise\", or \"explore\"\n\n# ----- MODE POLICY -----\n# \"stable\": Fixed seed (BASE_SEED)\n# \"diverse\": Deterministic seed based on problem_id hash (for rerun diversity)\nMODE_POLICY = \"stable\"  # \"stable\" or \"diverse\"\n\n# ----- DATA PATHS -----\nREFERENCE_CSV_PATH = None\nTEST_CSV_PATH = None\n\n# Detect environment and set paths\nif os.path.exists(\"/kaggle/input\"):\n    REFERENCE_CSV_PATH = \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\"\n    TEST_CSV_PATH = \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv\"\n    if not os.path.exists(REFERENCE_CSV_PATH):\n        REFERENCE_CSV_PATH = \"reference.csv\"\n        TEST_CSV_PATH = \"test.csv\"\nelse:\n    REFERENCE_CSV_PATH = \"reference.csv\"\n    TEST_CSV_PATH = \"test.csv\"\n\n# Create working directories\nos.makedirs(os.path.dirname(LOG_PATH), exist_ok=True) if os.path.dirname(LOG_PATH) else None\nos.makedirs(CACHE_DIR, exist_ok=True) if CACHE_DIR else None\n\nprint(f\"RUN_MODE: {RUN_MODE} (auto-detected: {_is_kaggle_rerun})\")\nprint(f\"REFERENCE_CSV_PATH: {REFERENCE_CSV_PATH}\")\nprint(f\"LOG_PATH: {LOG_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:16:41.372901Z","iopub.execute_input":"2025-12-29T10:16:41.373483Z","iopub.status.idle":"2025-12-29T10:16:41.379484Z","shell.execute_reply.started":"2025-12-29T10:16:41.373465Z","shell.execute_reply":"2025-12-29T10:16:41.379065Z"}},"outputs":[{"name":"stdout","text":"RUN_MODE: local_ref (auto-detected: False)\nREFERENCE_CSV_PATH: /kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\nLOG_PATH: /kaggle/working/aimo3_telemetry.jsonl\n","output_type":"stream"}],"execution_count":2},{"id":"ef35a8ff-1012-4df1-ba42-42c738f68634","cell_type":"markdown","source":"## CELL B — IMPORTS + SEED CONTROL","metadata":{}},{"id":"a7d32b6b-3173-424a-bfe9-464f537cc619","cell_type":"code","source":"# ================================\n# CELL B — IMPORTS + SEED CONTROL\n# ================================\n\nimport os\nimport re\nimport sys\nimport time\nimport json\nimport math\nimport random\nimport hashlib\nimport warnings\nfrom typing import Optional, Tuple, List, Dict, Any, Union\nfrom collections import Counter\nfrom contextlib import redirect_stdout, redirect_stderr\nimport io\n\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\n# ----- SEED CONTROL -----\nBASE_SEED = 42\nIS_KAGGLE_RERUN = os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\") == \"1\"\n\ndef get_seed_for_problem(problem_id: str = None) -> int:\n    \"\"\"\n    Get seed based on mode policy.\n    - stable: Always return BASE_SEED (deterministic across runs)\n    - diverse: Return deterministic seed based on problem_id hash (for rerun diversity)\n    \"\"\"\n    if MODE_POLICY == \"diverse\" and problem_id is not None:\n        # Deterministic hash-based seed for diversity\n        hash_val = int(hashlib.md5(problem_id.encode()).hexdigest()[:8], 16)\n        return BASE_SEED + (hash_val % 10000)\n    else:\n        return BASE_SEED\n\ndef set_seed(seed: int):\n    \"\"\"Set random seeds for reproducibility.\"\"\"\n    random.seed(seed)\n    try:\n        import numpy as np\n        np.random.seed(seed)\n    except ImportError:\n        pass\n    try:\n        import torch\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(seed)\n    except ImportError:\n        pass\n\n# Initialize with base seed (will be updated per-problem if diverse mode)\nCURRENT_SEED = BASE_SEED\nset_seed(CURRENT_SEED)\n\nprint(f\"IS_KAGGLE_RERUN: {IS_KAGGLE_RERUN}\")\nprint(f\"MODE_POLICY: {MODE_POLICY}\")\nprint(f\"BASE_SEED: {BASE_SEED}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:16:41.380221Z","iopub.execute_input":"2025-12-29T10:16:41.380362Z","iopub.status.idle":"2025-12-29T10:16:44.859863Z","shell.execute_reply.started":"2025-12-29T10:16:41.380352Z","shell.execute_reply":"2025-12-29T10:16:44.859404Z"}},"outputs":[{"name":"stdout","text":"IS_KAGGLE_RERUN: False\nMODE_POLICY: stable\nBASE_SEED: 42\n","output_type":"stream"}],"execution_count":3},{"id":"deb0c27e-8bbd-4d49-af51-0582e50b5019","cell_type":"markdown","source":"## CELL C — LAZY MODEL LOADER","metadata":{}},{"id":"67a5ba31-e55b-42f7-b97a-ffc9f9edc471","cell_type":"code","source":"# ================================\n# CELL C — LAZY MODEL LOADER\n# ================================\n\n_model_cache = {\"model\": None, \"tokenizer\": None, \"device\": None, \"loaded\": False, \"skip_reason\": None}\n\ndef get_device():\n    try:\n        import torch\n        if torch.cuda.is_available():\n            return \"cuda\"\n        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n            return \"mps\"\n    except ImportError:\n        pass\n    return \"cpu\"\n\ndef load_model():\n    \"\"\"\n    Lazy load model and tokenizer.\n    In Kaggle rerun: always use local_files_only=True.\n    If MODEL_PATH is None/missing: skip load immediately and log warning.\n    \"\"\"\n    global _model_cache\n    \n    if _model_cache[\"loaded\"]:\n        return _model_cache[\"model\"], _model_cache[\"tokenizer\"], _model_cache[\"device\"]\n    \n    # Check if we should skip loading\n    has_local_model = MODEL_PATH is not None and os.path.exists(MODEL_PATH)\n    \n    if IS_KAGGLE_RERUN and not has_local_model:\n        # In rerun mode without local model, skip loading\n        print(\"WARNING: No local model available (MODEL_PATH is None or missing)\")\n        print(\"Skipping model load - using fallback solver\")\n        _model_cache[\"model\"] = None\n        _model_cache[\"tokenizer\"] = None\n        _model_cache[\"device\"] = \"cpu\"\n        _model_cache[\"loaded\"] = True\n        _model_cache[\"skip_reason\"] = \"no_local_model_in_rerun\"\n        return None, None, \"cpu\"\n    \n    print(\"Loading model...\")\n    start_time = time.time()\n    \n    try:\n        import torch\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n        \n        device = get_device()\n        _model_cache[\"device\"] = device\n        print(f\"Using device: {device}\")\n        \n        # Determine model source and local_files_only setting\n        if has_local_model:\n            model_source = MODEL_PATH\n            local_only = True\n            print(f\"Loading from local path: {model_source}\")\n        elif IS_KAGGLE_RERUN:\n            # Should not reach here due to check above, but safety fallback\n            print(\"WARNING: Cannot load model - no local path and in rerun mode\")\n            raise RuntimeError(\"No local model available in rerun mode\")\n        else:\n            # Local development with internet - can download\n            model_source = MODEL_ID\n            local_only = False\n            print(f\"Loading from HuggingFace: {model_source}\")\n        \n        _model_cache[\"tokenizer\"] = AutoTokenizer.from_pretrained(\n            model_source, trust_remote_code=True, local_files_only=local_only\n        )\n        \n        dtype = torch.float16 if device == \"cuda\" else torch.float32\n        _model_cache[\"model\"] = AutoModelForCausalLM.from_pretrained(\n            model_source, torch_dtype=dtype,\n            device_map=\"auto\" if device == \"cuda\" else None,\n            trust_remote_code=True, local_files_only=local_only\n        )\n        \n        if device != \"cuda\":\n            _model_cache[\"model\"] = _model_cache[\"model\"].to(device)\n        \n        _model_cache[\"loaded\"] = True\n        print(f\"Model loaded in {time.time() - start_time:.2f}s\")\n        \n    except Exception as e:\n        print(f\"WARNING: Could not load model: {e}\")\n        print(\"Using rule-based solver...\")\n        _model_cache[\"model\"] = None\n        _model_cache[\"tokenizer\"] = None\n        _model_cache[\"device\"] = \"cpu\"\n        _model_cache[\"loaded\"] = True\n        _model_cache[\"skip_reason\"] = str(e)\n    \n    return _model_cache[\"model\"], _model_cache[\"tokenizer\"], _model_cache[\"device\"]\n\ndef is_model_available():\n    if _model_cache[\"loaded\"]:\n        return _model_cache[\"model\"] is not None\n    if MODEL_PATH and os.path.exists(MODEL_PATH):\n        return True\n    return False\n\nprint(\"Model loader initialized (lazy loading)\")\nprint(f\"MODEL_PATH: {MODEL_PATH}\")\nprint(f\"IS_KAGGLE_RERUN: {IS_KAGGLE_RERUN}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:16:44.860416Z","iopub.execute_input":"2025-12-29T10:16:44.860641Z","iopub.status.idle":"2025-12-29T10:16:44.868334Z","shell.execute_reply.started":"2025-12-29T10:16:44.860627Z","shell.execute_reply":"2025-12-29T10:16:44.867906Z"}},"outputs":[{"name":"stdout","text":"Model loader initialized (lazy loading)\nMODEL_PATH: None\nIS_KAGGLE_RERUN: False\n","output_type":"stream"}],"execution_count":4},{"id":"856da62c-c8ea-4241-9d6e-e79d189d4dbd","cell_type":"markdown","source":"## CELL D — TOOL-INTEGRATED REASONING + SAFE PYTHON EXECUTOR","metadata":{}},{"id":"f02d7eeb-0d36-4703-a6b2-710acce757b9","cell_type":"code","source":"# ================================\n# CELL D — TIR-lite + SAFE PYTHON EXECUTOR\n# ================================\n\nALLOWED_MODULES = {\"math\", \"fractions\", \"itertools\", \"functools\", \"collections\", \"decimal\", \"numbers\", \"cmath\", \"random\", \"statistics\"}\n\ntry:\n    import sympy\n    ALLOWED_MODULES.add(\"sympy\")\nexcept ImportError:\n    pass\n\ndef create_safe_globals():\n    \"\"\"Create a safe globals dict with pre-loaded allowed modules.\"\"\"\n    import math, fractions, itertools, functools, collections, decimal, random, statistics\n    \n    safe_globals = {\n        \"__builtins__\": {\n            \"abs\": abs, \"all\": all, \"any\": any, \"bin\": bin, \"bool\": bool, \"chr\": chr,\n            \"dict\": dict, \"divmod\": divmod, \"enumerate\": enumerate, \"filter\": filter,\n            \"float\": float, \"frozenset\": frozenset, \"hex\": hex, \"int\": int,\n            \"isinstance\": isinstance, \"len\": len, \"list\": list, \"map\": map, \"max\": max,\n            \"min\": min, \"oct\": oct, \"ord\": ord, \"pow\": pow, \"print\": print, \"range\": range,\n            \"repr\": repr, \"reversed\": reversed, \"round\": round, \"set\": set, \"slice\": slice,\n            \"sorted\": sorted, \"str\": str, \"sum\": sum, \"tuple\": tuple, \"type\": type, \"zip\": zip,\n            \"True\": True, \"False\": False, \"None\": None, \"complex\": complex,\n        },\n        # Pre-loaded modules (no import needed)\n        \"math\": math,\n        \"fractions\": fractions,\n        \"Fraction\": fractions.Fraction,\n        \"itertools\": itertools,\n        \"functools\": functools,\n        \"collections\": collections,\n        \"decimal\": decimal,\n        \"Decimal\": decimal.Decimal,\n        \"random\": random,\n        \"statistics\": statistics,\n    }\n    \n    try:\n        import sympy\n        safe_globals[\"sympy\"] = sympy\n    except ImportError:\n        pass\n    \n    return safe_globals\n\ndef strip_imports(code: str) -> str:\n    \"\"\"Remove import statements since modules are pre-loaded.\"\"\"\n    lines = code.split('\\n')\n    filtered = []\n    for line in lines:\n        stripped = line.strip()\n        # Skip import lines for allowed modules\n        if stripped.startswith('import ') or stripped.startswith('from '):\n            # Check if it's importing an allowed module\n            skip = False\n            for mod in ALLOWED_MODULES:\n                if mod in stripped:\n                    skip = True\n                    break\n            if skip:\n                continue\n        filtered.append(line)\n    return '\\n'.join(filtered)\n\ndef run_python(code: str, timeout_sec: float = 10.0) -> Tuple[bool, str]:\n    \"\"\"Execute Python code in a sandboxed environment.\"\"\"\n    import signal\n    \n    # Strip import statements for pre-loaded modules\n    code = strip_imports(code)\n    \n    output_capture = io.StringIO()\n    safe_globals = create_safe_globals()\n    safe_locals = {}\n    \n    def timeout_handler(signum, frame):\n        raise TimeoutError(\"Code execution timed out\")\n    \n    old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n    signal.alarm(int(timeout_sec))\n    \n    try:\n        with redirect_stdout(output_capture), redirect_stderr(output_capture):\n            exec(code, safe_globals, safe_locals)\n        \n        output = output_capture.getvalue()\n        \n        # Capture result variables\n        for var_name in [\"result\", \"answer\", \"ans\", \"final\", \"output\"]:\n            if var_name in safe_locals:\n                val = safe_locals[var_name]\n                if output:\n                    output += f\"\\n{var_name} = {val}\"\n                else:\n                    output = f\"{var_name} = {val}\"\n                break\n        \n        signal.alarm(0)\n        signal.signal(signal.SIGALRM, old_handler)\n        return True, output if output else \"Execution completed (no output)\"\n        \n    except TimeoutError as e:\n        signal.alarm(0)\n        signal.signal(signal.SIGALRM, old_handler)\n        return False, f\"Timeout: {str(e)}\"\n    except Exception as e:\n        signal.alarm(0)\n        signal.signal(signal.SIGALRM, old_handler)\n        return False, f\"Error: {type(e).__name__}: {str(e)}\"\n\ndef parse_python_block(text: str) -> Optional[str]:\n    \"\"\"Extract Python code block from text.\"\"\"\n    patterns = [r\"```python\\s*\\n(.*?)```\", r\"```py\\s*\\n(.*?)```\", r\"```\\s*\\n(.*?)```\"]\n    for pattern in patterns:\n        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n        if match:\n            return match.group(1).strip()\n    return None\n\ndef get_tir_prompt(problem: str) -> str:\n    \"\"\"Generate Tool-Integrated Reasoning prompt.\"\"\"\n    return f\"\"\"You are a mathematical problem solver. Solve the following problem step by step.\n\nRULES:\n1. Think through the problem carefully.\n2. If you need to compute something, write Python code in a ```python ... ``` block.\n3. After your reasoning, provide your final answer on a new line as: ANSWER: <integer>\n4. The answer must be an integer between 0 and 99999.\n5. If the problem asks for a remainder when divided by some number, compute that remainder.\n\nPROBLEM:\n{problem}\n\nSOLUTION:\"\"\"\n\ndef get_concise_prompt(problem: str) -> str:\n    \"\"\"Generate concise direct-answer prompt.\"\"\"\n    return f\"\"\"Solve this math problem. Give only the final integer answer (0-99999).\n\nProblem: {problem}\n\nANSWER:\"\"\"\n\ndef get_explore_prompt(problem: str) -> str:\n    \"\"\"Generate exploration prompt with more reasoning.\"\"\"\n    return f\"\"\"You are an expert mathematician. Carefully analyze this problem and explore multiple approaches.\n\nProblem:\n{problem}\n\nInstructions:\n1. Identify the key mathematical concepts involved.\n2. Consider multiple solution approaches.\n3. Use Python code (```python ... ```) for complex calculations.\n4. Verify your answer if possible.\n5. End with: ANSWER: <integer> (must be 0-99999)\n\nLet's solve this step by step:\"\"\"\n\nprint(\"Safe Python executor initialized\")\nprint(f\"Available modules: {ALLOWED_MODULES}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:16:44.869194Z","iopub.execute_input":"2025-12-29T10:16:44.869344Z","iopub.status.idle":"2025-12-29T10:16:45.110376Z","shell.execute_reply.started":"2025-12-29T10:16:44.869331Z","shell.execute_reply":"2025-12-29T10:16:45.109941Z"}},"outputs":[{"name":"stdout","text":"Safe Python executor initialized\nAvailable modules: {'numbers', 'random', 'sympy', 'decimal', 'cmath', 'statistics', 'functools', 'fractions', 'itertools', 'math', 'collections'}\n","output_type":"stream"}],"execution_count":5},{"id":"ad048229-fc01-4690-be33-8c8e87a37541","cell_type":"markdown","source":"## CELL E — ANSWER EXTRACTION + VALIDATION","metadata":{}},{"id":"687f0263-d3c2-4716-8bfb-590def754518","cell_type":"code","source":"# ================================\n# CELL E — ANSWER EXTRACTION + VALIDATION\n# ================================\n\ndef extract_answer(text: str) -> Tuple[Optional[int], str]:\n    \"\"\"\n    Extract integer answer from text.\n    Priority:\n    1. \"ANSWER: <int>\" pattern\n    2. \"The answer is <int>\" pattern (explicit)\n    3. \"\\\\boxed{<int>}\" pattern\n    4. Last integer in text (fallback)\n    \"\"\"\n    if not text:\n        return None, \"empty\"\n    \n    text = text.strip()\n    \n    # Priority 1: Explicit ANSWER: pattern\n    answer_patterns = [\n        r\"ANSWER\\s*:\\s*(\\d+)\",\n        r\"answer\\s*:\\s*(\\d+)\",\n        r\"Answer\\s*:\\s*(\\d+)\",\n        r\"final answer\\s*:\\s*(\\d+)\",\n        r\"Final answer\\s*:\\s*(\\d+)\",\n    ]\n    \n    for pattern in answer_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            try:\n                return int(match.group(1)), \"ANSWER\"\n            except ValueError:\n                continue\n    \n    # Priority 2: \"The answer is <int>\" pattern\n    the_answer_patterns = [\n        r\"[Tt]he\\s+answer\\s+is\\s*:?\\s*(\\d+)\",\n        r\"[Tt]he\\s+final\\s+answer\\s+is\\s*:?\\s*(\\d+)\",\n    ]\n    \n    for pattern in the_answer_patterns:\n        match = re.search(pattern, text)\n        if match:\n            try:\n                return int(match.group(1)), \"THE_ANSWER_IS\"\n            except ValueError:\n                continue\n    \n    # Priority 3: \\boxed{} pattern (LaTeX)\n    boxed_patterns = [\n        r\"\\\\\\\\boxed\\{(\\d+)\\}\",\n        r\"\\\\boxed\\{(\\d+)\\}\",\n        r\"\\$\\\\\\\\boxed\\{(\\d+)\\}\\$\",\n        r\"\\$\\\\boxed\\{(\\d+)\\}\\$\",\n    ]\n    \n    for pattern in boxed_patterns:\n        match = re.search(pattern, text)\n        if match:\n            try:\n                return int(match.group(1)), \"BOXED\"\n            except ValueError:\n                continue\n    \n    # Priority 4: Last integer in text (fallback)\n    integers = re.findall(r\"\\b(\\d+)\\b\", text)\n    if integers:\n        try:\n            return int(integers[-1]), \"LASTINT\"\n        except ValueError:\n            pass\n    \n    return None, \"none\"\n\ndef validate_answer(answer: Optional[int]) -> Tuple[bool, int]:\n    \"\"\"Validate and clamp answer to valid range [0, 99999].\"\"\"\n    if answer is None:\n        return False, 0\n    \n    if not isinstance(answer, int):\n        try:\n            answer = int(answer)\n        except (ValueError, TypeError):\n            return False, 0\n    \n    if 0 <= answer <= 99999:\n        return True, answer\n    \n    return False, max(0, min(99999, answer))\n\ndef safe_extract_answer(text: str) -> Tuple[int, Dict[str, Any]]:\n    \"\"\"Safely extract and validate answer, with fallback.\"\"\"\n    raw_answer, method = extract_answer(text)\n    is_valid, final_answer = validate_answer(raw_answer)\n    \n    metadata = {\n        \"raw_answer\": raw_answer,\n        \"method\": method,\n        \"is_valid\": is_valid,\n        \"fallback_used\": not is_valid,\n    }\n    \n    if not is_valid:\n        final_answer = 0\n        metadata[\"fallback_value\"] = 0\n    \n    return final_answer, metadata\n\nprint(\"Answer extraction functions initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:16:45.110837Z","iopub.execute_input":"2025-12-29T10:16:45.111026Z","iopub.status.idle":"2025-12-29T10:16:45.118031Z","shell.execute_reply.started":"2025-12-29T10:16:45.111014Z","shell.execute_reply":"2025-12-29T10:16:45.117625Z"}},"outputs":[{"name":"stdout","text":"Answer extraction functions initialized\n","output_type":"stream"}],"execution_count":6},{"id":"27550729-423b-4198-9e2e-2feb31ec0ef3","cell_type":"markdown","source":"## CELL F — CANDIDATE GENERATION + SELF-CONSISTENCY","metadata":{}},{"id":"1cf452a9-259a-444d-b40c-5ed26bd534e0","cell_type":"code","source":"# ================================\n# CELL F — CANDIDATE GENERATION + SELF-CONSISTENCY\n# ================================\n\ndef generate_one(problem: str, temperature: float = 0.3, max_new_tokens: int = MAX_NEW_TOKENS, prompt_style: str = \"tir\") -> Tuple[str, Dict[str, Any]]:\n    model, tokenizer, device = load_model()\n    \n    if prompt_style == \"tir\":\n        prompt = get_tir_prompt(problem)\n    elif prompt_style == \"concise\":\n        prompt = get_concise_prompt(problem)\n    else:\n        prompt = get_explore_prompt(problem)\n    \n    meta = {\"prompt_style\": prompt_style, \"temperature\": temperature, \"max_new_tokens\": max_new_tokens}\n    \n    if model is None:\n        return \"\", meta\n    \n    try:\n        import torch\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs, max_new_tokens=max_new_tokens,\n                temperature=temperature if temperature > 0 else 1.0,\n                do_sample=temperature > 0, top_p=0.95,\n                pad_token_id=tokenizer.eos_token_id,\n            )\n        \n        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        response = generated[len(prompt):].strip() if prompt in generated else generated.strip()\n        return response, meta\n        \n    except Exception as e:\n        meta[\"error\"] = str(e)\n        return \"\", meta\n\ndef execute_code_in_response(response: str) -> str:\n    code = parse_python_block(response)\n    if code:\n        ok, output = run_python(code)\n        if ok:\n            response += f\"\\n\\n[Code Output]\\n{output}\"\n    return response\n\ndef generate_candidates(problem: str, k: int = K_BASE, temperature_schedule: List[float] = None) -> List[Dict[str, Any]]:\n    if temperature_schedule is None:\n        temperature_schedule = [TEMPERATURE_BASE] * k\n    \n    candidates = []\n    \n    for i in range(k):\n        temp = temperature_schedule[i] if i < len(temperature_schedule) else TEMPERATURE_BASE\n        raw_text, meta = generate_one(problem, temperature=temp, prompt_style=PROMPT_STYLE)\n        processed_text = execute_code_in_response(raw_text)\n        answer, answer_meta = safe_extract_answer(processed_text)\n        \n        candidates.append({\n            \"answer\": answer, \"raw_text\": raw_text,\n            \"processed_text\": processed_text, \"metadata\": {**meta, **answer_meta},\n        })\n    \n    return candidates\n\ndef vote_candidates(candidates: List[Dict[str, Any]]) -> Dict[str, Any]:\n    if not candidates:\n        return {\"top_answer\": 0, \"top_count\": 0, \"total\": 0, \"vote_margin\": 0.0, \"entropy\": 0.0, \"answer_counts\": {}}\n    \n    answers = [c[\"answer\"] for c in candidates]\n    counter = Counter(answers)\n    total = len(answers)\n    \n    most_common = counter.most_common()\n    top_answer, top_count = most_common[0]\n    \n    second_count = most_common[1][1] if len(most_common) > 1 else 0\n    vote_margin = (top_count - second_count) / total\n    \n    entropy = 0.0\n    for count in counter.values():\n        p = count / total\n        if p > 0:\n            entropy -= p * math.log2(p)\n    \n    return {\n        \"top_answer\": top_answer, \"top_count\": top_count, \"total\": total,\n        \"vote_margin\": vote_margin, \"entropy\": entropy,\n        \"answer_counts\": dict(counter.most_common(5)),\n    }\n\ndef should_early_stop(vote_result: Dict[str, Any], threshold: float = VOTING_THRESHOLD) -> bool:\n    if vote_result[\"total\"] == 0:\n        return False\n    return vote_result[\"top_count\"] / vote_result[\"total\"] >= threshold\n\nprint(\"Candidate generation functions initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:16:45.118507Z","iopub.execute_input":"2025-12-29T10:16:45.118626Z","iopub.status.idle":"2025-12-29T10:16:45.134958Z","shell.execute_reply.started":"2025-12-29T10:16:45.118616Z","shell.execute_reply":"2025-12-29T10:16:45.134319Z"}},"outputs":[{"name":"stdout","text":"Candidate generation functions initialized\n","output_type":"stream"}],"execution_count":7},{"id":"b104b09e-6635-4f69-9d70-562183138f75","cell_type":"markdown","source":"## CELL G — VERIFIER","metadata":{}},{"id":"43e501c4-02f5-48db-ba0f-4abde8357f59","cell_type":"code","source":"# ================================\n# CELL G — VERIFIER (Rule-based + Optional LLM)\n# ================================\n\ndef rule_verifier(problem: str, answer: int) -> Dict[str, Any]:\n    checks = []\n    passed = True\n    reason = \"OK\"\n    \n    if not (0 <= answer <= 99999):\n        passed = False\n        reason = f\"Answer {answer} out of valid range [0, 99999]\"\n        checks.append((\"range_check\", False, reason))\n    else:\n        checks.append((\"range_check\", True, \"In valid range\"))\n    \n    problem_lower = problem.lower()\n    if \"remainder\" in problem_lower or \"modulo\" in problem_lower or \"mod \" in problem_lower:\n        mod_patterns = [r\"divided by\\s+(\\d+)\", r\"modulo\\s+(\\d+)\", r\"mod\\s+(\\d+)\", r\"\\(mod\\s*(\\d+)\\)\"]\n        for pattern in mod_patterns:\n            match = re.search(pattern, problem_lower)\n            if match:\n                mod_val = int(match.group(1))\n                if answer >= mod_val and mod_val < 100000:\n                    checks.append((\"mod_check\", False, f\"Answer {answer} >= modulo {mod_val}\"))\n                else:\n                    checks.append((\"mod_check\", True, f\"Answer {answer} < modulo {mod_val}\"))\n                break\n    \n    return {\"passed\": passed, \"reason\": reason, \"checks\": checks}\n\ndef llm_verifier(problem: str, answer: int) -> Dict[str, Any]:\n    model, tokenizer, device = load_model()\n    \n    if model is None:\n        return {\"passed\": True, \"reason\": \"LLM not available\", \"response\": \"\"}\n    \n    prompt = f\"\"\"Given this math problem and proposed answer, quickly check if the answer could be correct.\nIf you find a clear error or contradiction, say INVALID. Otherwise say VALID.\n\nProblem: {problem}\n\nProposed Answer: {answer}\n\nVerification (VALID or INVALID):\"\"\"\n    \n    try:\n        import torch\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        \n        with torch.no_grad():\n            outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.1, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n        \n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()\n        passed = \"INVALID\" not in response.upper()\n        \n        return {\"passed\": passed, \"reason\": \"VALID\" if passed else \"INVALID found\", \"response\": response[:200]}\n        \n    except Exception as e:\n        return {\"passed\": True, \"reason\": f\"Error: {str(e)}\", \"response\": \"\"}\n\ndef verify_answer(problem: str, answer: int, use_llm: bool = False) -> Dict[str, Any]:\n    result = {\"rule_verifier\": rule_verifier(problem, answer), \"llm_verifier\": None, \"final_passed\": True}\n    \n    if not result[\"rule_verifier\"][\"passed\"]:\n        result[\"final_passed\"] = False\n    \n    if use_llm and result[\"rule_verifier\"][\"passed\"]:\n        result[\"llm_verifier\"] = llm_verifier(problem, answer)\n        if not result[\"llm_verifier\"][\"passed\"]:\n            result[\"final_passed\"] = False\n    \n    return result\n\nprint(\"Verifier functions initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:16:45.135424Z","iopub.execute_input":"2025-12-29T10:16:45.135546Z","iopub.status.idle":"2025-12-29T10:16:45.147915Z","shell.execute_reply.started":"2025-12-29T10:16:45.135535Z","shell.execute_reply":"2025-12-29T10:16:45.147535Z"}},"outputs":[{"name":"stdout","text":"Verifier functions initialized\n","output_type":"stream"}],"execution_count":8},{"id":"3ff795e2-a0d3-4b30-820b-50de02cecbf6","cell_type":"markdown","source":"## CELL H — SOLVER ORCHESTRATOR","metadata":{}},{"id":"0a1b24c4-d49d-44e4-ad56-af746a2f4078","cell_type":"code","source":"# ================================\n# CELL H — SOLVER ORCHESTRATOR\n# ================================\n\ndef solve_problem(problem_id: str, problem_text: str) -> Tuple[int, Dict[str, Any]]:\n    \"\"\"\n    Main solver function that orchestrates the entire solving process.\n    \"\"\"\n    global CURRENT_SEED\n    start_time = time.time()\n    \n    # Set seed based on mode policy\n    CURRENT_SEED = get_seed_for_problem(problem_id)\n    set_seed(CURRENT_SEED)\n    \n    # Initialize telemetry with unique keys (no overwriting)\n    telemetry = {\n        \"id\": problem_id,\n        \"problem_hash\": hashlib.md5(problem_text.encode()).hexdigest()[:8],\n        \"elapsed_sec\": 0,\n        \"k_used\": 0,\n        \"candidates_summary\": [],\n        \"chosen_answer\": 0,\n        \"vote_margin\": 0.0,\n        \"verifier_used\": False,\n        \"verifier_pass\": True,\n        \"tool_calls_count\": 0,\n        \"parse_method_used\": \"none\",\n        \"difficulty_mode\": \"EASY\",\n        \"temperature_schedule\": [],\n        \"seed_used\": CURRENT_SEED,\n        \"run_policy\": MODE_POLICY,\n        \"is_rerun\": IS_KAGGLE_RERUN,\n        \"model_available\": is_model_available(),\n    }\n    \n    try:\n        if MODE_POLICY == \"diverse\" and IS_KAGGLE_RERUN:\n            mode, k = \"HARD\", K_MAX_HARD\n            temp_schedule = [TEMPERATURE_HARD] * k\n        else:\n            mode, k = \"EASY\", K_BASE\n            temp_schedule = [TEMPERATURE_BASE] * k\n        \n        telemetry[\"difficulty_mode\"] = mode\n        telemetry[\"temperature_schedule\"] = temp_schedule\n        \n        candidates = []\n        for i in range(k):\n            if time.time() - start_time > TIME_BUDGET_SEC_PER_PROBLEM * 0.8:\n                break\n            \n            raw_text, meta = generate_one(problem_text, temperature=temp_schedule[i])\n            processed_text = execute_code_in_response(raw_text)\n            answer, answer_meta = safe_extract_answer(processed_text)\n            \n            candidates.append({\n                \"answer\": answer,\n                \"raw_text_preview\": raw_text[:500],\n                \"gen_metadata\": {**meta, **answer_meta}\n            })\n            \n            if parse_python_block(raw_text):\n                telemetry[\"tool_calls_count\"] += 1\n        \n        telemetry[\"k_used\"] = len(candidates)\n        \n        vote_result = vote_candidates(candidates)\n        telemetry[\"vote_margin\"] = vote_result[\"vote_margin\"]\n        telemetry[\"candidates_summary\"] = [(a, c) for a, c in vote_result[\"answer_counts\"].items()]\n        \n        chosen_answer = vote_result[\"top_answer\"]\n        \n        for c in candidates:\n            if c[\"answer\"] == chosen_answer:\n                telemetry[\"parse_method_used\"] = c[\"gen_metadata\"].get(\"method\", \"none\")\n                break\n        \n        use_llm_verifier = vote_result[\"vote_margin\"] < 0.3 and mode == \"HARD\"\n        verification = verify_answer(problem_text, chosen_answer, use_llm=use_llm_verifier)\n        \n        telemetry[\"verifier_used\"] = True\n        telemetry[\"verifier_pass\"] = verification[\"final_passed\"]\n        \n        if not verification[\"final_passed\"]:\n            for answer, count in vote_result[\"answer_counts\"].items():\n                if answer != chosen_answer:\n                    alt_verify = verify_answer(problem_text, answer, use_llm=False)\n                    if alt_verify[\"final_passed\"]:\n                        chosen_answer = answer\n                        break\n        \n        is_valid, final_answer = validate_answer(chosen_answer)\n        if not is_valid:\n            telemetry[\"used_fallback\"] = True\n            final_answer = 0\n        \n        telemetry[\"chosen_answer\"] = final_answer\n        \n    except Exception as e:\n        telemetry[\"error_message\"] = str(e)\n        final_answer = 0\n        telemetry[\"chosen_answer\"] = final_answer\n        telemetry[\"used_fallback\"] = True\n    \n    telemetry[\"elapsed_sec\"] = time.time() - start_time\n    \n    return final_answer, telemetry\n\nprint(\"Solver orchestrator initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:16:45.148379Z","iopub.execute_input":"2025-12-29T10:16:45.148498Z","iopub.status.idle":"2025-12-29T10:16:45.160650Z","shell.execute_reply.started":"2025-12-29T10:16:45.148487Z","shell.execute_reply":"2025-12-29T10:16:45.160262Z"}},"outputs":[{"name":"stdout","text":"Solver orchestrator initialized\n","output_type":"stream"}],"execution_count":9},{"id":"40378de9-15a2-4362-9455-3e84f6c992a6","cell_type":"markdown","source":"## CELL I — TELEMETRY LOGGER","metadata":{}},{"id":"811351ee-81ba-491a-b293-a8ab8b4ab9ab","cell_type":"code","source":"# ================================\n# CELL I — TELEMETRY LOGGER\n# ================================\n\ndef append_jsonl(filepath: str, data: Dict[str, Any]):\n    os.makedirs(os.path.dirname(filepath), exist_ok=True) if os.path.dirname(filepath) else None\n    with open(filepath, \"a\") as f:\n        f.write(json.dumps(data, default=str) + \"\\n\")\n        f.flush()\n\ndef read_telemetry(filepath: str) -> List[Dict[str, Any]]:\n    entries = []\n    if os.path.exists(filepath):\n        with open(filepath, \"r\") as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    try:\n                        entries.append(json.loads(line))\n                    except json.JSONDecodeError:\n                        pass\n    return entries\n\ndef log_telemetry(telemetry: Dict[str, Any]):\n    append_jsonl(LOG_PATH, telemetry)\n\ndef print_telemetry_summary(telemetry_list: List[Dict[str, Any]]):\n    if not telemetry_list:\n        print(\"No telemetry data available\")\n        return\n    \n    n = len(telemetry_list)\n    total_time = sum(t.get(\"elapsed_sec\", 0) for t in telemetry_list)\n    avg_time = total_time / n if n > 0 else 0\n    \n    parse_methods = Counter(t.get(\"parse_method_used\", \"none\") for t in telemetry_list)\n    parse_fail_rate = parse_methods.get(\"none\", 0) / n if n > 0 else 0\n    \n    k_values = [t.get(\"k_used\", 0) for t in telemetry_list]\n    avg_k = sum(k_values) / n if n > 0 else 0\n    \n    verifier_used = sum(1 for t in telemetry_list if t.get(\"verifier_used\", False))\n    verifier_pass = sum(1 for t in telemetry_list if t.get(\"verifier_pass\", True))\n    \n    fallback_used = sum(1 for t in telemetry_list if t.get(\"fallback_used\", False))\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"TELEMETRY SUMMARY\")\n    print(\"=\"*50)\n    print(f\"Total problems: {n}\")\n    print(f\"Total time: {total_time:.2f}s\")\n    print(f\"Avg time per problem: {avg_time:.2f}s\")\n    print(f\"Parse fail rate: {parse_fail_rate:.2%}\")\n    print(f\"Avg k_used: {avg_k:.1f}\")\n    print(f\"K distribution: {Counter(k_values)}\")\n    print(f\"Parse methods: {dict(parse_methods)}\")\n    print(f\"Verifier usage: {verifier_used}/{n}\")\n    print(f\"Verifier pass rate: {verifier_pass}/{verifier_used if verifier_used > 0 else 1}\")\n    print(f\"Fallback used: {fallback_used}/{n}\")\n    print(\"=\"*50)\n\nprint(\"Telemetry logger initialized\")\nprint(f\"Log path: {LOG_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:16:45.161697Z","iopub.execute_input":"2025-12-29T10:16:45.161816Z","iopub.status.idle":"2025-12-29T10:16:45.174070Z","shell.execute_reply.started":"2025-12-29T10:16:45.161806Z","shell.execute_reply":"2025-12-29T10:16:45.173686Z"}},"outputs":[{"name":"stdout","text":"Telemetry logger initialized\nLog path: /kaggle/working/aimo3_telemetry.jsonl\n","output_type":"stream"}],"execution_count":10},{"id":"60f4650a-4809-4f8d-bccf-b44572bb412b","cell_type":"markdown","source":"## CELL J — LOCAL HARNESS (Reference CSV Regression)","metadata":{}},{"id":"3f84873e-5c53-478f-acfc-4ae2b6da51e4","cell_type":"code","source":"# ================================\n# CELL J — LOCAL HARNESS (Reference CSV Regression)\n# ================================\n\ndef run_reference_eval(csv_path: str = None, limit: int = None) -> Dict[str, Any]:\n    csv_path = csv_path or REFERENCE_CSV_PATH\n    \n    if not os.path.exists(csv_path):\n        print(f\"Reference CSV not found: {csv_path}\")\n        return {\"error\": \"File not found\", \"accuracy\": 0.0}\n    \n    print(f\"\\nRunning reference evaluation on: {csv_path}\")\n    print(\"=\"*60)\n    \n    df = pd.read_csv(csv_path)\n    \n    if limit:\n        df = df.head(limit)\n    \n    n_problems = len(df)\n    print(f\"Evaluating {n_problems} problems...\\n\")\n    \n    results = []\n    correct = 0\n    telemetry_list = []\n    \n    for idx, row in df.iterrows():\n        problem_id = str(row[\"id\"])\n        problem_text = row[\"problem\"]\n        expected_answer = int(row[\"answer\"])\n        \n        print(f\"[{idx+1}/{n_problems}] Problem {problem_id}...\")\n        \n        predicted_answer, telemetry = solve_problem(problem_id, problem_text)\n        \n        telemetry[\"expected_answer\"] = expected_answer\n        telemetry[\"is_correct\"] = (predicted_answer == expected_answer)\n        log_telemetry(telemetry)\n        telemetry_list.append(telemetry)\n        \n        is_correct = (predicted_answer == expected_answer)\n        if is_correct:\n            correct += 1\n            status = \"Y\"\n        else:\n            status = \"X\"\n        \n        print(f\"  {status} Predicted: {predicted_answer}, Expected: {expected_answer} ({telemetry['elapsed_sec']:.2f}s)\")\n        \n        results.append({\n            \"id\": problem_id, \"predicted\": predicted_answer,\n            \"expected\": expected_answer, \"correct\": is_correct,\n            \"elapsed_sec\": telemetry[\"elapsed_sec\"],\n        })\n    \n    accuracy = correct / n_problems if n_problems > 0 else 0.0\n    \n    print(\"\\n\" + \"=\"*60)\n    print(f\"ACCURACY: {correct}/{n_problems} = {accuracy:.2%}\")\n    print(\"=\"*60)\n    \n    print_telemetry_summary(telemetry_list)\n    \n    return {\"accuracy\": accuracy, \"correct\": correct, \"total\": n_problems, \"results\": results}\n\nprint(\"Local harness initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:16:45.174640Z","iopub.execute_input":"2025-12-29T10:16:45.174798Z","iopub.status.idle":"2025-12-29T10:16:45.185768Z","shell.execute_reply.started":"2025-12-29T10:16:45.174788Z","shell.execute_reply":"2025-12-29T10:16:45.185337Z"}},"outputs":[{"name":"stdout","text":"Local harness initialized\n","output_type":"stream"}],"execution_count":11},{"id":"3df7598d-d289-439c-aa26-eefa23cd50ac","cell_type":"markdown","source":"## CELL K — SUBMISSION GLUE (Kaggle Evaluation API)","metadata":{}},{"id":"87a81805-fceb-40cf-b7bb-33f9877f7658","cell_type":"code","source":"# ================================\n# CELL K — SUBMISSION GLUE (Kaggle Evaluation API)\n# ================================\n\nimport sys\nimport os\n\nkaggle_eval_paths = [\"/kaggle/input/kaggle-evaluation\", \"/kaggle/input\", \".\", \"..\"]\n\nfor path in kaggle_eval_paths:\n    if os.path.exists(os.path.join(path, \"kaggle_evaluation\")):\n        sys.path.insert(0, path)\n        break\n\ndef predict(test_input: Union[pd.DataFrame, dict, pd.Series]) -> pd.DataFrame:\n    \"\"\"\n    Kaggle prediction endpoint.\n    Accepts DataFrame, dict, or Series as input.\n    Returns DataFrame with columns 'id' and 'answer'.\n    \"\"\"\n    # Convert input to DataFrame if needed\n    if isinstance(test_input, dict):\n        test_df = pd.DataFrame([test_input])\n    elif isinstance(test_input, pd.Series):\n        test_df = pd.DataFrame([test_input.to_dict()])\n    elif isinstance(test_input, pd.DataFrame):\n        test_df = test_input\n    else:\n        raise ValueError(f\"predict() expects DataFrame, dict, or Series, got {type(test_input)}\")\n    \n    # Validate required columns\n    required_cols = {\"id\", \"problem\"}\n    missing_cols = required_cols - set(test_df.columns)\n    if missing_cols:\n        raise ValueError(f\"Input missing required columns: {missing_cols}\")\n    \n    results = []\n    \n    for idx, row in test_df.iterrows():\n        problem_id = str(row[\"id\"])\n        problem_text = str(row[\"problem\"])\n        \n        answer, telemetry = solve_problem(problem_id, problem_text)\n        log_telemetry(telemetry)\n        \n        results.append({\"id\": problem_id, \"answer\": int(answer)})\n    \n    return pd.DataFrame(results)\n\ndef setup_and_serve():\n    \"\"\"\n    Setup and start the inference server.\n    Handles missing run_local_gateway gracefully.\n    \"\"\"\n    try:\n        from kaggle_evaluation.aimo_3_inference_server import AIMO3InferenceServer\n        \n        server = AIMO3InferenceServer(predict)\n        \n        if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\") == \"1\":\n            print(\"Starting inference server (competition mode)...\")\n            server.serve()\n        else:\n            # Try run_local_gateway, but handle if it doesn't exist\n            print(\"Attempting local gateway test...\")\n            if hasattr(server, 'run_local_gateway'):\n                try:\n                    server.run_local_gateway()\n                except Exception as e:\n                    print(f\"run_local_gateway failed: {e}\")\n                    print(\"Falling back to direct serve()...\")\n                    server.serve()\n            else:\n                print(\"run_local_gateway not available, using serve()...\")\n                server.serve()\n            \n    except ImportError as e:\n        print(f\"kaggle_evaluation not available: {e}\")\n        print(\"Running in local-only mode\")\n    except Exception as e:\n        print(f\"Error in setup_and_serve: {e}\")\n        print(\"Running in local-only mode\")\n\nprint(\"Submission glue initialized\")\nprint(\"predict() accepts: DataFrame, dict, or Series\")\nprint(\"Use setup_and_serve() to start the server\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:16:45.186229Z","iopub.execute_input":"2025-12-29T10:16:45.186371Z","iopub.status.idle":"2025-12-29T10:16:45.198590Z","shell.execute_reply.started":"2025-12-29T10:16:45.186360Z","shell.execute_reply":"2025-12-29T10:16:45.198188Z"}},"outputs":[{"name":"stdout","text":"Submission glue initialized\npredict() accepts: DataFrame, dict, or Series\nUse setup_and_serve() to start the server\n","output_type":"stream"}],"execution_count":12},{"id":"9582f05e-7ac3-4297-b0cd-65cc497bf2ff","cell_type":"markdown","source":"## SELF TEST — Unit Tests","metadata":{}},{"id":"8f507983-c46d-4fdf-9d9a-513043ac8ac9","cell_type":"code","source":"# ================================\n# SELF TEST 1: Schema/Predict Unit Test\n# ================================\n\ndef test_predict_schema():\n    print(\"\\nTEST 1: Schema/Predict Unit Test\")\n    print(\"-\"*40)\n    \n    test_df = pd.DataFrame({\"id\": [\"test001\"], \"problem\": [\"What is $1+1$?\"]})\n    result_df = predict(test_df)\n    \n    assert \"id\" in result_df.columns, \"Missing 'id' column\"\n    assert \"answer\" in result_df.columns, \"Missing 'answer' column\"\n    assert len(result_df) == 1, f\"Expected 1 row, got {len(result_df)}\"\n    \n    answer = result_df[\"answer\"].iloc[0]\n    assert isinstance(answer, (int, type(1))), f\"Answer should be int, got {type(answer)}\"\n    assert 0 <= answer <= 99999, f\"Answer {answer} out of range [0, 99999]\"\n    \n    print(f\"OK Output schema correct\")\n    print(f\"OK Answer: {answer} (valid int in [0, 99999])\")\n    print(\"TEST 1 PASSED\\n\")\n    return True\n\nif RUN_MODE == \"local_ref\":\n    try:\n        test_predict_schema()\n    except AssertionError as e:\n        print(f\"TEST 1 FAILED: {e}\")\n    except Exception as e:\n        print(f\"TEST 1 ERROR: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:16:45.199015Z","iopub.execute_input":"2025-12-29T10:16:45.199128Z","iopub.status.idle":"2025-12-29T10:28:08.389802Z","shell.execute_reply.started":"2025-12-29T10:16:45.199119Z","shell.execute_reply":"2025-12-29T10:28:08.389342Z"}},"outputs":[{"name":"stdout","text":"\nTEST 1: Schema/Predict Unit Test\n----------------------------------------\nLoading model...\nUsing device: cuda\nLoading from HuggingFace: /kaggle/input/qwq-32b-preview/transformers/default/1\n","output_type":"stream"},{"name":"stderr","text":"2025-12-29 10:16:56.106597: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767003416.302751      78 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767003416.360341      78 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67b5bf095e054c8686adc08e2ebfc33e"}},"metadata":{}},{"name":"stdout","text":"Model loaded in 671.43s\nTEST 1 FAILED: Answer should be int, got <class 'numpy.int64'>\n","output_type":"stream"}],"execution_count":13},{"id":"6be5c955-7bb0-42b5-85c8-fdec0389c869","cell_type":"code","source":"# ================================\n# SELF TEST 2: Reference Evaluation\n# ================================\n\ndef test_reference_eval():\n    print(\"\\nTEST 2: Reference Evaluation\")\n    print(\"-\"*40)\n    \n    result = run_reference_eval(limit=2)\n    \n    assert \"accuracy\" in result, \"Missing 'accuracy' in result\"\n    assert \"total\" in result, \"Missing 'total' in result\"\n    assert result[\"total\"] == 2, f\"Expected 2 problems, got {result['total']}\"\n    \n    print(f\"OK Accuracy: {result['accuracy']:.2%}\")\n    print(f\"OK No crashes during evaluation\")\n    print(\"TEST 2 PASSED\\n\")\n    return True\n\nif RUN_MODE == \"local_ref\":\n    try:\n        if os.path.exists(REFERENCE_CSV_PATH):\n            test_reference_eval()\n        else:\n            print(\"Skipping TEST 2: reference.csv not found\")\n    except AssertionError as e:\n        print(f\"TEST 2 FAILED: {e}\")\n    except Exception as e:\n        print(f\"TEST 2 ERROR: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:28:08.390303Z","iopub.execute_input":"2025-12-29T10:28:08.390596Z","iopub.status.idle":"2025-12-29T10:32:59.273791Z","shell.execute_reply.started":"2025-12-29T10:28:08.390584Z","shell.execute_reply":"2025-12-29T10:32:59.273252Z"}},"outputs":[{"name":"stdout","text":"\nTEST 2: Reference Evaluation\n----------------------------------------\n\nRunning reference evaluation on: /kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\n============================================================\nEvaluating 2 problems...\n\n[1/2] Problem 0e644e...\n  X Predicted: 5, Expected: 336 (130.25s)\n[2/2] Problem 26de63...\n  X Predicted: 16, Expected: 32951 (160.61s)\n\n============================================================\nACCURACY: 0/2 = 0.00%\n============================================================\n\n==================================================\nTELEMETRY SUMMARY\n==================================================\nTotal problems: 2\nTotal time: 290.86s\nAvg time per problem: 145.43s\nParse fail rate: 0.00%\nAvg k_used: 2.0\nK distribution: Counter({2: 2})\nParse methods: {'LASTINT': 2}\nVerifier usage: 2/2\nVerifier pass rate: 2/2\nFallback used: 0/2\n==================================================\nOK Accuracy: 0.00%\nOK No crashes during evaluation\nTEST 2 PASSED\n\n","output_type":"stream"}],"execution_count":14},{"id":"9011c467-02e5-497e-b3e5-fe54da165040","cell_type":"code","source":"# ================================\n# SELF TEST 3: Tool Executor\n# ================================\n\ndef test_tool_executor():\n    print(\"\\nTEST 3: Tool Executor\")\n    print(\"-\"*40)\n    \n    # Test 1: Simple arithmetic\n    ok, output = run_python(\"result = 2 + 3\")\n    assert ok, f\"Execution failed: {output}\"\n    assert \"5\" in output, f\"Expected '5' in output, got: {output}\"\n    print(f\"OK Simple arithmetic: 2+3 = 5\")\n    \n    # Test 2: Math module (already loaded, no import needed)\n    ok, output = run_python(\"result = math.factorial(5)\")\n    assert ok, f\"Execution failed: {output}\"\n    assert \"120\" in output, f\"Expected '120' in output, got: {output}\"\n    print(f\"OK Math module: factorial(5) = 120\")\n    \n    # Test 3: Code with import statement (should be stripped)\n    ok, output = run_python(\"import math\\nresult = math.sqrt(16)\")\n    assert ok, f\"Execution failed: {output}\"\n    assert \"4\" in output, f\"Expected '4' in output, got: {output}\"\n    print(f\"OK Import stripping works: sqrt(16) = 4\")\n    \n    # Test 4: Timeout handling\n    ok, output = run_python(\"x = 1\", timeout_sec=1)\n    assert ok, f\"Simple code should not timeout\"\n    print(f\"OK Timeout handling works\")\n    \n    print(\"TEST 3 PASSED\\n\")\n    return True\n\nif RUN_MODE == \"local_ref\":\n    try:\n        test_tool_executor()\n    except AssertionError as e:\n        print(f\"TEST 3 FAILED: {e}\")\n    except Exception as e:\n        print(f\"TEST 3 ERROR: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:32:59.274347Z","iopub.execute_input":"2025-12-29T10:32:59.274660Z","iopub.status.idle":"2025-12-29T10:32:59.279060Z","shell.execute_reply.started":"2025-12-29T10:32:59.274648Z","shell.execute_reply":"2025-12-29T10:32:59.278669Z"}},"outputs":[{"name":"stdout","text":"\nTEST 3: Tool Executor\n----------------------------------------\nOK Simple arithmetic: 2+3 = 5\nOK Math module: factorial(5) = 120\nOK Import stripping works: sqrt(16) = 4\nOK Timeout handling works\nTEST 3 PASSED\n\n","output_type":"stream"}],"execution_count":15},{"id":"632cf983-2d6d-448f-b16f-4bf601a8c78c","cell_type":"code","source":"# ================================\n# SELF TEST 4: Answer Extraction\n# ================================\n\ndef test_answer_extraction():\n    print(\"\\nTEST 4: Answer Extraction\")\n    print(\"-\"*40)\n    \n    test_cases = [\n        (\"ANSWER: 42\", 42, \"ANSWER\"),\n        (\"The answer is 123\", 123, \"THE_ANSWER_IS\"),\n        (\"After calculation, we get 456. ANSWER: 456\", 456, \"ANSWER\"),\n        (\"Result: 789\", 789, \"LASTINT\"),\n        (\"The final answer is 999\", 999, \"THE_ANSWER_IS\"),\n    ]\n    \n    for text, expected_answer, expected_method in test_cases:\n        answer, method = extract_answer(text)\n        assert answer == expected_answer, f\"Expected {expected_answer}, got {answer} for '{text}'\"\n        assert method == expected_method, f\"Expected method {expected_method}, got {method} for '{text}'\"\n        print(f\"OK '{text[:30]}...' -> {answer} ({method})\")\n    \n    print(\"TEST 4 PASSED\\n\")\n    return True\n\nif RUN_MODE == \"local_ref\":\n    try:\n        test_answer_extraction()\n    except AssertionError as e:\n        print(f\"TEST 4 FAILED: {e}\")\n    except Exception as e:\n        print(f\"TEST 4 ERROR: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:32:59.279496Z","iopub.execute_input":"2025-12-29T10:32:59.279605Z","iopub.status.idle":"2025-12-29T10:32:59.291910Z","shell.execute_reply.started":"2025-12-29T10:32:59.279596Z","shell.execute_reply":"2025-12-29T10:32:59.291476Z"}},"outputs":[{"name":"stdout","text":"\nTEST 4: Answer Extraction\n----------------------------------------\nOK 'ANSWER: 42...' -> 42 (ANSWER)\nOK 'The answer is 123...' -> 123 (THE_ANSWER_IS)\nOK 'After calculation, we get 456....' -> 456 (ANSWER)\nOK 'Result: 789...' -> 789 (LASTINT)\nOK 'The final answer is 999...' -> 999 (THE_ANSWER_IS)\nTEST 4 PASSED\n\n","output_type":"stream"}],"execution_count":16},{"id":"7f1f7228-b0ff-4afb-9a67-6b6614aa96c7","cell_type":"code","source":"# ================================\n# SELF TEST 5: predict() Input Types\n# ================================\n\ndef test_predict_input_types():\n    print(\"\\nTEST 5: predict() Input Types\")\n    print(\"-\"*40)\n    \n    # Test 1: DataFrame input\n    df_input = pd.DataFrame({\"id\": [\"test_df\"], \"problem\": [\"What is $2+2$?\"]})\n    result = predict(df_input)\n    assert isinstance(result, pd.DataFrame), \"Result should be DataFrame\"\n    assert \"answer\" in result.columns, \"Missing 'answer' column\"\n    print(\"OK DataFrame input works\")\n    \n    # Test 2: dict input\n    dict_input = {\"id\": \"test_dict\", \"problem\": \"What is $3+3$?\"}\n    result = predict(dict_input)\n    assert isinstance(result, pd.DataFrame), \"Result should be DataFrame\"\n    assert len(result) == 1, \"Should have 1 row\"\n    print(\"OK dict input works\")\n    \n    # Test 3: Series input\n    series_input = pd.Series({\"id\": \"test_series\", \"problem\": \"What is $4+4$?\"})\n    result = predict(series_input)\n    assert isinstance(result, pd.DataFrame), \"Result should be DataFrame\"\n    assert len(result) == 1, \"Should have 1 row\"\n    print(\"OK Series input works\")\n    \n    print(\"TEST 5 PASSED\\n\")\n    return True\n\nif RUN_MODE == \"local_ref\":\n    try:\n        test_predict_input_types()\n    except AssertionError as e:\n        print(f\"TEST 5 FAILED: {e}\")\n    except Exception as e:\n        print(f\"TEST 5 ERROR: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:32:59.292382Z","iopub.execute_input":"2025-12-29T10:32:59.292515Z","iopub.status.idle":"2025-12-29T10:34:29.578530Z","shell.execute_reply.started":"2025-12-29T10:32:59.292505Z","shell.execute_reply":"2025-12-29T10:34:29.578116Z"}},"outputs":[{"name":"stdout","text":"\nTEST 5: predict() Input Types\n----------------------------------------\nOK DataFrame input works\nOK dict input works\nOK Series input works\nTEST 5 PASSED\n\n","output_type":"stream"}],"execution_count":17},{"id":"0732d50f-f2a8-458a-9468-f082025d9b77","cell_type":"markdown","source":"## MAIN EXECUTION","metadata":{}},{"id":"8ff34e47-99d0-483c-b069-a2261e19b815","cell_type":"code","source":"# ================================\n# MAIN EXECUTION\n# ================================\n\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\"*60)\n    print(\"AIMO3 BASELINE NOTEBOOK\")\n    print(\"=\"*60)\n    print(f\"RUN_MODE: {RUN_MODE}\")\n    print(f\"CURRENT_SEED: {CURRENT_SEED}\")\n    print(f\"IS_KAGGLE_RERUN: {IS_KAGGLE_RERUN}\")\n    print(\"=\"*60 + \"\\n\")\n    \n    if RUN_MODE == \"local_ref\":\n        print(\"Running in LOCAL/DEBUG mode...\")\n        print(\"Evaluating reference.csv...\\n\")\n        \n        if os.path.exists(LOG_PATH):\n            os.remove(LOG_PATH)\n        \n        if os.path.exists(REFERENCE_CSV_PATH):\n            result = run_reference_eval()\n            print(f\"\\nFinal Accuracy: {result['accuracy']:.2%}\")\n        else:\n            print(f\"Reference CSV not found: {REFERENCE_CSV_PATH}\")\n            print(\"Running self-tests only...\")\n        \n    elif RUN_MODE == \"submit_auto\":\n        print(\"Running in SUBMISSION mode...\")\n        print(\"Starting inference server...\\n\")\n        \n        setup_and_serve()\n    \n    else:\n        print(f\"Unknown RUN_MODE: {RUN_MODE}\")\n        print(\"Valid options: 'local_ref', 'submit_auto'\")\n\nprint(\"\\nNotebook execution complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:34:29.579019Z","iopub.execute_input":"2025-12-29T10:34:29.579146Z","iopub.status.idle":"2025-12-29T10:57:57.170070Z","shell.execute_reply.started":"2025-12-29T10:34:29.579135Z","shell.execute_reply":"2025-12-29T10:57:57.169632Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nAIMO3 BASELINE NOTEBOOK\n============================================================\nRUN_MODE: local_ref\nCURRENT_SEED: 42\nIS_KAGGLE_RERUN: False\n============================================================\n\nRunning in LOCAL/DEBUG mode...\nEvaluating reference.csv...\n\n\nRunning reference evaluation on: /kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\n============================================================\nEvaluating 10 problems...\n\n[1/10] Problem 0e644e...\n  X Predicted: 5, Expected: 336 (129.91s)\n[2/10] Problem 26de63...\n  X Predicted: 16, Expected: 32951 (160.53s)\n[3/10] Problem 424e18...\n  X Predicted: 62140, Expected: 21818 (102.40s)\n[4/10] Problem 42d360...\n  X Predicted: 32192, Expected: 32193 (123.65s)\n[5/10] Problem 641659...\n  X Predicted: 1, Expected: 57447 (161.16s)\n[6/10] Problem 86e8e5...\n  X Predicted: 1, Expected: 8687 (120.38s)\n[7/10] Problem 92ba6a...\n  Y Predicted: 50, Expected: 50 (161.90s)\n[8/10] Problem 9c1c5f...\n  X Predicted: 12, Expected: 580 (165.79s)\n[9/10] Problem a295e9...\n  X Predicted: 3, Expected: 520 (116.40s)\n[10/10] Problem dd7f5e...\n  X Predicted: 8, Expected: 160 (165.44s)\n\n============================================================\nACCURACY: 1/10 = 10.00%\n============================================================\n\n==================================================\nTELEMETRY SUMMARY\n==================================================\nTotal problems: 10\nTotal time: 1407.57s\nAvg time per problem: 140.76s\nParse fail rate: 0.00%\nAvg k_used: 2.0\nK distribution: Counter({2: 10})\nParse methods: {'LASTINT': 6, 'BOXED': 4}\nVerifier usage: 10/10\nVerifier pass rate: 10/10\nFallback used: 0/10\n==================================================\n\nFinal Accuracy: 10.00%\n\nNotebook execution complete.\n","output_type":"stream"}],"execution_count":18}]}