{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AIMO3 Baseline Notebook\n## AI Mathematical Olympiad \u2013 Progress Prize 3\n\n### Quick Start Guide\n1. **RUN_MODE Selection**:\n   - `\"local_ref\"`: Debug mode - runs evaluation on `reference.csv` (10 problems)\n   - `\"submit_auto\"`: Kaggle submission mode - uses `kaggle_evaluation` API\n\n2. **Model Setup (Kaggle)**:\n   - Add your model as a Kaggle Dataset/Model input\n   - Update `MODEL_PATH` in CONFIG to point to `/kaggle/input/your-model-name`\n   - Or use Kaggle's built-in models\n\n3. **Telemetry**:\n   - Logs saved to `/kaggle/working/aimo3_telemetry.jsonl`\n   - Download after submission for analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL A \u2014 CONFIG (Constants)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL A \u2014 CONFIG (Constants)\n# ================================\n\nimport os\n\n# ----- RUN MODE AUTO DETECTION -----\n# Automatically set to \"submit_auto\" if KAGGLE_IS_COMPETITION_RERUN is set\n_is_kaggle_rerun = os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\") == \"1\"\nif _is_kaggle_rerun:\n    RUN_MODE = \"submit_auto\"\nelse:\n    # Available modes:\n    # \"local_ref\": Run evaluation on reference.csv (10 problems) - for debugging\n    # \"submit_auto\": Kaggle submission mode - uses kaggle_evaluation API\n    # \"quick_tune\": Fast Optuna tuning on reference subset (5-10 trials)\n    # \"full_tune\": Full Optuna tuning on reference subset (20-50 trials)\n    RUN_MODE = \"local_ref\"  # Change as needed\n\n# ----- TIME BUDGET -----\nTIME_BUDGET_SEC_PER_PROBLEM = 180  # Increased for complex problems\n\n# ----- GENERATION PARAMS (defaults, can be tuned) -----\nK_BASE = 8              # Increased: more candidates for voting\nK_MAX_HARD = 16         # Max candidates for hard problems  \nTEMPERATURE_BASE = 0.7  # Increased: more diversity\nTEMPERATURE_HARD = 0.9  # Temperature for exploration\nMAX_NEW_TOKENS = 4096   # Increased for longer reasoning\nTOP_P = 0.95            # Nucleus sampling\nTOP_K = 50              # Top-k sampling\n\n# ----- VOTING & SELECTION -----\nVOTING_THRESHOLD = 0.5  # Lowered: require more consensus\nSELECTION_STRATEGY = \"majority_vote\"  # \"majority_vote\", \"verifier_weighted\", \"consensus\"\n\n# ----- PATHS -----\nLOG_PATH = \"/kaggle/working/aimo3_telemetry.jsonl\"\nCACHE_DIR = \"/kaggle/working/cache\"\nBEST_CONFIG_PATH = \"/kaggle/working/best_config.json\"\n\n# ----- MODEL CONFIG -----\nMODEL_PATH = \"/kaggle/input/qwq-32b-preview/transformers/default/1\"  # QwQ-32B model\nMODEL_ID = \"/kaggle/input/qwq-32b-preview/transformers/default/1\"\n\n# ----- PROMPT STYLE -----\n# \"strict_final\": Two-pass with strict FINAL: tag\n# \"tir\": Tool-Integrated Reasoning (original)\n# \"concise\": Direct answer only\nPROMPT_STYLE = \"strict_final\"\n\n# ----- MODE POLICY -----\nMODE_POLICY = \"stable\"  # \"stable\" or \"diverse\"\n\n# ----- DATA PATHS -----\nREFERENCE_CSV_PATH = None\nTEST_CSV_PATH = None\n\n# Detect environment and set paths\nif os.path.exists(\"/kaggle/input\"):\n    REFERENCE_CSV_PATH = \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\"\n    TEST_CSV_PATH = \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv\"\n    if not os.path.exists(REFERENCE_CSV_PATH):\n        REFERENCE_CSV_PATH = \"reference.csv\"\n        TEST_CSV_PATH = \"test.csv\"\nelse:\n    REFERENCE_CSV_PATH = \"reference.csv\"\n    TEST_CSV_PATH = \"test.csv\"\n\n# Create working directories\nos.makedirs(os.path.dirname(LOG_PATH), exist_ok=True) if os.path.dirname(LOG_PATH) else None\nos.makedirs(CACHE_DIR, exist_ok=True) if CACHE_DIR else None\n\n# ----- TUNING PARAMS -----\nTUNE_TRIALS_QUICK = 10\nTUNE_TRIALS_FULL = 30\nTUNE_TIMEOUT_SEC = 3600  # 1 hour max for tuning\n\nprint(f\"RUN_MODE: {RUN_MODE} (auto-detected rerun: {_is_kaggle_rerun})\")\nprint(f\"MODEL_PATH: {MODEL_PATH}\")\nprint(f\"PROMPT_STYLE: {PROMPT_STYLE}\")\nprint(f\"K_BASE: {K_BASE}, TEMPERATURE_BASE: {TEMPERATURE_BASE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL B \u2014 IMPORTS + SEED CONTROL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL B \u2014 IMPORTS + SEED CONTROL\n# ================================\n\nimport os\nimport re\nimport sys\nimport time\nimport json\nimport math\nimport random\nimport hashlib\nimport warnings\nfrom typing import Optional, Tuple, List, Dict, Any, Union\nfrom collections import Counter\nfrom contextlib import redirect_stdout, redirect_stderr\nimport io\n\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\n# ----- SEED CONTROL -----\nBASE_SEED = 42\nIS_KAGGLE_RERUN = os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\") == \"1\"\n\ndef get_seed_for_problem(problem_id: str = None) -> int:\n    \"\"\"\n    Get seed based on mode policy.\n    - stable: Always return BASE_SEED (deterministic across runs)\n    - diverse: Return deterministic seed based on problem_id hash (for rerun diversity)\n    \"\"\"\n    if MODE_POLICY == \"diverse\" and problem_id is not None:\n        # Deterministic hash-based seed for diversity\n        hash_val = int(hashlib.md5(problem_id.encode()).hexdigest()[:8], 16)\n        return BASE_SEED + (hash_val % 10000)\n    else:\n        return BASE_SEED\n\ndef set_seed(seed: int):\n    \"\"\"Set random seeds for reproducibility.\"\"\"\n    random.seed(seed)\n    try:\n        import numpy as np\n        np.random.seed(seed)\n    except ImportError:\n        pass\n    try:\n        import torch\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(seed)\n    except ImportError:\n        pass\n\n# Initialize with base seed (will be updated per-problem if diverse mode)\nCURRENT_SEED = BASE_SEED\nset_seed(CURRENT_SEED)\n\nprint(f\"IS_KAGGLE_RERUN: {IS_KAGGLE_RERUN}\")\nprint(f\"MODE_POLICY: {MODE_POLICY}\")\nprint(f\"BASE_SEED: {BASE_SEED}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL C \u2014 LAZY MODEL LOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL C \u2014 LAZY MODEL LOADER\n# ================================\n\n_model_cache = {\"model\": None, \"tokenizer\": None, \"device\": None, \"loaded\": False, \"skip_reason\": None}\n\ndef get_device():\n    try:\n        import torch\n        if torch.cuda.is_available():\n            return \"cuda\"\n        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n            return \"mps\"\n    except ImportError:\n        pass\n    return \"cpu\"\n\ndef load_model():\n    \"\"\"\n    Lazy load model and tokenizer.\n    In Kaggle rerun: always use local_files_only=True.\n    If MODEL_PATH is None/missing: skip load immediately and log warning.\n    \"\"\"\n    global _model_cache\n    \n    if _model_cache[\"loaded\"]:\n        return _model_cache[\"model\"], _model_cache[\"tokenizer\"], _model_cache[\"device\"]\n    \n    # Check if we should skip loading\n    has_local_model = MODEL_PATH is not None and os.path.exists(MODEL_PATH)\n    \n    if IS_KAGGLE_RERUN and not has_local_model:\n        # In rerun mode without local model, skip loading\n        print(\"WARNING: No local model available (MODEL_PATH is None or missing)\")\n        print(\"Skipping model load - using fallback solver\")\n        _model_cache[\"model\"] = None\n        _model_cache[\"tokenizer\"] = None\n        _model_cache[\"device\"] = \"cpu\"\n        _model_cache[\"loaded\"] = True\n        _model_cache[\"skip_reason\"] = \"no_local_model_in_rerun\"\n        return None, None, \"cpu\"\n    \n    print(\"Loading model...\")\n    start_time = time.time()\n    \n    try:\n        import torch\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n        \n        device = get_device()\n        _model_cache[\"device\"] = device\n        print(f\"Using device: {device}\")\n        \n        # Determine model source and local_files_only setting\n        if has_local_model:\n            model_source = MODEL_PATH\n            local_only = True\n            print(f\"Loading from local path: {model_source}\")\n        elif IS_KAGGLE_RERUN:\n            # Should not reach here due to check above, but safety fallback\n            print(\"WARNING: Cannot load model - no local path and in rerun mode\")\n            raise RuntimeError(\"No local model available in rerun mode\")\n        else:\n            # Local development with internet - can download\n            model_source = MODEL_ID\n            local_only = False\n            print(f\"Loading from HuggingFace: {model_source}\")\n        \n        _model_cache[\"tokenizer\"] = AutoTokenizer.from_pretrained(\n            model_source, trust_remote_code=True, local_files_only=local_only\n        )\n        \n        dtype = torch.float16 if device == \"cuda\" else torch.float32\n        _model_cache[\"model\"] = AutoModelForCausalLM.from_pretrained(\n            model_source, torch_dtype=dtype,\n            device_map=\"auto\" if device == \"cuda\" else None,\n            trust_remote_code=True, local_files_only=local_only\n        )\n        \n        if device != \"cuda\":\n            _model_cache[\"model\"] = _model_cache[\"model\"].to(device)\n        \n        _model_cache[\"loaded\"] = True\n        print(f\"Model loaded in {time.time() - start_time:.2f}s\")\n        \n    except Exception as e:\n        print(f\"WARNING: Could not load model: {e}\")\n        print(\"Using rule-based solver...\")\n        _model_cache[\"model\"] = None\n        _model_cache[\"tokenizer\"] = None\n        _model_cache[\"device\"] = \"cpu\"\n        _model_cache[\"loaded\"] = True\n        _model_cache[\"skip_reason\"] = str(e)\n    \n    return _model_cache[\"model\"], _model_cache[\"tokenizer\"], _model_cache[\"device\"]\n\ndef is_model_available():\n    if _model_cache[\"loaded\"]:\n        return _model_cache[\"model\"] is not None\n    if MODEL_PATH and os.path.exists(MODEL_PATH):\n        return True\n    return False\n\nprint(\"Model loader initialized (lazy loading)\")\nprint(f\"MODEL_PATH: {MODEL_PATH}\")\nprint(f\"IS_KAGGLE_RERUN: {IS_KAGGLE_RERUN}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL D \u2014 TOOL-INTEGRATED REASONING + SAFE PYTHON EXECUTOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL D \u2014 TIR-lite + SAFE PYTHON EXECUTOR + STRICT PROMPTS\n# ================================\n\nALLOWED_MODULES = {\"math\", \"fractions\", \"itertools\", \"functools\", \"collections\", \"decimal\", \"numbers\", \"cmath\", \"random\", \"statistics\"}\n\ntry:\n    import sympy\n    ALLOWED_MODULES.add(\"sympy\")\nexcept ImportError:\n    pass\n\ndef create_safe_globals():\n    \"\"\"Create a safe globals dict with pre-loaded allowed modules.\"\"\"\n    import math, fractions, itertools, functools, collections, decimal, random, statistics\n    \n    safe_globals = {\n        \"__builtins__\": {\n            \"abs\": abs, \"all\": all, \"any\": any, \"bin\": bin, \"bool\": bool, \"chr\": chr,\n            \"dict\": dict, \"divmod\": divmod, \"enumerate\": enumerate, \"filter\": filter,\n            \"float\": float, \"frozenset\": frozenset, \"hex\": hex, \"int\": int,\n            \"isinstance\": isinstance, \"len\": len, \"list\": list, \"map\": map, \"max\": max,\n            \"min\": min, \"oct\": oct, \"ord\": ord, \"pow\": pow, \"print\": print, \"range\": range,\n            \"repr\": repr, \"reversed\": reversed, \"round\": round, \"set\": set, \"slice\": slice,\n            \"sorted\": sorted, \"str\": str, \"sum\": sum, \"tuple\": tuple, \"type\": type, \"zip\": zip,\n            \"True\": True, \"False\": False, \"None\": None, \"complex\": complex,\n        },\n        \"math\": math,\n        \"fractions\": fractions,\n        \"Fraction\": fractions.Fraction,\n        \"itertools\": itertools,\n        \"functools\": functools,\n        \"collections\": collections,\n        \"decimal\": decimal,\n        \"Decimal\": decimal.Decimal,\n        \"random\": random,\n        \"statistics\": statistics,\n    }\n    \n    try:\n        import sympy\n        safe_globals[\"sympy\"] = sympy\n    except ImportError:\n        pass\n    \n    return safe_globals\n\ndef strip_imports(code: str) -> str:\n    \"\"\"Remove import statements since modules are pre-loaded.\"\"\"\n    lines = code.split('\\n')\n    filtered = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped.startswith('import ') or stripped.startswith('from '):\n            skip = False\n            for mod in ALLOWED_MODULES:\n                if mod in stripped:\n                    skip = True\n                    break\n            if skip:\n                continue\n        filtered.append(line)\n    return '\\n'.join(filtered)\n\ndef run_python(code: str, timeout_sec: float = 10.0) -> Tuple[bool, str]:\n    \"\"\"Execute Python code in a sandboxed environment.\"\"\"\n    import signal\n    \n    code = strip_imports(code)\n    output_capture = io.StringIO()\n    safe_globals = create_safe_globals()\n    safe_locals = {}\n    \n    def timeout_handler(signum, frame):\n        raise TimeoutError(\"Code execution timed out\")\n    \n    old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n    signal.alarm(int(timeout_sec))\n    \n    try:\n        with redirect_stdout(output_capture), redirect_stderr(output_capture):\n            exec(code, safe_globals, safe_locals)\n        \n        output = output_capture.getvalue()\n        \n        for var_name in [\"result\", \"answer\", \"ans\", \"final\", \"output\"]:\n            if var_name in safe_locals:\n                val = safe_locals[var_name]\n                if output:\n                    output += f\"\\n{var_name} = {val}\"\n                else:\n                    output = f\"{var_name} = {val}\"\n                break\n        \n        signal.alarm(0)\n        signal.signal(signal.SIGALRM, old_handler)\n        return True, output if output else \"Execution completed (no output)\"\n        \n    except TimeoutError as e:\n        signal.alarm(0)\n        signal.signal(signal.SIGALRM, old_handler)\n        return False, f\"Timeout: {str(e)}\"\n    except Exception as e:\n        signal.alarm(0)\n        signal.signal(signal.SIGALRM, old_handler)\n        return False, f\"Error: {type(e).__name__}: {str(e)}\"\n\ndef parse_python_block(text: str) -> Optional[str]:\n    \"\"\"Extract Python code block from text.\"\"\"\n    patterns = [r\"```python\\s*\\n(.*?)```\", r\"```py\\s*\\n(.*?)```\", r\"```\\s*\\n(.*?)```\"]\n    for pattern in patterns:\n        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n        if match:\n            return match.group(1).strip()\n    return None\n\n# ============================================================\n# NEW STRICT FINAL PROMPT - Forces model to use FINAL: tag\n# ============================================================\ndef get_strict_final_prompt(problem: str) -> str:\n    \"\"\"\n    Two-pass style prompt that enforces strict FINAL: <integer> format.\n    This prevents the LASTINT fallback from grabbing wrong numbers.\n    \"\"\"\n    return f\"\"\"You are solving a mathematical olympiad problem. Follow these rules EXACTLY:\n\nPROBLEM:\n{problem}\n\nINSTRUCTIONS:\n1. Work through the problem step by step, showing your reasoning.\n2. If you need to compute something, you may use Python code in ```python ... ``` blocks.\n3. After completing your solution, you MUST end with EXACTLY ONE line in this format:\n\nFINAL: <your_integer_answer>\n\nCRITICAL RULES:\n- The FINAL line must contain ONLY the tag and a single non-negative integer\n- The answer must be between 0 and 99999\n- Do NOT put any other numbers on the FINAL line\n- Do NOT include units, commas, or explanations on the FINAL line\n- If the problem asks for a remainder mod N, give just that remainder\n\nExample of correct final line: FINAL: 42\nExample of INCORRECT: FINAL: The answer is 42\nExample of INCORRECT: FINAL: 42 (mod 1000)\n\nBEGIN YOUR SOLUTION:\"\"\"\n\ndef get_answer_only_prompt(problem: str, reasoning: str = \"\") -> str:\n    \"\"\"\n    Second pass: extract just the final answer from reasoning.\n    \"\"\"\n    return f\"\"\"Based on the following problem and solution, extract ONLY the final numerical answer.\n\nPROBLEM:\n{problem}\n\nSOLUTION/REASONING:\n{reasoning}\n\nWhat is the final integer answer? Respond with ONLY a single integer on one line, nothing else.\nFINAL:\"\"\"\n\ndef get_tir_prompt(problem: str) -> str:\n    \"\"\"Generate Tool-Integrated Reasoning prompt (original style).\"\"\"\n    return f\"\"\"You are a mathematical problem solver. Solve the following problem step by step.\n\nRULES:\n1. Think through the problem carefully.\n2. If you need to compute something, write Python code in a ```python ... ``` block.\n3. After your reasoning, provide your final answer as: FINAL: <integer>\n4. The answer must be an integer between 0 and 99999.\n\nPROBLEM:\n{problem}\n\nSOLUTION:\"\"\"\n\ndef get_concise_prompt(problem: str) -> str:\n    \"\"\"Generate concise direct-answer prompt.\"\"\"\n    return f\"\"\"Solve this math problem. Give only the final integer answer (0-99999).\n\nProblem: {problem}\n\nFINAL:\"\"\"\n\ndef get_prompt(problem: str, style: str = None) -> str:\n    \"\"\"Get prompt based on configured style.\"\"\"\n    style = style or PROMPT_STYLE\n    if style == \"strict_final\":\n        return get_strict_final_prompt(problem)\n    elif style == \"tir\":\n        return get_tir_prompt(problem)\n    elif style == \"concise\":\n        return get_concise_prompt(problem)\n    else:\n        return get_strict_final_prompt(problem)\n\nprint(\"Safe Python executor initialized\")\nprint(f\"Prompt style: {PROMPT_STYLE}\")\nprint(f\"Available modules: {ALLOWED_MODULES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL E \u2014 ANSWER EXTRACTION + VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL E \u2014 ANSWER EXTRACTION + VALIDATION (STRICT PRIORITY)\n# ================================\n\ndef clean_number_string(s: str) -> str:\n    \"\"\"Clean number string: remove commas, spaces, handle negatives.\"\"\"\n    s = s.strip()\n    s = s.replace(\",\", \"\").replace(\" \", \"\")\n    # Handle negative (take absolute value for this competition)\n    if s.startswith(\"-\"):\n        s = s[1:]\n    return s\n\ndef extract_answer(text: str) -> Tuple[Optional[int], str]:\n    \"\"\"\n    Extract integer answer from text with STRICT priority order.\n    \n    Priority (highest to lowest):\n    1. FINAL: <int> - our strict format\n    2. ANSWER: <int> - explicit answer tag\n    3. The answer is <int> - explicit statement\n    4. \\\\boxed{<int>} - LaTeX boxed\n    5. Last standalone integer (LASTINT) - fallback only\n    \"\"\"\n    if not text:\n        return None, \"empty\"\n    \n    text = text.strip()\n    \n    # Priority 1: FINAL: pattern (STRICT - our enforced format)\n    final_patterns = [\n        r\"FINAL\\s*:\\s*(\\d+)\\s*$\",  # At end of line\n        r\"FINAL\\s*:\\s*(\\d+)\",        # Anywhere\n        r\"^\\s*(\\d+)\\s*$\",            # Just a number on its own line (for answer-only prompts)\n    ]\n    \n    # Check each line from bottom up for FINAL pattern\n    lines = text.strip().split(\"\\n\")\n    for line in reversed(lines):\n        line = line.strip()\n        for pattern in final_patterns[:2]:  # Only FINAL patterns\n            match = re.search(pattern, line, re.IGNORECASE)\n            if match:\n                try:\n                    num_str = clean_number_string(match.group(1))\n                    return int(num_str), \"FINAL\"\n                except ValueError:\n                    continue\n    \n    # Priority 2: ANSWER: pattern\n    answer_patterns = [\n        r\"ANSWER\\s*:\\s*(\\d+)\",\n        r\"answer\\s*:\\s*(\\d+)\",\n        r\"Answer\\s*:\\s*(\\d+)\",\n        r\"final answer\\s*:\\s*(\\d+)\",\n        r\"Final answer\\s*:\\s*(\\d+)\",\n    ]\n    \n    for pattern in answer_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            try:\n                num_str = clean_number_string(match.group(1))\n                return int(num_str), \"ANSWER\"\n            except ValueError:\n                continue\n    \n    # Priority 3: \"The answer is <int>\" pattern\n    the_answer_patterns = [\n        r\"[Tt]he\\s+answer\\s+is\\s*:?\\s*(\\d+)\",\n        r\"[Tt]he\\s+final\\s+answer\\s+is\\s*:?\\s*(\\d+)\",\n        r\"[Aa]nswer\\s*=\\s*(\\d+)\",\n    ]\n    \n    for pattern in the_answer_patterns:\n        match = re.search(pattern, text)\n        if match:\n            try:\n                num_str = clean_number_string(match.group(1))\n                return int(num_str), \"THE_ANSWER_IS\"\n            except ValueError:\n                continue\n    \n    # Priority 4: \\boxed{} pattern (LaTeX)\n    boxed_patterns = [\n        r\"\\\\boxed\\{(\\d+)\\}\",\n        r\"\\$\\\\boxed\\{(\\d+)\\}\\$\",\n        r\"boxed\\{(\\d+)\\}\",\n    ]\n    \n    for pattern in boxed_patterns:\n        match = re.search(pattern, text)\n        if match:\n            try:\n                num_str = clean_number_string(match.group(1))\n                return int(num_str), \"BOXED\"\n            except ValueError:\n                continue\n    \n    # Priority 5: LASTINT fallback - but with more care\n    # Only consider integers that look like answers (not dates, counts, etc.)\n    # Look for integers at the end of the text or after \"=\" or \":\"\n    \n    # First try: integers after \"=\" or \"is\" near the end\n    late_text = text[-500:] if len(text) > 500 else text\n    late_patterns = [\n        r\"=\\s*(\\d+)\\s*$\",\n        r\"=\\s*(\\d+)\\s*[.\\n]\",\n        r\"is\\s+(\\d+)\\s*[.\\n]\",\n        r\"get\\s+(\\d+)\\s*[.\\n]\",\n    ]\n    \n    for pattern in late_patterns:\n        matches = list(re.finditer(pattern, late_text, re.IGNORECASE))\n        if matches:\n            try:\n                num_str = clean_number_string(matches[-1].group(1))\n                return int(num_str), \"LASTINT_CONTEXT\"\n            except ValueError:\n                continue\n    \n    # Final fallback: last integer in text\n    integers = re.findall(r\"\\b(\\d+)\\b\", text)\n    if integers:\n        try:\n            num_str = clean_number_string(integers[-1])\n            return int(num_str), \"LASTINT\"\n        except ValueError:\n            pass\n    \n    return None, \"none\"\n\ndef validate_answer(answer: Optional[int]) -> Tuple[bool, int]:\n    \"\"\"Validate and clamp answer to valid range [0, 99999].\"\"\"\n    if answer is None:\n        return False, 0\n    \n    if not isinstance(answer, (int, float)):\n        try:\n            answer = int(answer)\n        except (ValueError, TypeError):\n            return False, 0\n    \n    answer = int(answer)  # Ensure int type\n    \n    if 0 <= answer <= 99999:\n        return True, answer\n    \n    # Clamp to valid range\n    return False, max(0, min(99999, answer))\n\ndef safe_extract_answer(text: str) -> Tuple[int, Dict[str, Any]]:\n    \"\"\"Safely extract and validate answer, with fallback.\"\"\"\n    raw_answer, method = extract_answer(text)\n    is_valid, final_answer = validate_answer(raw_answer)\n    \n    metadata = {\n        \"raw_answer\": raw_answer,\n        \"method\": method,\n        \"is_valid\": is_valid,\n        \"fallback_used\": not is_valid or method in [\"LASTINT\", \"LASTINT_CONTEXT\"],\n    }\n    \n    if not is_valid:\n        final_answer = 0\n        metadata[\"fallback_value\"] = 0\n    \n    return final_answer, metadata\n\nprint(\"Answer extraction functions initialized (strict priority)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL F \u2014 CANDIDATE GENERATION + SELF-CONSISTENCY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL F \u2014 CANDIDATE GENERATION + SELF-CONSISTENCY (IMPROVED)\n# ================================\n\ndef generate_one(problem: str, temperature: float = 0.7, max_new_tokens: int = None, \n                 prompt_style: str = None, top_p: float = None, top_k: int = None) -> Tuple[str, Dict[str, Any]]:\n    \"\"\"Generate one solution candidate.\"\"\"\n    model, tokenizer, device = load_model()\n    \n    max_new_tokens = max_new_tokens or MAX_NEW_TOKENS\n    prompt_style = prompt_style or PROMPT_STYLE\n    top_p = top_p or TOP_P\n    top_k = top_k or TOP_K\n    \n    prompt = get_prompt(problem, prompt_style)\n    \n    meta = {\n        \"prompt_style\": prompt_style, \n        \"temperature\": temperature, \n        \"max_new_tokens\": max_new_tokens,\n        \"top_p\": top_p,\n        \"top_k\": top_k\n    }\n    \n    if model is None:\n        return \"\", meta\n    \n    try:\n        import torch\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        \n        gen_kwargs = {\n            \"max_new_tokens\": max_new_tokens,\n            \"pad_token_id\": tokenizer.eos_token_id,\n            \"eos_token_id\": tokenizer.eos_token_id,\n        }\n        \n        if temperature > 0:\n            gen_kwargs.update({\n                \"do_sample\": True,\n                \"temperature\": temperature,\n                \"top_p\": top_p,\n                \"top_k\": top_k,\n            })\n        else:\n            gen_kwargs[\"do_sample\"] = False\n        \n        with torch.no_grad():\n            outputs = model.generate(**inputs, **gen_kwargs)\n        \n        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Extract response after prompt\n        if prompt in generated:\n            response = generated[len(prompt):].strip()\n        else:\n            response = generated.strip()\n        \n        return response, meta\n        \n    except Exception as e:\n        meta[\"error\"] = str(e)\n        return \"\", meta\n\ndef execute_code_in_response(response: str) -> str:\n    \"\"\"Execute Python code blocks in response and append output.\"\"\"\n    code = parse_python_block(response)\n    if code:\n        ok, output = run_python(code)\n        if ok:\n            response += f\"\\n\\n[Code Output]\\n{output}\"\n        else:\n            response += f\"\\n\\n[Code Error]\\n{output}\"\n    return response\n\ndef generate_candidates(problem: str, k: int = None, temperature_schedule: List[float] = None,\n                       config: Dict = None) -> List[Dict[str, Any]]:\n    \"\"\"\n    Generate k solution candidates with optional temperature schedule.\n    \"\"\"\n    k = k or K_BASE\n    config = config or {}\n    \n    if temperature_schedule is None:\n        # Varied temperature schedule for diversity\n        base_temp = config.get(\"temperature\", TEMPERATURE_BASE)\n        temperature_schedule = [base_temp + 0.1 * (i % 3) for i in range(k)]\n    \n    candidates = []\n    \n    for i in range(k):\n        temp = temperature_schedule[i] if i < len(temperature_schedule) else TEMPERATURE_BASE\n        \n        raw_text, meta = generate_one(\n            problem, \n            temperature=temp,\n            max_new_tokens=config.get(\"max_new_tokens\", MAX_NEW_TOKENS),\n            prompt_style=config.get(\"prompt_style\", PROMPT_STYLE),\n            top_p=config.get(\"top_p\", TOP_P),\n            top_k=config.get(\"top_k\", TOP_K)\n        )\n        \n        # Execute any code in response\n        processed_text = execute_code_in_response(raw_text)\n        \n        # Extract answer\n        answer, answer_meta = safe_extract_answer(processed_text)\n        \n        candidates.append({\n            \"answer\": answer,\n            \"raw_text\": raw_text,\n            \"processed_text\": processed_text,\n            \"metadata\": {**meta, **answer_meta},\n            \"candidate_idx\": i,\n        })\n    \n    return candidates\n\ndef vote_candidates(candidates: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"Apply self-consistency voting to candidates.\"\"\"\n    if not candidates:\n        return {\"top_answer\": 0, \"top_count\": 0, \"total\": 0, \"vote_margin\": 0.0, \"entropy\": 0.0, \"answer_counts\": {}}\n    \n    answers = [c[\"answer\"] for c in candidates]\n    counter = Counter(answers)\n    total = len(answers)\n    \n    most_common = counter.most_common()\n    top_answer, top_count = most_common[0]\n    \n    second_count = most_common[1][1] if len(most_common) > 1 else 0\n    vote_margin = (top_count - second_count) / total\n    \n    # Calculate entropy for confidence estimation\n    entropy = 0.0\n    for count in counter.values():\n        p = count / total\n        if p > 0:\n            entropy -= p * math.log2(p)\n    \n    # Track which extraction methods were used\n    methods_used = Counter(c[\"metadata\"].get(\"method\", \"none\") for c in candidates)\n    \n    return {\n        \"top_answer\": top_answer,\n        \"top_count\": top_count,\n        \"total\": total,\n        \"vote_margin\": vote_margin,\n        \"entropy\": entropy,\n        \"answer_counts\": dict(counter.most_common(10)),\n        \"methods_used\": dict(methods_used),\n    }\n\ndef select_best_answer(candidates: List[Dict[str, Any]], vote_result: Dict[str, Any],\n                       strategy: str = None) -> Tuple[int, Dict[str, Any]]:\n    \"\"\"\n    Select best answer using configured strategy.\n    \"\"\"\n    strategy = strategy or SELECTION_STRATEGY\n    \n    if not candidates:\n        return 0, {\"strategy\": strategy, \"reason\": \"no_candidates\"}\n    \n    if strategy == \"majority_vote\":\n        return vote_result[\"top_answer\"], {\"strategy\": strategy, \"confidence\": vote_result[\"vote_margin\"]}\n    \n    elif strategy == \"verifier_weighted\":\n        # Weight by parse method reliability\n        method_weights = {\"FINAL\": 1.0, \"ANSWER\": 0.9, \"THE_ANSWER_IS\": 0.8, \"BOXED\": 0.7, \"LASTINT_CONTEXT\": 0.5, \"LASTINT\": 0.3}\n        \n        weighted_votes = Counter()\n        for c in candidates:\n            method = c[\"metadata\"].get(\"method\", \"LASTINT\")\n            weight = method_weights.get(method, 0.3)\n            weighted_votes[c[\"answer\"]] += weight\n        \n        best_answer = weighted_votes.most_common(1)[0][0]\n        return best_answer, {\"strategy\": strategy, \"weighted_votes\": dict(weighted_votes.most_common(5))}\n    \n    elif strategy == \"consensus\":\n        # Require stronger consensus\n        if vote_result[\"vote_margin\"] >= 0.5:\n            return vote_result[\"top_answer\"], {\"strategy\": strategy, \"reason\": \"strong_consensus\"}\n        else:\n            # Fall back to verifier weighted\n            return select_best_answer(candidates, vote_result, \"verifier_weighted\")\n    \n    else:\n        return vote_result[\"top_answer\"], {\"strategy\": \"fallback_majority\"}\n\ndef should_early_stop(vote_result: Dict[str, Any], threshold: float = None) -> bool:\n    \"\"\"Check if we should stop early based on voting confidence.\"\"\"\n    threshold = threshold or VOTING_THRESHOLD\n    if vote_result[\"total\"] == 0:\n        return False\n    return vote_result[\"top_count\"] / vote_result[\"total\"] >= threshold\n\nprint(\"Candidate generation functions initialized\")\nprint(f\"Selection strategy: {SELECTION_STRATEGY}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL G \u2014 VERIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL G \u2014 VERIFIER (Rule-based + Optional LLM)\n# ================================\n\ndef rule_verifier(problem: str, answer: int) -> Dict[str, Any]:\n    checks = []\n    passed = True\n    reason = \"OK\"\n    \n    if not (0 <= answer <= 99999):\n        passed = False\n        reason = f\"Answer {answer} out of valid range [0, 99999]\"\n        checks.append((\"range_check\", False, reason))\n    else:\n        checks.append((\"range_check\", True, \"In valid range\"))\n    \n    problem_lower = problem.lower()\n    if \"remainder\" in problem_lower or \"modulo\" in problem_lower or \"mod \" in problem_lower:\n        mod_patterns = [r\"divided by\\s+(\\d+)\", r\"modulo\\s+(\\d+)\", r\"mod\\s+(\\d+)\", r\"\\(mod\\s*(\\d+)\\)\"]\n        for pattern in mod_patterns:\n            match = re.search(pattern, problem_lower)\n            if match:\n                mod_val = int(match.group(1))\n                if answer >= mod_val and mod_val < 100000:\n                    checks.append((\"mod_check\", False, f\"Answer {answer} >= modulo {mod_val}\"))\n                else:\n                    checks.append((\"mod_check\", True, f\"Answer {answer} < modulo {mod_val}\"))\n                break\n    \n    return {\"passed\": passed, \"reason\": reason, \"checks\": checks}\n\ndef llm_verifier(problem: str, answer: int) -> Dict[str, Any]:\n    model, tokenizer, device = load_model()\n    \n    if model is None:\n        return {\"passed\": True, \"reason\": \"LLM not available\", \"response\": \"\"}\n    \n    prompt = f\"\"\"Given this math problem and proposed answer, quickly check if the answer could be correct.\nIf you find a clear error or contradiction, say INVALID. Otherwise say VALID.\n\nProblem: {problem}\n\nProposed Answer: {answer}\n\nVerification (VALID or INVALID):\"\"\"\n    \n    try:\n        import torch\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        \n        with torch.no_grad():\n            outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.1, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n        \n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()\n        passed = \"INVALID\" not in response.upper()\n        \n        return {\"passed\": passed, \"reason\": \"VALID\" if passed else \"INVALID found\", \"response\": response[:200]}\n        \n    except Exception as e:\n        return {\"passed\": True, \"reason\": f\"Error: {str(e)}\", \"response\": \"\"}\n\ndef verify_answer(problem: str, answer: int, use_llm: bool = False) -> Dict[str, Any]:\n    result = {\"rule_verifier\": rule_verifier(problem, answer), \"llm_verifier\": None, \"final_passed\": True}\n    \n    if not result[\"rule_verifier\"][\"passed\"]:\n        result[\"final_passed\"] = False\n    \n    if use_llm and result[\"rule_verifier\"][\"passed\"]:\n        result[\"llm_verifier\"] = llm_verifier(problem, answer)\n        if not result[\"llm_verifier\"][\"passed\"]:\n            result[\"final_passed\"] = False\n    \n    return result\n\nprint(\"Verifier functions initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL H \u2014 SOLVER ORCHESTRATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL H \u2014 SOLVER ORCHESTRATOR (IMPROVED)\n# ================================\n\ndef solve_problem(problem_id: str, problem_text: str, config: Dict = None) -> Tuple[int, Dict[str, Any]]:\n    \"\"\"\n    Main solver function with configurable parameters for tuning.\n    \"\"\"\n    global CURRENT_SEED\n    start_time = time.time()\n    config = config or {}\n    \n    # Set seed based on mode policy\n    CURRENT_SEED = get_seed_for_problem(problem_id)\n    set_seed(CURRENT_SEED)\n    \n    # Get parameters from config or defaults\n    k = config.get(\"k\", K_BASE)\n    temperature = config.get(\"temperature\", TEMPERATURE_BASE)\n    max_new_tokens = config.get(\"max_new_tokens\", MAX_NEW_TOKENS)\n    selection_strategy = config.get(\"selection_strategy\", SELECTION_STRATEGY)\n    \n    # Initialize telemetry\n    telemetry = {\n        \"id\": problem_id,\n        \"problem_hash\": hashlib.md5(problem_text.encode()).hexdigest()[:8],\n        \"elapsed_sec\": 0,\n        \"k_used\": 0,\n        \"candidates_summary\": [],\n        \"chosen_answer\": 0,\n        \"vote_margin\": 0.0,\n        \"vote_entropy\": 0.0,\n        \"verifier_used\": False,\n        \"verifier_pass\": True,\n        \"tool_calls_count\": 0,\n        \"parse_method_used\": \"none\",\n        \"methods_distribution\": {},\n        \"difficulty_mode\": \"NORMAL\",\n        \"temperature_used\": temperature,\n        \"seed_used\": CURRENT_SEED,\n        \"run_policy\": MODE_POLICY,\n        \"is_rerun\": IS_KAGGLE_RERUN,\n        \"model_available\": is_model_available(),\n        \"config_used\": config,\n        \"selection_strategy\": selection_strategy,\n    }\n    \n    try:\n        # Temperature schedule with variation\n        temp_schedule = [temperature + 0.1 * (i % 3) for i in range(k)]\n        telemetry[\"temperature_schedule\"] = temp_schedule[:5]  # Log first 5\n        \n        # Generate candidates with time budget awareness\n        candidates = []\n        time_per_candidate = (TIME_BUDGET_SEC_PER_PROBLEM * 0.7) / k\n        \n        for i in range(k):\n            elapsed = time.time() - start_time\n            remaining = TIME_BUDGET_SEC_PER_PROBLEM - elapsed\n            \n            if remaining < time_per_candidate * 0.5:\n                break  # Stop if not enough time for another candidate\n            \n            raw_text, meta = generate_one(\n                problem_text, \n                temperature=temp_schedule[i],\n                max_new_tokens=max_new_tokens,\n                prompt_style=config.get(\"prompt_style\", PROMPT_STYLE)\n            )\n            processed_text = execute_code_in_response(raw_text)\n            answer, answer_meta = safe_extract_answer(processed_text)\n            \n            candidates.append({\n                \"answer\": answer,\n                \"raw_text_preview\": raw_text[:500],\n                \"gen_metadata\": {**meta, **answer_meta}\n            })\n            \n            if parse_python_block(raw_text):\n                telemetry[\"tool_calls_count\"] += 1\n            \n            # Early stop check\n            if len(candidates) >= 3:\n                partial_vote = vote_candidates(candidates)\n                if should_early_stop(partial_vote, 0.8):  # Strong consensus\n                    break\n        \n        telemetry[\"k_used\"] = len(candidates)\n        \n        # Vote on candidates\n        vote_result = vote_candidates(candidates)\n        telemetry[\"vote_margin\"] = vote_result[\"vote_margin\"]\n        telemetry[\"vote_entropy\"] = vote_result[\"entropy\"]\n        telemetry[\"candidates_summary\"] = [(a, c) for a, c in vote_result[\"answer_counts\"].items()]\n        telemetry[\"methods_distribution\"] = vote_result.get(\"methods_used\", {})\n        \n        # Select best answer\n        chosen_answer, selection_info = select_best_answer(candidates, vote_result, selection_strategy)\n        telemetry[\"selection_info\"] = selection_info\n        \n        # Find parse method used for chosen answer\n        for c in candidates:\n            if c[\"answer\"] == chosen_answer:\n                telemetry[\"parse_method_used\"] = c[\"gen_metadata\"].get(\"method\", \"none\")\n                break\n        \n        # Optional verification for low confidence\n        if vote_result[\"vote_margin\"] < 0.3:\n            verification = verify_answer(problem_text, chosen_answer, use_llm=False)\n            telemetry[\"verifier_used\"] = True\n            telemetry[\"verifier_pass\"] = verification[\"final_passed\"]\n            \n            if not verification[\"final_passed\"]:\n                # Try next best answer\n                for answer, count in vote_result[\"answer_counts\"].items():\n                    if answer != chosen_answer:\n                        alt_verify = verify_answer(problem_text, answer, use_llm=False)\n                        if alt_verify[\"final_passed\"]:\n                            chosen_answer = answer\n                            telemetry[\"switched_answer\"] = True\n                            break\n        \n        # Final validation\n        is_valid, final_answer = validate_answer(chosen_answer)\n        if not is_valid:\n            telemetry[\"used_fallback\"] = True\n            final_answer = 0\n        \n        telemetry[\"chosen_answer\"] = final_answer\n        \n    except Exception as e:\n        telemetry[\"error_message\"] = str(e)\n        final_answer = 0\n        telemetry[\"chosen_answer\"] = final_answer\n        telemetry[\"used_fallback\"] = True\n    \n    telemetry[\"elapsed_sec\"] = time.time() - start_time\n    \n    return final_answer, telemetry\n\nprint(\"Solver orchestrator initialized (improved)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL I \u2014 TELEMETRY LOGGER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL I \u2014 TELEMETRY LOGGER\n# ================================\n\ndef append_jsonl(filepath: str, data: Dict[str, Any]):\n    os.makedirs(os.path.dirname(filepath), exist_ok=True) if os.path.dirname(filepath) else None\n    with open(filepath, \"a\") as f:\n        f.write(json.dumps(data, default=str) + \"\\n\")\n        f.flush()\n\ndef read_telemetry(filepath: str) -> List[Dict[str, Any]]:\n    entries = []\n    if os.path.exists(filepath):\n        with open(filepath, \"r\") as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    try:\n                        entries.append(json.loads(line))\n                    except json.JSONDecodeError:\n                        pass\n    return entries\n\ndef log_telemetry(telemetry: Dict[str, Any]):\n    append_jsonl(LOG_PATH, telemetry)\n\ndef print_telemetry_summary(telemetry_list: List[Dict[str, Any]]):\n    if not telemetry_list:\n        print(\"No telemetry data available\")\n        return\n    \n    n = len(telemetry_list)\n    total_time = sum(t.get(\"elapsed_sec\", 0) for t in telemetry_list)\n    avg_time = total_time / n if n > 0 else 0\n    \n    parse_methods = Counter(t.get(\"parse_method_used\", \"none\") for t in telemetry_list)\n    parse_fail_rate = parse_methods.get(\"none\", 0) / n if n > 0 else 0\n    \n    k_values = [t.get(\"k_used\", 0) for t in telemetry_list]\n    avg_k = sum(k_values) / n if n > 0 else 0\n    \n    verifier_used = sum(1 for t in telemetry_list if t.get(\"verifier_used\", False))\n    verifier_pass = sum(1 for t in telemetry_list if t.get(\"verifier_pass\", True))\n    \n    fallback_used = sum(1 for t in telemetry_list if t.get(\"fallback_used\", False))\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"TELEMETRY SUMMARY\")\n    print(\"=\"*50)\n    print(f\"Total problems: {n}\")\n    print(f\"Total time: {total_time:.2f}s\")\n    print(f\"Avg time per problem: {avg_time:.2f}s\")\n    print(f\"Parse fail rate: {parse_fail_rate:.2%}\")\n    print(f\"Avg k_used: {avg_k:.1f}\")\n    print(f\"K distribution: {Counter(k_values)}\")\n    print(f\"Parse methods: {dict(parse_methods)}\")\n    print(f\"Verifier usage: {verifier_used}/{n}\")\n    print(f\"Verifier pass rate: {verifier_pass}/{verifier_used if verifier_used > 0 else 1}\")\n    print(f\"Fallback used: {fallback_used}/{n}\")\n    print(\"=\"*50)\n\nprint(\"Telemetry logger initialized\")\nprint(f\"Log path: {LOG_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL J \u2014 LOCAL HARNESS (Reference CSV Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL J \u2014 LOCAL HARNESS (Reference CSV Regression)\n# ================================\n\ndef run_reference_eval(csv_path: str = None, limit: int = None) -> Dict[str, Any]:\n    csv_path = csv_path or REFERENCE_CSV_PATH\n    \n    if not os.path.exists(csv_path):\n        print(f\"Reference CSV not found: {csv_path}\")\n        return {\"error\": \"File not found\", \"accuracy\": 0.0}\n    \n    print(f\"\\nRunning reference evaluation on: {csv_path}\")\n    print(\"=\"*60)\n    \n    df = pd.read_csv(csv_path)\n    \n    if limit:\n        df = df.head(limit)\n    \n    n_problems = len(df)\n    print(f\"Evaluating {n_problems} problems...\\n\")\n    \n    results = []\n    correct = 0\n    telemetry_list = []\n    \n    for idx, row in df.iterrows():\n        problem_id = str(row[\"id\"])\n        problem_text = row[\"problem\"]\n        expected_answer = int(row[\"answer\"])\n        \n        print(f\"[{idx+1}/{n_problems}] Problem {problem_id}...\")\n        \n        predicted_answer, telemetry = solve_problem(problem_id, problem_text)\n        \n        telemetry[\"expected_answer\"] = expected_answer\n        telemetry[\"is_correct\"] = (predicted_answer == expected_answer)\n        log_telemetry(telemetry)\n        telemetry_list.append(telemetry)\n        \n        is_correct = (predicted_answer == expected_answer)\n        if is_correct:\n            correct += 1\n            status = \"Y\"\n        else:\n            status = \"X\"\n        \n        print(f\"  {status} Predicted: {predicted_answer}, Expected: {expected_answer} ({telemetry['elapsed_sec']:.2f}s)\")\n        \n        results.append({\n            \"id\": problem_id, \"predicted\": predicted_answer,\n            \"expected\": expected_answer, \"correct\": is_correct,\n            \"elapsed_sec\": telemetry[\"elapsed_sec\"],\n        })\n    \n    accuracy = correct / n_problems if n_problems > 0 else 0.0\n    \n    print(\"\\n\" + \"=\"*60)\n    print(f\"ACCURACY: {correct}/{n_problems} = {accuracy:.2%}\")\n    print(\"=\"*60)\n    \n    print_telemetry_summary(telemetry_list)\n    \n    return {\"accuracy\": accuracy, \"correct\": correct, \"total\": n_problems, \"results\": results}\n\nprint(\"Local harness initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL K \u2014 SUBMISSION GLUE (Kaggle Evaluation API)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL K \u2014 SUBMISSION GLUE (Kaggle Evaluation API)\n# ================================\n\nimport sys\nimport os\n\nkaggle_eval_paths = [\"/kaggle/input/kaggle-evaluation\", \"/kaggle/input\", \".\", \"..\"]\n\nfor path in kaggle_eval_paths:\n    if os.path.exists(os.path.join(path, \"kaggle_evaluation\")):\n        sys.path.insert(0, path)\n        break\n\ndef predict(test_input: Union[pd.DataFrame, dict, pd.Series]) -> pd.DataFrame:\n    \"\"\"\n    Kaggle prediction endpoint.\n    Accepts DataFrame, dict, or Series as input.\n    Returns DataFrame with columns 'id' and 'answer'.\n    \"\"\"\n    # Convert input to DataFrame if needed\n    if isinstance(test_input, dict):\n        test_df = pd.DataFrame([test_input])\n    elif isinstance(test_input, pd.Series):\n        test_df = pd.DataFrame([test_input.to_dict()])\n    elif isinstance(test_input, pd.DataFrame):\n        test_df = test_input\n    else:\n        raise ValueError(f\"predict() expects DataFrame, dict, or Series, got {type(test_input)}\")\n    \n    # Validate required columns\n    required_cols = {\"id\", \"problem\"}\n    missing_cols = required_cols - set(test_df.columns)\n    if missing_cols:\n        raise ValueError(f\"Input missing required columns: {missing_cols}\")\n    \n    results = []\n    \n    for idx, row in test_df.iterrows():\n        problem_id = str(row[\"id\"])\n        problem_text = str(row[\"problem\"])\n        \n        answer, telemetry = solve_problem(problem_id, problem_text)\n        log_telemetry(telemetry)\n        \n        results.append({\"id\": problem_id, \"answer\": int(answer)})\n    \n    return pd.DataFrame(results)\n\ndef setup_and_serve():\n    \"\"\"\n    Setup and start the inference server.\n    Handles missing run_local_gateway gracefully.\n    \"\"\"\n    try:\n        from kaggle_evaluation.aimo_3_inference_server import AIMO3InferenceServer\n        \n        server = AIMO3InferenceServer(predict)\n        \n        if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\") == \"1\":\n            print(\"Starting inference server (competition mode)...\")\n            server.serve()\n        else:\n            # Try run_local_gateway, but handle if it doesn't exist\n            print(\"Attempting local gateway test...\")\n            if hasattr(server, 'run_local_gateway'):\n                try:\n                    server.run_local_gateway()\n                except Exception as e:\n                    print(f\"run_local_gateway failed: {e}\")\n                    print(\"Falling back to direct serve()...\")\n                    server.serve()\n            else:\n                print(\"run_local_gateway not available, using serve()...\")\n                server.serve()\n            \n    except ImportError as e:\n        print(f\"kaggle_evaluation not available: {e}\")\n        print(\"Running in local-only mode\")\n    except Exception as e:\n        print(f\"Error in setup_and_serve: {e}\")\n        print(\"Running in local-only mode\")\n\nprint(\"Submission glue initialized\")\nprint(\"predict() accepts: DataFrame, dict, or Series\")\nprint(\"Use setup_and_serve() to start the server\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SELF TEST \u2014 Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# SELF TEST 1: Schema/Predict Unit Test\n# ================================\n\ndef test_predict_schema():\n    print(\"\\nTEST 1: Schema/Predict Unit Test\")\n    print(\"-\"*40)\n    \n    test_df = pd.DataFrame({\"id\": [\"test001\"], \"problem\": [\"What is $1+1$?\"]})\n    result_df = predict(test_df)\n    \n    assert \"id\" in result_df.columns, \"Missing 'id' column\"\n    assert \"answer\" in result_df.columns, \"Missing 'answer' column\"\n    assert len(result_df) == 1, f\"Expected 1 row, got {len(result_df)}\"\n    \n    answer = result_df[\"answer\"].iloc[0]\n    assert isinstance(answer, (int, type(1))), f\"Answer should be int, got {type(answer)}\"\n    assert 0 <= answer <= 99999, f\"Answer {answer} out of range [0, 99999]\"\n    \n    print(f\"OK Output schema correct\")\n    print(f\"OK Answer: {answer} (valid int in [0, 99999])\")\n    print(\"TEST 1 PASSED\\n\")\n    return True\n\nif RUN_MODE == \"local_ref\":\n    try:\n        test_predict_schema()\n    except AssertionError as e:\n        print(f\"TEST 1 FAILED: {e}\")\n    except Exception as e:\n        print(f\"TEST 1 ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# SELF TEST 2: Reference Evaluation\n# ================================\n\ndef test_reference_eval():\n    print(\"\\nTEST 2: Reference Evaluation\")\n    print(\"-\"*40)\n    \n    result = run_reference_eval(limit=2)\n    \n    assert \"accuracy\" in result, \"Missing 'accuracy' in result\"\n    assert \"total\" in result, \"Missing 'total' in result\"\n    assert result[\"total\"] == 2, f\"Expected 2 problems, got {result['total']}\"\n    \n    print(f\"OK Accuracy: {result['accuracy']:.2%}\")\n    print(f\"OK No crashes during evaluation\")\n    print(\"TEST 2 PASSED\\n\")\n    return True\n\nif RUN_MODE == \"local_ref\":\n    try:\n        if os.path.exists(REFERENCE_CSV_PATH):\n            test_reference_eval()\n        else:\n            print(\"Skipping TEST 2: reference.csv not found\")\n    except AssertionError as e:\n        print(f\"TEST 2 FAILED: {e}\")\n    except Exception as e:\n        print(f\"TEST 2 ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# SELF TEST 3: Tool Executor\n# ================================\n\ndef test_tool_executor():\n    print(\"\\nTEST 3: Tool Executor\")\n    print(\"-\"*40)\n    \n    # Test 1: Simple arithmetic\n    ok, output = run_python(\"result = 2 + 3\")\n    assert ok, f\"Execution failed: {output}\"\n    assert \"5\" in output, f\"Expected '5' in output, got: {output}\"\n    print(f\"OK Simple arithmetic: 2+3 = 5\")\n    \n    # Test 2: Math module (already loaded, no import needed)\n    ok, output = run_python(\"result = math.factorial(5)\")\n    assert ok, f\"Execution failed: {output}\"\n    assert \"120\" in output, f\"Expected '120' in output, got: {output}\"\n    print(f\"OK Math module: factorial(5) = 120\")\n    \n    # Test 3: Code with import statement (should be stripped)\n    ok, output = run_python(\"import math\\nresult = math.sqrt(16)\")\n    assert ok, f\"Execution failed: {output}\"\n    assert \"4\" in output, f\"Expected '4' in output, got: {output}\"\n    print(f\"OK Import stripping works: sqrt(16) = 4\")\n    \n    # Test 4: Timeout handling\n    ok, output = run_python(\"x = 1\", timeout_sec=1)\n    assert ok, f\"Simple code should not timeout\"\n    print(f\"OK Timeout handling works\")\n    \n    print(\"TEST 3 PASSED\\n\")\n    return True\n\nif RUN_MODE == \"local_ref\":\n    try:\n        test_tool_executor()\n    except AssertionError as e:\n        print(f\"TEST 3 FAILED: {e}\")\n    except Exception as e:\n        print(f\"TEST 3 ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# SELF TEST 4: Answer Extraction\n# ================================\n\ndef test_answer_extraction():\n    print(\"\\nTEST 4: Answer Extraction\")\n    print(\"-\"*40)\n    \n    test_cases = [\n        (\"ANSWER: 42\", 42, \"ANSWER\"),\n        (\"The answer is 123\", 123, \"THE_ANSWER_IS\"),\n        (\"After calculation, we get 456. ANSWER: 456\", 456, \"ANSWER\"),\n        (\"Result: 789\", 789, \"LASTINT\"),\n        (\"The final answer is 999\", 999, \"THE_ANSWER_IS\"),\n    ]\n    \n    for text, expected_answer, expected_method in test_cases:\n        answer, method = extract_answer(text)\n        assert answer == expected_answer, f\"Expected {expected_answer}, got {answer} for '{text}'\"\n        assert method == expected_method, f\"Expected method {expected_method}, got {method} for '{text}'\"\n        print(f\"OK '{text[:30]}...' -> {answer} ({method})\")\n    \n    print(\"TEST 4 PASSED\\n\")\n    return True\n\nif RUN_MODE == \"local_ref\":\n    try:\n        test_answer_extraction()\n    except AssertionError as e:\n        print(f\"TEST 4 FAILED: {e}\")\n    except Exception as e:\n        print(f\"TEST 4 ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# SELF TEST 5: predict() Input Types\n# ================================\n\ndef test_predict_input_types():\n    print(\"\\nTEST 5: predict() Input Types\")\n    print(\"-\"*40)\n    \n    # Test 1: DataFrame input\n    df_input = pd.DataFrame({\"id\": [\"test_df\"], \"problem\": [\"What is $2+2$?\"]})\n    result = predict(df_input)\n    assert isinstance(result, pd.DataFrame), \"Result should be DataFrame\"\n    assert \"answer\" in result.columns, \"Missing 'answer' column\"\n    print(\"OK DataFrame input works\")\n    \n    # Test 2: dict input\n    dict_input = {\"id\": \"test_dict\", \"problem\": \"What is $3+3$?\"}\n    result = predict(dict_input)\n    assert isinstance(result, pd.DataFrame), \"Result should be DataFrame\"\n    assert len(result) == 1, \"Should have 1 row\"\n    print(\"OK dict input works\")\n    \n    # Test 3: Series input\n    series_input = pd.Series({\"id\": \"test_series\", \"problem\": \"What is $4+4$?\"})\n    result = predict(series_input)\n    assert isinstance(result, pd.DataFrame), \"Result should be DataFrame\"\n    assert len(result) == 1, \"Should have 1 row\"\n    print(\"OK Series input works\")\n    \n    print(\"TEST 5 PASSED\\n\")\n    return True\n\nif RUN_MODE == \"local_ref\":\n    try:\n        test_predict_input_types()\n    except AssertionError as e:\n        print(f\"TEST 5 FAILED: {e}\")\n    except Exception as e:\n        print(f\"TEST 5 ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TUNING MODULE \u2014 Optuna Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# TUNING MODULE \u2014 Optuna Hyperparameter Optimization\n# ================================\n\ndef run_optuna_tuning(n_trials: int = 10, timeout: int = 3600) -> Dict[str, Any]:\n    \"\"\"\n    Run Optuna hyperparameter tuning on reference.csv subset.\n    \n    Search space:\n    - k: number of candidates (4-16)\n    - temperature: sampling temperature (0.3-1.0)\n    - max_new_tokens: generation length (1024-4096)\n    - prompt_style: prompt template type\n    - selection_strategy: answer selection method\n    - top_p: nucleus sampling (0.8-1.0)\n    \"\"\"\n    try:\n        import optuna\n        from optuna.pruners import MedianPruner\n        from optuna.samplers import TPESampler\n    except ImportError:\n        print(\"Installing optuna...\")\n        import subprocess\n        subprocess.run([\"pip\", \"install\", \"optuna\", \"-q\"])\n        import optuna\n        from optuna.pruners import MedianPruner\n        from optuna.samplers import TPESampler\n    \n    # Load reference data\n    if not os.path.exists(REFERENCE_CSV_PATH):\n        print(f\"Reference CSV not found: {REFERENCE_CSV_PATH}\")\n        return {\"error\": \"File not found\"}\n    \n    df = pd.read_csv(REFERENCE_CSV_PATH)\n    n_problems = len(df)\n    print(f\"Tuning on {n_problems} reference problems\")\n    \n    def objective(trial):\n        \"\"\"Optuna objective function.\"\"\"\n        # Sample hyperparameters\n        config = {\n            \"k\": trial.suggest_int(\"k\", 4, 12),\n            \"temperature\": trial.suggest_float(\"temperature\", 0.3, 0.9),\n            \"max_new_tokens\": trial.suggest_int(\"max_new_tokens\", 1024, 3072, step=512),\n            \"prompt_style\": trial.suggest_categorical(\"prompt_style\", [\"strict_final\", \"tir\"]),\n            \"selection_strategy\": trial.suggest_categorical(\"selection_strategy\", \n                                                            [\"majority_vote\", \"verifier_weighted\", \"consensus\"]),\n            \"top_p\": trial.suggest_float(\"top_p\", 0.85, 0.98),\n        }\n        \n        # Evaluate on reference problems\n        correct = 0\n        total_time = 0\n        \n        for idx, row in df.iterrows():\n            problem_id = str(row[\"id\"])\n            problem_text = row[\"problem\"]\n            expected = int(row[\"answer\"])\n            \n            predicted, telemetry = solve_problem(problem_id, problem_text, config=config)\n            \n            if predicted == expected:\n                correct += 1\n            \n            total_time += telemetry.get(\"elapsed_sec\", 0)\n            \n            # Report intermediate value for pruning\n            trial.report(correct / (idx + 1), idx)\n            \n            # Pruning: stop if clearly bad\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n        \n        accuracy = correct / n_problems\n        avg_time = total_time / n_problems\n        \n        # Penalize slow configs\n        time_penalty = max(0, (avg_time - TIME_BUDGET_SEC_PER_PROBLEM) / TIME_BUDGET_SEC_PER_PROBLEM)\n        score = accuracy - 0.1 * time_penalty\n        \n        return score\n    \n    # Create study\n    sampler = TPESampler(seed=BASE_SEED)\n    pruner = MedianPruner(n_startup_trials=3, n_warmup_steps=2)\n    \n    study = optuna.create_study(\n        direction=\"maximize\",\n        sampler=sampler,\n        pruner=pruner,\n        study_name=\"aimo3_tuning\"\n    )\n    \n    print(f\"Starting Optuna tuning: {n_trials} trials, {timeout}s timeout\")\n    \n    try:\n        study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True)\n    except KeyboardInterrupt:\n        print(\"Tuning interrupted\")\n    \n    # Get best config\n    best_config = study.best_params\n    best_score = study.best_value\n    \n    print(f\"\\nBest config (score={best_score:.4f}):\")\n    for k, v in best_config.items():\n        print(f\"  {k}: {v}\")\n    \n    # Save best config\n    result = {\n        \"best_config\": best_config,\n        \"best_score\": best_score,\n        \"n_trials\": len(study.trials),\n        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n    }\n    \n    with open(BEST_CONFIG_PATH, \"w\") as f:\n        json.dump(result, f, indent=2)\n    \n    print(f\"\\nBest config saved to: {BEST_CONFIG_PATH}\")\n    \n    return result\n\ndef load_best_config() -> Dict[str, Any]:\n    \"\"\"Load best config from previous tuning run.\"\"\"\n    if os.path.exists(BEST_CONFIG_PATH):\n        with open(BEST_CONFIG_PATH) as f:\n            data = json.load(f)\n        print(f\"Loaded best config (score={data.get('best_score', 'N/A')})\")\n        return data.get(\"best_config\", {})\n    else:\n        print(\"No saved config found, using defaults\")\n        return {}\n\nprint(\"Tuning module initialized\")\nprint(f\"Quick tune: {TUNE_TRIALS_QUICK} trials\")\nprint(f\"Full tune: {TUNE_TRIALS_FULL} trials\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MAIN EXECUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# MAIN EXECUTION\n# ================================\n\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\"*60)\n    print(\"AIMO3 BASELINE NOTEBOOK - IMPROVED\")\n    print(\"=\"*60)\n    print(f\"RUN_MODE: {RUN_MODE}\")\n    print(f\"CURRENT_SEED: {CURRENT_SEED}\")\n    print(f\"IS_KAGGLE_RERUN: {IS_KAGGLE_RERUN}\")\n    print(f\"PROMPT_STYLE: {PROMPT_STYLE}\")\n    print(f\"K_BASE: {K_BASE}, TEMPERATURE_BASE: {TEMPERATURE_BASE}\")\n    print(\"=\"*60 + \"\\n\")\n    \n    # Try to load best config from tuning\n    best_config = load_best_config() if os.path.exists(BEST_CONFIG_PATH) else {}\n    if best_config:\n        print(f\"Using tuned config: {best_config}\")\n    \n    if RUN_MODE == \"local_ref\":\n        print(\"Running in LOCAL/DEBUG mode...\")\n        print(\"Evaluating reference.csv...\\n\")\n        \n        if os.path.exists(LOG_PATH):\n            os.remove(LOG_PATH)\n        \n        if os.path.exists(REFERENCE_CSV_PATH):\n            result = run_reference_eval()\n            print(f\"\\nFinal Accuracy: {result['accuracy']:.2%}\")\n        else:\n            print(f\"Reference CSV not found: {REFERENCE_CSV_PATH}\")\n            print(\"Running self-tests only...\")\n        \n    elif RUN_MODE == \"quick_tune\":\n        print(\"Running QUICK TUNE mode...\")\n        result = run_optuna_tuning(n_trials=TUNE_TRIALS_QUICK, timeout=TUNE_TIMEOUT_SEC // 2)\n        \n        # Validate with best config\n        if \"best_config\" in result:\n            print(\"\\nValidating with best config...\")\n            # Re-run reference eval with best config\n            # (would need to pass config through, simplified here)\n    \n    elif RUN_MODE == \"full_tune\":\n        print(\"Running FULL TUNE mode...\")\n        result = run_optuna_tuning(n_trials=TUNE_TRIALS_FULL, timeout=TUNE_TIMEOUT_SEC)\n        \n    elif RUN_MODE == \"submit_auto\":\n        print(\"Running in SUBMISSION mode...\")\n        print(\"Starting inference server...\\n\")\n        setup_and_serve()\n    \n    else:\n        print(f\"Unknown RUN_MODE: {RUN_MODE}\")\n        print(\"Valid options: 'local_ref', 'submit_auto', 'quick_tune', 'full_tune'\")\n\nprint(\"\\nNotebook execution complete.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}