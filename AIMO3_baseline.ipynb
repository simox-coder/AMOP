{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AIMO3 Baseline Notebook\n## AI Mathematical Olympiad \u2013 Progress Prize 3\n\n### Quick Start Guide\n1. **RUN_MODE Selection**:\n   - `\"local_ref\"`: Debug mode - runs evaluation on `reference.csv` (10 problems)\n   - `\"submit_auto\"`: Kaggle submission mode - uses `kaggle_evaluation` API\n\n2. **Model Setup (Kaggle)**:\n   - Add your model as a Kaggle Dataset/Model input\n   - Update `MODEL_PATH` in CONFIG to point to `/kaggle/input/your-model-name`\n   - Or use Kaggle's built-in models\n\n3. **Telemetry**:\n   - Logs saved to `/kaggle/working/aimo3_telemetry.jsonl`\n   - Download after submission for analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL A \u2014 CONFIG (Constants)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL A \u2014 CONFIG (Constants)\n# ================================\n\nimport os\n\n# ----- RUN MODE -----\n# \"local_ref\": Run evaluation on reference.csv (10 problems) - for debugging\n# \"submit_auto\": Kaggle submission mode - uses kaggle_evaluation API\nRUN_MODE = \"local_ref\"  # Change to \"submit_auto\" for Kaggle submission\n\n# ----- TIME BUDGET -----\nTIME_BUDGET_SEC_PER_PROBLEM = 120  # seconds per problem\n\n# ----- GENERATION PARAMS -----\nK_BASE = 4              # Base number of candidates for easy problems\nK_MAX_HARD = 8          # Max candidates for hard problems\nTEMPERATURE_BASE = 0.3  # Temperature for stable generation\nTEMPERATURE_HARD = 0.7  # Temperature for exploration\nMAX_NEW_TOKENS = 2048   # Max tokens per generation\n\n# ----- VOTING & EARLY STOP -----\nVOTING_THRESHOLD = 0.6  # Stop early if top answer >= this fraction\n\n# ----- PATHS -----\nLOG_PATH = \"/kaggle/working/aimo3_telemetry.jsonl\"\nCACHE_DIR = \"/kaggle/working/cache\"\n\n# ----- MODEL CONFIG -----\nMODEL_PATH = None  # Set to model path when available\nMODEL_ID = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"  # Fallback model ID\n\n# ----- PROMPT STYLE -----\nPROMPT_STYLE = \"tir\"  # \"tir\", \"concise\", or \"explore\"\n\n# ----- MODE POLICY -----\nMODE_POLICY = \"stable\"  # \"stable\" or \"diverse\"\n\n# ----- DATA PATHS -----\nREFERENCE_CSV_PATH = None\nTEST_CSV_PATH = None\n\n# Detect environment and set paths\nif os.path.exists(\"/kaggle/input\"):\n    REFERENCE_CSV_PATH = \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\"\n    TEST_CSV_PATH = \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv\"\n    if not os.path.exists(REFERENCE_CSV_PATH):\n        REFERENCE_CSV_PATH = \"reference.csv\"\n        TEST_CSV_PATH = \"test.csv\"\nelse:\n    REFERENCE_CSV_PATH = \"reference.csv\"\n    TEST_CSV_PATH = \"test.csv\"\n\n# Create working directories\nos.makedirs(os.path.dirname(LOG_PATH), exist_ok=True) if os.path.dirname(LOG_PATH) else None\nos.makedirs(CACHE_DIR, exist_ok=True) if CACHE_DIR else None\n\nprint(f\"RUN_MODE: {RUN_MODE}\")\nprint(f\"REFERENCE_CSV_PATH: {REFERENCE_CSV_PATH}\")\nprint(f\"LOG_PATH: {LOG_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL B \u2014 IMPORTS + SEED CONTROL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL B \u2014 IMPORTS + SEED CONTROL\n# ================================\n\nimport os\nimport re\nimport sys\nimport time\nimport json\nimport math\nimport random\nimport hashlib\nimport warnings\nfrom typing import Optional, Tuple, List, Dict, Any\nfrom collections import Counter\nfrom contextlib import redirect_stdout, redirect_stderr\nimport io\n\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\n# ----- SEED CONTROL -----\nBASE_SEED = 42\nIS_KAGGLE_RERUN = os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\") is not None\n\ndef get_seed_for_run():\n    if IS_KAGGLE_RERUN:\n        session_seed = int(time.time()) % 100000\n        return BASE_SEED + session_seed\n    else:\n        return BASE_SEED\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    try:\n        import numpy as np\n        np.random.seed(seed)\n    except ImportError:\n        pass\n    try:\n        import torch\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(seed)\n    except ImportError:\n        pass\n\nCURRENT_SEED = get_seed_for_run()\nset_seed(CURRENT_SEED)\n\nprint(f\"IS_KAGGLE_RERUN: {IS_KAGGLE_RERUN}\")\nprint(f\"CURRENT_SEED: {CURRENT_SEED}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL C \u2014 LAZY MODEL LOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL C \u2014 LAZY MODEL LOADER\n# ================================\n\n_model_cache = {\"model\": None, \"tokenizer\": None, \"device\": None, \"loaded\": False}\n\ndef get_device():\n    try:\n        import torch\n        if torch.cuda.is_available():\n            return \"cuda\"\n        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n            return \"mps\"\n    except ImportError:\n        pass\n    return \"cpu\"\n\ndef load_model():\n    global _model_cache\n    \n    if _model_cache[\"loaded\"]:\n        return _model_cache[\"model\"], _model_cache[\"tokenizer\"], _model_cache[\"device\"]\n    \n    print(\"Loading model...\")\n    start_time = time.time()\n    \n    try:\n        import torch\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n        \n        device = get_device()\n        _model_cache[\"device\"] = device\n        print(f\"Using device: {device}\")\n        \n        model_source = MODEL_PATH if MODEL_PATH and os.path.exists(MODEL_PATH) else MODEL_ID\n        print(f\"Loading from: {model_source}\")\n        \n        _model_cache[\"tokenizer\"] = AutoTokenizer.from_pretrained(\n            model_source, trust_remote_code=True,\n            local_files_only=(MODEL_PATH is not None and os.path.exists(MODEL_PATH))\n        )\n        \n        dtype = torch.float16 if device == \"cuda\" else torch.float32\n        _model_cache[\"model\"] = AutoModelForCausalLM.from_pretrained(\n            model_source, torch_dtype=dtype,\n            device_map=\"auto\" if device == \"cuda\" else None,\n            trust_remote_code=True,\n            local_files_only=(MODEL_PATH is not None and os.path.exists(MODEL_PATH))\n        )\n        \n        if device != \"cuda\":\n            _model_cache[\"model\"] = _model_cache[\"model\"].to(device)\n        \n        _model_cache[\"loaded\"] = True\n        print(f\"Model loaded in {time.time() - start_time:.2f}s\")\n        \n    except Exception as e:\n        print(f\"Warning: Could not load model: {e}\")\n        print(\"Using rule-based solver...\")\n        _model_cache[\"model\"] = None\n        _model_cache[\"tokenizer\"] = None\n        _model_cache[\"device\"] = \"cpu\"\n        _model_cache[\"loaded\"] = True\n    \n    return _model_cache[\"model\"], _model_cache[\"tokenizer\"], _model_cache[\"device\"]\n\ndef is_model_available():\n    if _model_cache[\"loaded\"]:\n        return _model_cache[\"model\"] is not None\n    if MODEL_PATH and os.path.exists(MODEL_PATH):\n        return True\n    return False\n\nprint(\"Model loader initialized (lazy loading)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL D \u2014 TOOL-INTEGRATED REASONING + SAFE PYTHON EXECUTOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL D \u2014 TIR-lite + SAFE PYTHON EXECUTOR\n# ================================\n\nALLOWED_MODULES = {\"math\", \"fractions\", \"itertools\", \"functools\", \"collections\", \"decimal\", \"numbers\", \"cmath\", \"random\", \"statistics\"}\n\ntry:\n    import sympy\n    ALLOWED_MODULES.add(\"sympy\")\nexcept ImportError:\n    pass\n\ndef create_safe_globals():\n    \"\"\"Create a safe globals dict with pre-loaded allowed modules.\"\"\"\n    import math, fractions, itertools, functools, collections, decimal, random, statistics\n    \n    safe_globals = {\n        \"__builtins__\": {\n            \"abs\": abs, \"all\": all, \"any\": any, \"bin\": bin, \"bool\": bool, \"chr\": chr,\n            \"dict\": dict, \"divmod\": divmod, \"enumerate\": enumerate, \"filter\": filter,\n            \"float\": float, \"frozenset\": frozenset, \"hex\": hex, \"int\": int,\n            \"isinstance\": isinstance, \"len\": len, \"list\": list, \"map\": map, \"max\": max,\n            \"min\": min, \"oct\": oct, \"ord\": ord, \"pow\": pow, \"print\": print, \"range\": range,\n            \"repr\": repr, \"reversed\": reversed, \"round\": round, \"set\": set, \"slice\": slice,\n            \"sorted\": sorted, \"str\": str, \"sum\": sum, \"tuple\": tuple, \"type\": type, \"zip\": zip,\n            \"True\": True, \"False\": False, \"None\": None, \"complex\": complex,\n        },\n        # Pre-loaded modules (no import needed)\n        \"math\": math,\n        \"fractions\": fractions,\n        \"Fraction\": fractions.Fraction,\n        \"itertools\": itertools,\n        \"functools\": functools,\n        \"collections\": collections,\n        \"decimal\": decimal,\n        \"Decimal\": decimal.Decimal,\n        \"random\": random,\n        \"statistics\": statistics,\n    }\n    \n    try:\n        import sympy\n        safe_globals[\"sympy\"] = sympy\n    except ImportError:\n        pass\n    \n    return safe_globals\n\ndef strip_imports(code: str) -> str:\n    \"\"\"Remove import statements since modules are pre-loaded.\"\"\"\n    lines = code.split('\\n')\n    filtered = []\n    for line in lines:\n        stripped = line.strip()\n        # Skip import lines for allowed modules\n        if stripped.startswith('import ') or stripped.startswith('from '):\n            # Check if it's importing an allowed module\n            skip = False\n            for mod in ALLOWED_MODULES:\n                if mod in stripped:\n                    skip = True\n                    break\n            if skip:\n                continue\n        filtered.append(line)\n    return '\\n'.join(filtered)\n\ndef run_python(code: str, timeout_sec: float = 10.0) -> Tuple[bool, str]:\n    \"\"\"Execute Python code in a sandboxed environment.\"\"\"\n    import signal\n    \n    # Strip import statements for pre-loaded modules\n    code = strip_imports(code)\n    \n    output_capture = io.StringIO()\n    safe_globals = create_safe_globals()\n    safe_locals = {}\n    \n    def timeout_handler(signum, frame):\n        raise TimeoutError(\"Code execution timed out\")\n    \n    old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n    signal.alarm(int(timeout_sec))\n    \n    try:\n        with redirect_stdout(output_capture), redirect_stderr(output_capture):\n            exec(code, safe_globals, safe_locals)\n        \n        output = output_capture.getvalue()\n        \n        # Capture result variables\n        for var_name in [\"result\", \"answer\", \"ans\", \"final\", \"output\"]:\n            if var_name in safe_locals:\n                val = safe_locals[var_name]\n                if output:\n                    output += f\"\\n{var_name} = {val}\"\n                else:\n                    output = f\"{var_name} = {val}\"\n                break\n        \n        signal.alarm(0)\n        signal.signal(signal.SIGALRM, old_handler)\n        return True, output if output else \"Execution completed (no output)\"\n        \n    except TimeoutError as e:\n        signal.alarm(0)\n        signal.signal(signal.SIGALRM, old_handler)\n        return False, f\"Timeout: {str(e)}\"\n    except Exception as e:\n        signal.alarm(0)\n        signal.signal(signal.SIGALRM, old_handler)\n        return False, f\"Error: {type(e).__name__}: {str(e)}\"\n\ndef parse_python_block(text: str) -> Optional[str]:\n    \"\"\"Extract Python code block from text.\"\"\"\n    patterns = [r\"```python\\s*\\n(.*?)```\", r\"```py\\s*\\n(.*?)```\", r\"```\\s*\\n(.*?)```\"]\n    for pattern in patterns:\n        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n        if match:\n            return match.group(1).strip()\n    return None\n\ndef get_tir_prompt(problem: str) -> str:\n    \"\"\"Generate Tool-Integrated Reasoning prompt.\"\"\"\n    return f\"\"\"You are a mathematical problem solver. Solve the following problem step by step.\n\nRULES:\n1. Think through the problem carefully.\n2. If you need to compute something, write Python code in a ```python ... ``` block.\n3. After your reasoning, provide your final answer on a new line as: ANSWER: <integer>\n4. The answer must be an integer between 0 and 99999.\n5. If the problem asks for a remainder when divided by some number, compute that remainder.\n\nPROBLEM:\n{problem}\n\nSOLUTION:\"\"\"\n\ndef get_concise_prompt(problem: str) -> str:\n    \"\"\"Generate concise direct-answer prompt.\"\"\"\n    return f\"\"\"Solve this math problem. Give only the final integer answer (0-99999).\n\nProblem: {problem}\n\nANSWER:\"\"\"\n\ndef get_explore_prompt(problem: str) -> str:\n    \"\"\"Generate exploration prompt with more reasoning.\"\"\"\n    return f\"\"\"You are an expert mathematician. Carefully analyze this problem and explore multiple approaches.\n\nProblem:\n{problem}\n\nInstructions:\n1. Identify the key mathematical concepts involved.\n2. Consider multiple solution approaches.\n3. Use Python code (```python ... ```) for complex calculations.\n4. Verify your answer if possible.\n5. End with: ANSWER: <integer> (must be 0-99999)\n\nLet's solve this step by step:\"\"\"\n\nprint(\"Safe Python executor initialized\")\nprint(f\"Available modules: {ALLOWED_MODULES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL E \u2014 ANSWER EXTRACTION + VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL E \u2014 ANSWER EXTRACTION + VALIDATION\n# ================================\n\ndef extract_answer(text: str) -> Tuple[Optional[int], str]:\n    if not text:\n        return None, \"empty\"\n    \n    text = text.strip()\n    \n    answer_patterns = [\n        r\"ANSWER\\s*:\\s*(\\d+)\", r\"answer\\s*:\\s*(\\d+)\", r\"Answer\\s*:\\s*(\\d+)\",\n        r\"The answer is\\s*:\\s*(\\d+)\", r\"The answer is\\s+(\\d+)\",\n        r\"final answer\\s*:\\s*(\\d+)\", r\"Final answer\\s*:\\s*(\\d+)\",\n    ]\n    \n    for pattern in answer_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            try:\n                return int(match.group(1)), \"ANSWER\"\n            except ValueError:\n                continue\n    \n    boxed_patterns = [r\"\\\\\\\\boxed\\{(\\d+)\\}\", r\"\\\\boxed\\{(\\d+)\\}\", r\"\\$\\\\\\\\boxed\\{(\\d+)\\}\\$\", r\"\\$\\\\boxed\\{(\\d+)\\}\\$\"]\n    \n    for pattern in boxed_patterns:\n        match = re.search(pattern, text)\n        if match:\n            try:\n                return int(match.group(1)), \"BOXED\"\n            except ValueError:\n                continue\n    \n    integers = re.findall(r\"\\b(\\d+)\\b\", text)\n    if integers:\n        try:\n            return int(integers[-1]), \"LASTINT\"\n        except ValueError:\n            pass\n    \n    return None, \"none\"\n\ndef validate_answer(answer: Optional[int]) -> Tuple[bool, int]:\n    if answer is None:\n        return False, 0\n    \n    if not isinstance(answer, int):\n        try:\n            answer = int(answer)\n        except (ValueError, TypeError):\n            return False, 0\n    \n    if 0 <= answer <= 99999:\n        return True, answer\n    \n    return False, max(0, min(99999, answer))\n\ndef safe_extract_answer(text: str) -> Tuple[int, Dict[str, Any]]:\n    raw_answer, method = extract_answer(text)\n    is_valid, final_answer = validate_answer(raw_answer)\n    \n    metadata = {\n        \"raw_answer\": raw_answer, \"method\": method,\n        \"is_valid\": is_valid, \"fallback_used\": not is_valid,\n    }\n    \n    if not is_valid:\n        final_answer = 0\n        metadata[\"fallback_value\"] = 0\n    \n    return final_answer, metadata\n\nprint(\"Answer extraction functions initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL F \u2014 CANDIDATE GENERATION + SELF-CONSISTENCY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL F \u2014 CANDIDATE GENERATION + SELF-CONSISTENCY\n# ================================\n\ndef generate_one(problem: str, temperature: float = 0.3, max_new_tokens: int = MAX_NEW_TOKENS, prompt_style: str = \"tir\") -> Tuple[str, Dict[str, Any]]:\n    model, tokenizer, device = load_model()\n    \n    if prompt_style == \"tir\":\n        prompt = get_tir_prompt(problem)\n    elif prompt_style == \"concise\":\n        prompt = get_concise_prompt(problem)\n    else:\n        prompt = get_explore_prompt(problem)\n    \n    meta = {\"prompt_style\": prompt_style, \"temperature\": temperature, \"max_new_tokens\": max_new_tokens}\n    \n    if model is None:\n        return \"\", meta\n    \n    try:\n        import torch\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs, max_new_tokens=max_new_tokens,\n                temperature=temperature if temperature > 0 else 1.0,\n                do_sample=temperature > 0, top_p=0.95,\n                pad_token_id=tokenizer.eos_token_id,\n            )\n        \n        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        response = generated[len(prompt):].strip() if prompt in generated else generated.strip()\n        return response, meta\n        \n    except Exception as e:\n        meta[\"error\"] = str(e)\n        return \"\", meta\n\ndef execute_code_in_response(response: str) -> str:\n    code = parse_python_block(response)\n    if code:\n        ok, output = run_python(code)\n        if ok:\n            response += f\"\\n\\n[Code Output]\\n{output}\"\n    return response\n\ndef generate_candidates(problem: str, k: int = K_BASE, temperature_schedule: List[float] = None) -> List[Dict[str, Any]]:\n    if temperature_schedule is None:\n        temperature_schedule = [TEMPERATURE_BASE] * k\n    \n    candidates = []\n    \n    for i in range(k):\n        temp = temperature_schedule[i] if i < len(temperature_schedule) else TEMPERATURE_BASE\n        raw_text, meta = generate_one(problem, temperature=temp, prompt_style=PROMPT_STYLE)\n        processed_text = execute_code_in_response(raw_text)\n        answer, answer_meta = safe_extract_answer(processed_text)\n        \n        candidates.append({\n            \"answer\": answer, \"raw_text\": raw_text,\n            \"processed_text\": processed_text, \"metadata\": {**meta, **answer_meta},\n        })\n    \n    return candidates\n\ndef vote_candidates(candidates: List[Dict[str, Any]]) -> Dict[str, Any]:\n    if not candidates:\n        return {\"top_answer\": 0, \"top_count\": 0, \"total\": 0, \"vote_margin\": 0.0, \"entropy\": 0.0, \"answer_counts\": {}}\n    \n    answers = [c[\"answer\"] for c in candidates]\n    counter = Counter(answers)\n    total = len(answers)\n    \n    most_common = counter.most_common()\n    top_answer, top_count = most_common[0]\n    \n    second_count = most_common[1][1] if len(most_common) > 1 else 0\n    vote_margin = (top_count - second_count) / total\n    \n    entropy = 0.0\n    for count in counter.values():\n        p = count / total\n        if p > 0:\n            entropy -= p * math.log2(p)\n    \n    return {\n        \"top_answer\": top_answer, \"top_count\": top_count, \"total\": total,\n        \"vote_margin\": vote_margin, \"entropy\": entropy,\n        \"answer_counts\": dict(counter.most_common(5)),\n    }\n\ndef should_early_stop(vote_result: Dict[str, Any], threshold: float = VOTING_THRESHOLD) -> bool:\n    if vote_result[\"total\"] == 0:\n        return False\n    return vote_result[\"top_count\"] / vote_result[\"total\"] >= threshold\n\nprint(\"Candidate generation functions initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL G \u2014 VERIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL G \u2014 VERIFIER (Rule-based + Optional LLM)\n# ================================\n\ndef rule_verifier(problem: str, answer: int) -> Dict[str, Any]:\n    checks = []\n    passed = True\n    reason = \"OK\"\n    \n    if not (0 <= answer <= 99999):\n        passed = False\n        reason = f\"Answer {answer} out of valid range [0, 99999]\"\n        checks.append((\"range_check\", False, reason))\n    else:\n        checks.append((\"range_check\", True, \"In valid range\"))\n    \n    problem_lower = problem.lower()\n    if \"remainder\" in problem_lower or \"modulo\" in problem_lower or \"mod \" in problem_lower:\n        mod_patterns = [r\"divided by\\s+(\\d+)\", r\"modulo\\s+(\\d+)\", r\"mod\\s+(\\d+)\", r\"\\(mod\\s*(\\d+)\\)\"]\n        for pattern in mod_patterns:\n            match = re.search(pattern, problem_lower)\n            if match:\n                mod_val = int(match.group(1))\n                if answer >= mod_val and mod_val < 100000:\n                    checks.append((\"mod_check\", False, f\"Answer {answer} >= modulo {mod_val}\"))\n                else:\n                    checks.append((\"mod_check\", True, f\"Answer {answer} < modulo {mod_val}\"))\n                break\n    \n    return {\"passed\": passed, \"reason\": reason, \"checks\": checks}\n\ndef llm_verifier(problem: str, answer: int) -> Dict[str, Any]:\n    model, tokenizer, device = load_model()\n    \n    if model is None:\n        return {\"passed\": True, \"reason\": \"LLM not available\", \"response\": \"\"}\n    \n    prompt = f\"\"\"Given this math problem and proposed answer, quickly check if the answer could be correct.\nIf you find a clear error or contradiction, say INVALID. Otherwise say VALID.\n\nProblem: {problem}\n\nProposed Answer: {answer}\n\nVerification (VALID or INVALID):\"\"\"\n    \n    try:\n        import torch\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        \n        with torch.no_grad():\n            outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.1, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n        \n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()\n        passed = \"INVALID\" not in response.upper()\n        \n        return {\"passed\": passed, \"reason\": \"VALID\" if passed else \"INVALID found\", \"response\": response[:200]}\n        \n    except Exception as e:\n        return {\"passed\": True, \"reason\": f\"Error: {str(e)}\", \"response\": \"\"}\n\ndef verify_answer(problem: str, answer: int, use_llm: bool = False) -> Dict[str, Any]:\n    result = {\"rule_verifier\": rule_verifier(problem, answer), \"llm_verifier\": None, \"final_passed\": True}\n    \n    if not result[\"rule_verifier\"][\"passed\"]:\n        result[\"final_passed\"] = False\n    \n    if use_llm and result[\"rule_verifier\"][\"passed\"]:\n        result[\"llm_verifier\"] = llm_verifier(problem, answer)\n        if not result[\"llm_verifier\"][\"passed\"]:\n            result[\"final_passed\"] = False\n    \n    return result\n\nprint(\"Verifier functions initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL H \u2014 SOLVER ORCHESTRATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL H \u2014 SOLVER ORCHESTRATOR\n# ================================\n\ndef solve_problem(problem_id: str, problem_text: str) -> Tuple[int, Dict[str, Any]]:\n    start_time = time.time()\n    \n    telemetry = {\n        \"id\": problem_id, \"elapsed_sec\": 0, \"k_used\": 0, \"candidates\": [],\n        \"chosen_answer\": 0, \"vote_margin\": 0.0, \"verifier_used\": False,\n        \"verifier_pass\": True, \"tool_calls_count\": 0, \"parse_method_used\": \"none\",\n        \"mode\": \"EASY\", \"temperature_schedule\": [], \"seed\": CURRENT_SEED,\n        \"run_policy\": MODE_POLICY, \"is_rerun\": IS_KAGGLE_RERUN,\n    }\n    \n    try:\n        if MODE_POLICY == \"diverse\" and IS_KAGGLE_RERUN:\n            mode, k = \"HARD\", K_MAX_HARD\n            temp_schedule = [TEMPERATURE_HARD] * k\n        else:\n            mode, k = \"EASY\", K_BASE\n            temp_schedule = [TEMPERATURE_BASE] * k\n        \n        telemetry[\"mode\"] = mode\n        telemetry[\"temperature_schedule\"] = temp_schedule\n        \n        candidates = []\n        for i in range(k):\n            if time.time() - start_time > TIME_BUDGET_SEC_PER_PROBLEM * 0.8:\n                break\n            \n            raw_text, meta = generate_one(problem_text, temperature=temp_schedule[i])\n            processed_text = execute_code_in_response(raw_text)\n            answer, answer_meta = safe_extract_answer(processed_text)\n            \n            candidates.append({\"answer\": answer, \"raw_text\": raw_text[:500], \"metadata\": {**meta, **answer_meta}})\n            \n            if parse_python_block(raw_text):\n                telemetry[\"tool_calls_count\"] += 1\n        \n        telemetry[\"k_used\"] = len(candidates)\n        \n        vote_result = vote_candidates(candidates)\n        telemetry[\"vote_margin\"] = vote_result[\"vote_margin\"]\n        telemetry[\"candidates\"] = [(a, c) for a, c in vote_result[\"answer_counts\"].items()]\n        \n        chosen_answer = vote_result[\"top_answer\"]\n        \n        for c in candidates:\n            if c[\"answer\"] == chosen_answer:\n                telemetry[\"parse_method_used\"] = c[\"metadata\"].get(\"method\", \"none\")\n                break\n        \n        use_llm_verifier = vote_result[\"vote_margin\"] < 0.3 and mode == \"HARD\"\n        verification = verify_answer(problem_text, chosen_answer, use_llm=use_llm_verifier)\n        \n        telemetry[\"verifier_used\"] = True\n        telemetry[\"verifier_pass\"] = verification[\"final_passed\"]\n        \n        if not verification[\"final_passed\"]:\n            for answer, count in vote_result[\"answer_counts\"].items():\n                if answer != chosen_answer:\n                    alt_verify = verify_answer(problem_text, answer, use_llm=False)\n                    if alt_verify[\"final_passed\"]:\n                        chosen_answer = answer\n                        break\n        \n        is_valid, final_answer = validate_answer(chosen_answer)\n        if not is_valid:\n            telemetry[\"fallback_used\"] = True\n            final_answer = 0\n        \n        telemetry[\"chosen_answer\"] = final_answer\n        \n    except Exception as e:\n        telemetry[\"error\"] = str(e)\n        final_answer = 0\n        telemetry[\"chosen_answer\"] = final_answer\n        telemetry[\"fallback_used\"] = True\n    \n    telemetry[\"elapsed_sec\"] = time.time() - start_time\n    \n    return final_answer, telemetry\n\nprint(\"Solver orchestrator initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL I \u2014 TELEMETRY LOGGER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL I \u2014 TELEMETRY LOGGER\n# ================================\n\ndef append_jsonl(filepath: str, data: Dict[str, Any]):\n    os.makedirs(os.path.dirname(filepath), exist_ok=True) if os.path.dirname(filepath) else None\n    with open(filepath, \"a\") as f:\n        f.write(json.dumps(data, default=str) + \"\\n\")\n        f.flush()\n\ndef read_telemetry(filepath: str) -> List[Dict[str, Any]]:\n    entries = []\n    if os.path.exists(filepath):\n        with open(filepath, \"r\") as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    try:\n                        entries.append(json.loads(line))\n                    except json.JSONDecodeError:\n                        pass\n    return entries\n\ndef log_telemetry(telemetry: Dict[str, Any]):\n    append_jsonl(LOG_PATH, telemetry)\n\ndef print_telemetry_summary(telemetry_list: List[Dict[str, Any]]):\n    if not telemetry_list:\n        print(\"No telemetry data available\")\n        return\n    \n    n = len(telemetry_list)\n    total_time = sum(t.get(\"elapsed_sec\", 0) for t in telemetry_list)\n    avg_time = total_time / n if n > 0 else 0\n    \n    parse_methods = Counter(t.get(\"parse_method_used\", \"none\") for t in telemetry_list)\n    parse_fail_rate = parse_methods.get(\"none\", 0) / n if n > 0 else 0\n    \n    k_values = [t.get(\"k_used\", 0) for t in telemetry_list]\n    avg_k = sum(k_values) / n if n > 0 else 0\n    \n    verifier_used = sum(1 for t in telemetry_list if t.get(\"verifier_used\", False))\n    verifier_pass = sum(1 for t in telemetry_list if t.get(\"verifier_pass\", True))\n    \n    fallback_used = sum(1 for t in telemetry_list if t.get(\"fallback_used\", False))\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"TELEMETRY SUMMARY\")\n    print(\"=\"*50)\n    print(f\"Total problems: {n}\")\n    print(f\"Total time: {total_time:.2f}s\")\n    print(f\"Avg time per problem: {avg_time:.2f}s\")\n    print(f\"Parse fail rate: {parse_fail_rate:.2%}\")\n    print(f\"Avg k_used: {avg_k:.1f}\")\n    print(f\"K distribution: {Counter(k_values)}\")\n    print(f\"Parse methods: {dict(parse_methods)}\")\n    print(f\"Verifier usage: {verifier_used}/{n}\")\n    print(f\"Verifier pass rate: {verifier_pass}/{verifier_used if verifier_used > 0 else 1}\")\n    print(f\"Fallback used: {fallback_used}/{n}\")\n    print(\"=\"*50)\n\nprint(\"Telemetry logger initialized\")\nprint(f\"Log path: {LOG_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL J \u2014 LOCAL HARNESS (Reference CSV Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL J \u2014 LOCAL HARNESS (Reference CSV Regression)\n# ================================\n\ndef run_reference_eval(csv_path: str = None, limit: int = None) -> Dict[str, Any]:\n    csv_path = csv_path or REFERENCE_CSV_PATH\n    \n    if not os.path.exists(csv_path):\n        print(f\"Reference CSV not found: {csv_path}\")\n        return {\"error\": \"File not found\", \"accuracy\": 0.0}\n    \n    print(f\"\\nRunning reference evaluation on: {csv_path}\")\n    print(\"=\"*60)\n    \n    df = pd.read_csv(csv_path)\n    \n    if limit:\n        df = df.head(limit)\n    \n    n_problems = len(df)\n    print(f\"Evaluating {n_problems} problems...\\n\")\n    \n    results = []\n    correct = 0\n    telemetry_list = []\n    \n    for idx, row in df.iterrows():\n        problem_id = str(row[\"id\"])\n        problem_text = row[\"problem\"]\n        expected_answer = int(row[\"answer\"])\n        \n        print(f\"[{idx+1}/{n_problems}] Problem {problem_id}...\")\n        \n        predicted_answer, telemetry = solve_problem(problem_id, problem_text)\n        \n        telemetry[\"expected_answer\"] = expected_answer\n        telemetry[\"is_correct\"] = (predicted_answer == expected_answer)\n        log_telemetry(telemetry)\n        telemetry_list.append(telemetry)\n        \n        is_correct = (predicted_answer == expected_answer)\n        if is_correct:\n            correct += 1\n            status = \"Y\"\n        else:\n            status = \"X\"\n        \n        print(f\"  {status} Predicted: {predicted_answer}, Expected: {expected_answer} ({telemetry['elapsed_sec']:.2f}s)\")\n        \n        results.append({\n            \"id\": problem_id, \"predicted\": predicted_answer,\n            \"expected\": expected_answer, \"correct\": is_correct,\n            \"elapsed_sec\": telemetry[\"elapsed_sec\"],\n        })\n    \n    accuracy = correct / n_problems if n_problems > 0 else 0.0\n    \n    print(\"\\n\" + \"=\"*60)\n    print(f\"ACCURACY: {correct}/{n_problems} = {accuracy:.2%}\")\n    print(\"=\"*60)\n    \n    print_telemetry_summary(telemetry_list)\n    \n    return {\"accuracy\": accuracy, \"correct\": correct, \"total\": n_problems, \"results\": results}\n\nprint(\"Local harness initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CELL K \u2014 SUBMISSION GLUE (Kaggle Evaluation API)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# CELL K \u2014 SUBMISSION GLUE (Kaggle Evaluation API)\n# ================================\n\nimport sys\nimport os\n\nkaggle_eval_paths = [\"/kaggle/input/kaggle-evaluation\", \"/kaggle/input\", \".\", \"..\"]\n\nfor path in kaggle_eval_paths:\n    if os.path.exists(os.path.join(path, \"kaggle_evaluation\")):\n        sys.path.insert(0, path)\n        break\n\ndef predict(test_df: pd.DataFrame) -> pd.DataFrame:\n    results = []\n    \n    for idx, row in test_df.iterrows():\n        problem_id = str(row[\"id\"])\n        problem_text = str(row[\"problem\"])\n        \n        answer, telemetry = solve_problem(problem_id, problem_text)\n        log_telemetry(telemetry)\n        \n        results.append({\"id\": problem_id, \"answer\": int(answer)})\n    \n    return pd.DataFrame(results)\n\ndef setup_and_serve():\n    try:\n        from kaggle_evaluation.aimo_3_inference_server import AIMO3InferenceServer\n        \n        server = AIMO3InferenceServer(predict)\n        \n        if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n            print(\"Starting inference server (competition mode)...\")\n            server.serve()\n        else:\n            print(\"Running local gateway test...\")\n            server.run_local_gateway()\n            \n    except ImportError as e:\n        print(f\"kaggle_evaluation not available: {e}\")\n        print(\"Running in local-only mode\")\n\nprint(\"Submission glue initialized\")\nprint(\"Use setup_and_serve() to start the server\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SELF TEST \u2014 Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# SELF TEST 1: Schema/Predict Unit Test\n# ================================\n\ndef test_predict_schema():\n    print(\"\\nTEST 1: Schema/Predict Unit Test\")\n    print(\"-\"*40)\n    \n    test_df = pd.DataFrame({\"id\": [\"test001\"], \"problem\": [\"What is $1+1$?\"]})\n    result_df = predict(test_df)\n    \n    assert \"id\" in result_df.columns, \"Missing 'id' column\"\n    assert \"answer\" in result_df.columns, \"Missing 'answer' column\"\n    assert len(result_df) == 1, f\"Expected 1 row, got {len(result_df)}\"\n    \n    answer = result_df[\"answer\"].iloc[0]\n    assert isinstance(answer, (int, type(1))), f\"Answer should be int, got {type(answer)}\"\n    assert 0 <= answer <= 99999, f\"Answer {answer} out of range [0, 99999]\"\n    \n    print(f\"OK Output schema correct\")\n    print(f\"OK Answer: {answer} (valid int in [0, 99999])\")\n    print(\"TEST 1 PASSED\\n\")\n    return True\n\nif RUN_MODE == \"local_ref\":\n    try:\n        test_predict_schema()\n    except AssertionError as e:\n        print(f\"TEST 1 FAILED: {e}\")\n    except Exception as e:\n        print(f\"TEST 1 ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# SELF TEST 2: Reference Evaluation\n# ================================\n\ndef test_reference_eval():\n    print(\"\\nTEST 2: Reference Evaluation\")\n    print(\"-\"*40)\n    \n    result = run_reference_eval(limit=2)\n    \n    assert \"accuracy\" in result, \"Missing 'accuracy' in result\"\n    assert \"total\" in result, \"Missing 'total' in result\"\n    assert result[\"total\"] == 2, f\"Expected 2 problems, got {result['total']}\"\n    \n    print(f\"OK Accuracy: {result['accuracy']:.2%}\")\n    print(f\"OK No crashes during evaluation\")\n    print(\"TEST 2 PASSED\\n\")\n    return True\n\nif RUN_MODE == \"local_ref\":\n    try:\n        if os.path.exists(REFERENCE_CSV_PATH):\n            test_reference_eval()\n        else:\n            print(\"Skipping TEST 2: reference.csv not found\")\n    except AssertionError as e:\n        print(f\"TEST 2 FAILED: {e}\")\n    except Exception as e:\n        print(f\"TEST 2 ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# SELF TEST 3: Tool Executor\n# ================================\n\ndef test_tool_executor():\n    print(\"\\nTEST 3: Tool Executor\")\n    print(\"-\"*40)\n    \n    # Test 1: Simple arithmetic\n    ok, output = run_python(\"result = 2 + 3\")\n    assert ok, f\"Execution failed: {output}\"\n    assert \"5\" in output, f\"Expected '5' in output, got: {output}\"\n    print(f\"OK Simple arithmetic: 2+3 = 5\")\n    \n    # Test 2: Math module (already loaded, no import needed)\n    ok, output = run_python(\"result = math.factorial(5)\")\n    assert ok, f\"Execution failed: {output}\"\n    assert \"120\" in output, f\"Expected '120' in output, got: {output}\"\n    print(f\"OK Math module: factorial(5) = 120\")\n    \n    # Test 3: Code with import statement (should be stripped)\n    ok, output = run_python(\"import math\\nresult = math.sqrt(16)\")\n    assert ok, f\"Execution failed: {output}\"\n    assert \"4\" in output, f\"Expected '4' in output, got: {output}\"\n    print(f\"OK Import stripping works: sqrt(16) = 4\")\n    \n    # Test 4: Timeout handling\n    ok, output = run_python(\"x = 1\", timeout_sec=1)\n    assert ok, f\"Simple code should not timeout\"\n    print(f\"OK Timeout handling works\")\n    \n    print(\"TEST 3 PASSED\\n\")\n    return True\n\nif RUN_MODE == \"local_ref\":\n    try:\n        test_tool_executor()\n    except AssertionError as e:\n        print(f\"TEST 3 FAILED: {e}\")\n    except Exception as e:\n        print(f\"TEST 3 ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# SELF TEST 4: Answer Extraction\n# ================================\n\ndef test_answer_extraction():\n    print(\"\\nTEST 4: Answer Extraction\")\n    print(\"-\"*40)\n    \n    test_cases = [\n        (\"ANSWER: 42\", 42, \"ANSWER\"),\n        (\"The answer is 123\", 123, \"LASTINT\"),\n        (\"After calculation, we get 456. ANSWER: 456\", 456, \"ANSWER\"),\n        (\"Result: 789\", 789, \"LASTINT\"),\n    ]\n    \n    for text, expected_answer, expected_method in test_cases:\n        answer, method = extract_answer(text)\n        assert answer == expected_answer, f\"Expected {expected_answer}, got {answer} for '{text}'\"\n        assert method == expected_method, f\"Expected method {expected_method}, got {method}\"\n        print(f\"OK '{text[:30]}...' -> {answer} ({method})\")\n    \n    print(\"TEST 4 PASSED\\n\")\n    return True\n\nif RUN_MODE == \"local_ref\":\n    try:\n        test_answer_extraction()\n    except AssertionError as e:\n        print(f\"TEST 4 FAILED: {e}\")\n    except Exception as e:\n        print(f\"TEST 4 ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MAIN EXECUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n# MAIN EXECUTION\n# ================================\n\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\"*60)\n    print(\"AIMO3 BASELINE NOTEBOOK\")\n    print(\"=\"*60)\n    print(f\"RUN_MODE: {RUN_MODE}\")\n    print(f\"CURRENT_SEED: {CURRENT_SEED}\")\n    print(f\"IS_KAGGLE_RERUN: {IS_KAGGLE_RERUN}\")\n    print(\"=\"*60 + \"\\n\")\n    \n    if RUN_MODE == \"local_ref\":\n        print(\"Running in LOCAL/DEBUG mode...\")\n        print(\"Evaluating reference.csv...\\n\")\n        \n        if os.path.exists(LOG_PATH):\n            os.remove(LOG_PATH)\n        \n        if os.path.exists(REFERENCE_CSV_PATH):\n            result = run_reference_eval()\n            print(f\"\\nFinal Accuracy: {result['accuracy']:.2%}\")\n        else:\n            print(f\"Reference CSV not found: {REFERENCE_CSV_PATH}\")\n            print(\"Running self-tests only...\")\n        \n    elif RUN_MODE == \"submit_auto\":\n        print(\"Running in SUBMISSION mode...\")\n        print(\"Starting inference server...\\n\")\n        \n        setup_and_serve()\n    \n    else:\n        print(f\"Unknown RUN_MODE: {RUN_MODE}\")\n        print(\"Valid options: 'local_ref', 'submit_auto'\")\n\nprint(\"\\nNotebook execution complete.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}