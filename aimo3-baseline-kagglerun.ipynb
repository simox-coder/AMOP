{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512db654",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T17:08:22.578039Z",
     "iopub.status.busy": "2026-01-01T17:08:22.577795Z",
     "iopub.status.idle": "2026-01-01T17:08:22.588563Z",
     "shell.execute_reply": "2026-01-01T17:08:22.588177Z"
    },
    "papermill": {
     "duration": 0.016119,
     "end_time": "2026-01-01T17:08:22.589662",
     "exception": false,
     "start_time": "2026-01-01T17:08:22.573543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AIMO3 TOP-ORIENTED SUBMISSION — Qwen-3 30B-A3B Thinking (H100)\n",
    "# FIXES:\n",
    "# - proper left-padding setup downstream\n",
    "# - separate TOOL_POOL_SIZE (python workers) vs TOOL_THREAD_WORKERS (threads)\n",
    "# - add GEN_BATCH_SIZE for GPU utilization\n",
    "# ============================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os, re, time, math, json, random, glob\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "MODEL_PATH = \"/kaggle/input/qwen-3/transformers/30b-a3b-thinking-2507-fp8/1\"\n",
    "SEED = int(os.getenv(\"SEED\", \"42\"))\n",
    "\n",
    "# Warm/cache\n",
    "WARMUP_USR_LIB = os.getenv(\"WARMUP_USR_LIB\", \"0\") == \"1\"\n",
    "CACHE_MODEL_FILES = os.getenv(\"CACHE_MODEL_FILES\", \"1\") == \"1\"\n",
    "CACHE_CHUNK_MB = int(os.getenv(\"CACHE_CHUNK_MB\", \"512\"))\n",
    "CACHE_WORKERS = int(os.getenv(\"CACHE_WORKERS\", \"8\"))\n",
    "CACHE_EXTS = (\".safetensors\", \".bin\", \".pt\")\n",
    "\n",
    "# Time (mặc định 4h55m giống style V22 notebook, tránh chết sát giờ)\n",
    "HARD_WALL_SECONDS = int(os.getenv(\"HARD_WALL_SECONDS\", str((4 * 60 + 55) * 60)))\n",
    "TOTAL_QUESTIONS = int(os.getenv(\"TOTAL_QUESTIONS\", \"110\"))\n",
    "MIN_BUDGET_S = float(os.getenv(\"MIN_BUDGET_S\", \"10\"))\n",
    "MAX_BUDGET_S = float(os.getenv(\"MAX_BUDGET_S\", \"420\"))\n",
    "\n",
    "# Tool loop (CPU)\n",
    "TOOL_POOL_SIZE = int(os.getenv(\"TOOL_POOL_SIZE\", \"12\"))  # số python subprocess workers\n",
    "TOOL_THREAD_WORKERS = int(os.getenv(\"TOOL_THREAD_WORKERS\", str(min(12, TOOL_POOL_SIZE))))\n",
    "TOOL_TIMEOUT_S = float(os.getenv(\"TOOL_TIMEOUT_S\", \"6.0\"))\n",
    "MAX_TURNS = int(os.getenv(\"MAX_TURNS\", \"10\"))\n",
    "\n",
    "# Sampling (accuracy vs time)\n",
    "STAGE1_BATCH = int(os.getenv(\"STAGE1_BATCH\", \"2\"))\n",
    "STAGE2_BATCH = int(os.getenv(\"STAGE2_BATCH\", \"3\"))\n",
    "CONFIDENT_RATIO = float(os.getenv(\"CONFIDENT_RATIO\", \"0.78\"))\n",
    "VERIFY_RATIO = float(os.getenv(\"VERIFY_RATIO\", \"0.66\"))\n",
    "VERIFY_TOP_N = int(os.getenv(\"VERIFY_TOP_N\", \"3\"))\n",
    "\n",
    "# Generation (GPU)\n",
    "MAX_MODEL_LEN = int(os.getenv(\"MAX_MODEL_LEN\", \"12288\"))   # 16k ok, nhưng 12k thường nhanh hơn\n",
    "DTYPE = os.getenv(\"DTYPE\", \"bfloat16\")\n",
    "GPU_MEM_UTIL = float(os.getenv(\"GPU_MEM_UTIL\", \"0.92\"))\n",
    "\n",
    "# Batch size cho HF backend (đẩy GPU util lên)\n",
    "GEN_BATCH_SIZE = int(os.getenv(\"GEN_BATCH_SIZE\", \"8\"))  # H100 + 30B fp8 thường chịu 6-10\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cfb9e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T17:08:22.594540Z",
     "iopub.status.busy": "2026-01-01T17:08:22.594378Z",
     "iopub.status.idle": "2026-01-01T17:09:27.138458Z",
     "shell.execute_reply": "2026-01-01T17:09:27.138004Z"
    },
    "papermill": {
     "duration": 64.549778,
     "end_time": "2026-01-01T17:09:27.141582",
     "exception": false,
     "start_time": "2026-01-01T17:08:22.591804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cache_model] warmed ~29.03 GB in 64.5s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 2/13 — WARMUP HELPERS\n",
    "# =========================\n",
    "def warmup_usr_lib() -> None:\n",
    "    import subprocess\n",
    "    cmd = \"find /kaggle/usr/lib -type f -print0 | xargs -0 -P 32 -n 500 cat > /dev/null\"\n",
    "    subprocess.run(cmd, shell=True, check=False)\n",
    "\n",
    "def cache_model(path: str, exts=CACHE_EXTS, num_workers: int = 8, chunk_mb: int = 256) -> None:\n",
    "    import multiprocessing\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "    def warmup_file(fpath: str) -> int:\n",
    "        chunk = chunk_mb * 1024 * 1024\n",
    "        total = 0\n",
    "        with open(fpath, \"rb\") as f:\n",
    "            while True:\n",
    "                b = f.read(chunk)\n",
    "                if not b:\n",
    "                    break\n",
    "                total += len(b)\n",
    "        return total\n",
    "\n",
    "    if not os.path.isdir(path):\n",
    "        return\n",
    "\n",
    "    files = [\n",
    "        os.path.join(root, name)\n",
    "        for root, _, names in os.walk(path)\n",
    "        for name in names\n",
    "        if name.endswith(exts)\n",
    "    ]\n",
    "    if not files:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        cpu = multiprocessing.cpu_count()\n",
    "    except Exception:\n",
    "        cpu = 4\n",
    "    num_workers = max(1, min(num_workers, cpu, 16))\n",
    "    files.sort(key=lambda f: os.path.getsize(f), reverse=True)\n",
    "\n",
    "    t0 = time.time()\n",
    "    total = 0\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
    "        futs = [ex.submit(warmup_file, f) for f in files]\n",
    "        for fut in as_completed(futs):\n",
    "            total += fut.result()\n",
    "\n",
    "    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "        gb = total / 1024**3\n",
    "        print(f\"[cache_model] warmed ~{gb:.2f} GB in {time.time()-t0:.1f}s\")\n",
    "\n",
    "if WARMUP_USR_LIB:\n",
    "    warmup_usr_lib()\n",
    "if CACHE_MODEL_FILES:\n",
    "    cache_model(MODEL_PATH, num_workers=CACHE_WORKERS, chunk_mb=CACHE_CHUNK_MB)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba216cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T17:09:27.146690Z",
     "iopub.status.busy": "2026-01-01T17:09:27.146495Z",
     "iopub.status.idle": "2026-01-01T17:10:06.903184Z",
     "shell.execute_reply": "2026-01-01T17:10:06.902730Z"
    },
    "papermill": {
     "duration": 39.761195,
     "end_time": "2026-01-01T17:10:06.904839",
     "exception": false,
     "start_time": "2026-01-01T17:09:27.143644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-01 17:09:51.872711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767287392.311090      44 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767287392.429465      44 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767287393.515497      44 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767287393.515531      44 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767287393.515533      44 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767287393.515534      44 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 3/13 — IMPORTS\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import polars as pl\n",
    "except Exception:\n",
    "    pl = None\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b66575b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T17:10:06.910315Z",
     "iopub.status.busy": "2026-01-01T17:10:06.909920Z",
     "iopub.status.idle": "2026-01-01T17:10:06.919420Z",
     "shell.execute_reply": "2026-01-01T17:10:06.919036Z"
    },
    "papermill": {
     "duration": 0.013094,
     "end_time": "2026-01-01T17:10:06.920204",
     "exception": false,
     "start_time": "2026-01-01T17:10:06.907110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 4/13 — PARSERS + UTILS  [FIXED: DON'T DELETE TOOL/BOXED INSIDE <think>]\n",
    "# =========================\n",
    "import re\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "# Case-insensitive for tags\n",
    "_THINK_BLOCK_RE = re.compile(r\"<think>.*?</think>\", re.DOTALL | re.IGNORECASE)\n",
    "_THINK_TAG_RE   = re.compile(r\"</?think>\", re.IGNORECASE)\n",
    "\n",
    "_BOXED_RE = re.compile(r\"\\\\boxed\\{([^}]*)\\}\")\n",
    "_TOOL_RE  = re.compile(r\"<tool:python>\\s*(.*?)\\s*</tool:python>\", re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "def remove_think_blocks(text: str) -> str:\n",
    "    \"\"\"Remove entire <think>...</think> blocks (for history display).\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return _THINK_BLOCK_RE.sub(\"\", text).strip()\n",
    "\n",
    "def remove_think_tags(text: str) -> str:\n",
    "    \"\"\"Remove only <think> tags but KEEP content (for parsing).\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return _THINK_TAG_RE.sub(\"\", text).strip()\n",
    "\n",
    "def clean_for_history(text: str, limit: int = 800) -> str:\n",
    "    \"\"\"Keep history short and avoid leaking huge thinking.\"\"\"\n",
    "    t = remove_think_blocks(text)\n",
    "    t = t.strip()\n",
    "    if len(t) > limit:\n",
    "        t = t[:limit]\n",
    "    return t\n",
    "\n",
    "def parse_boxed_int(text: str) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Parse \\\\boxed{int} even if it appears inside <think>...</think>.\n",
    "    Try raw first, then tag-stripped.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    for t in (text, remove_think_tags(text)):\n",
    "        m = _BOXED_RE.search(t)\n",
    "        if not m:\n",
    "            continue\n",
    "        raw = m.group(1).strip()\n",
    "        if not re.fullmatch(r\"[+-]?\\d+\", raw):\n",
    "            continue\n",
    "        v = int(raw)\n",
    "        return v if 0 <= v <= 99999 else None\n",
    "\n",
    "    return None\n",
    "\n",
    "def parse_tool_code(text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Parse <tool:python>...</tool:python> even if it appears inside <think>.\n",
    "    Try raw first, then tag-stripped.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    for t in (text, remove_think_tags(text)):\n",
    "        m = _TOOL_RE.search(t)\n",
    "        if m:\n",
    "            return m.group(1).strip()\n",
    "\n",
    "    return None\n",
    "\n",
    "def fallback_last_int(text: str) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Last-resort: grab the last integer from tag-stripped text (do NOT drop think content).\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    t = remove_think_tags(text)\n",
    "    nums = re.findall(r\"[-+]?\\d+\", t)\n",
    "    if not nums:\n",
    "        return None\n",
    "    try:\n",
    "        v = int(nums[-1])\n",
    "    except Exception:\n",
    "        return None\n",
    "    return v if 0 <= v <= 99999 else None\n",
    "\n",
    "def mod100000(x: int) -> int:\n",
    "    return int(x) % 100000\n",
    "\n",
    "def clamp(x: float, lo: float, hi: float) -> float:\n",
    "    return float(min(hi, max(lo, x)))\n",
    "\n",
    "def trim_history(hist: List[Tuple[str, str]], max_items: int = 10) -> List[Tuple[str, str]]:\n",
    "    return hist if len(hist) <= max_items else hist[-max_items:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7004b754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T17:10:06.925082Z",
     "iopub.status.busy": "2026-01-01T17:10:06.924908Z",
     "iopub.status.idle": "2026-01-01T17:10:06.930709Z",
     "shell.execute_reply": "2026-01-01T17:10:06.930359Z"
    },
    "papermill": {
     "duration": 0.009066,
     "end_time": "2026-01-01T17:10:06.931435",
     "exception": false,
     "start_time": "2026-01-01T17:10:06.922369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 5/13 — DIFFICULTY ROUTER\n",
    "# =========================\n",
    "@dataclass(frozen=True)\n",
    "class Plan:\n",
    "    tag: str\n",
    "    budget_weight: float\n",
    "    stage1_max_k: int\n",
    "    stage2_max_k: int\n",
    "    stage1_max_tokens: int\n",
    "    stage2_max_tokens: int\n",
    "    temp1: float\n",
    "    temp2: float\n",
    "    top_p1: float\n",
    "    top_p2: float\n",
    "\n",
    "def route_problem(problem: str) -> Plan:\n",
    "    p = (problem or \"\").lower()\n",
    "\n",
    "    if any(k in p for k in [\"triangle\",\"circle\",\"radius\",\"angle\",\"tangent\",\"perpendicular\",\"circum\",\"inscribed\"]):\n",
    "        return Plan(\"GEO\", 1.20, 6, 14, 950, 1800, 0.55, 0.75, 0.92, 0.90)\n",
    "\n",
    "    if any(k in p for k in [\"mod\",\"congruen\",\"prime\",\"gcd\",\"lcm\",\"divis\",\"remainder\",\"coprime\",\"valuation\",\"phi(\"]):\n",
    "        return Plan(\"NT\", 1.30, 6, 16, 980, 1950, 0.55, 0.78, 0.92, 0.90)\n",
    "\n",
    "    if any(k in p for k in [\"f(\",\"functional\",\"for all real\",\"for all integers\",\"for all x\",\"for all n\",\"satisfies\"]):\n",
    "        return Plan(\"FUNC\", 1.25, 6, 16, 980, 1950, 0.55, 0.78, 0.92, 0.90)\n",
    "\n",
    "    if any(k in p for k in [\"probability\",\"expected\",\"random\",\"uniform\",\"dice\",\"coin\",\"distribution\"]):\n",
    "        return Plan(\"PROB\", 1.15, 6, 14, 950, 1800, 0.55, 0.75, 0.92, 0.90)\n",
    "\n",
    "    if any(k in p for k in [\"ways\",\"choose\",\"arrangements\",\"permutation\",\"combination\",\"graph\",\"color\",\"pigeonhole\",\"invariant\"]):\n",
    "        return Plan(\"COMB\", 1.20, 6, 16, 950, 1900, 0.55, 0.78, 0.92, 0.90)\n",
    "\n",
    "    return Plan(\"ALG\", 1.00, 6, 14, 900, 1700, 0.50, 0.72, 0.92, 0.90)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1a9c8b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T17:10:06.936025Z",
     "iopub.status.busy": "2026-01-01T17:10:06.935870Z",
     "iopub.status.idle": "2026-01-01T17:10:06.938564Z",
     "shell.execute_reply": "2026-01-01T17:10:06.938239Z"
    },
    "papermill": {
     "duration": 0.005908,
     "end_time": "2026-01-01T17:10:06.939302",
     "exception": false,
     "start_time": "2026-01-01T17:10:06.933394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 6/13 — PROMPTS\n",
    "# =========================\n",
    "SYSTEM_TIR = (\n",
    "    \"You are an olympiad math solver.\\n\"\n",
    "    \"You MUST follow this protocol:\\n\\n\"\n",
    "    \"If you need computation, output exactly:\\n\"\n",
    "    \"<tool:python>\\n\"\n",
    "    \"# python code\\n\"\n",
    "    \"</tool:python>\\n\\n\"\n",
    "    \"If you are ready to answer, output exactly ONE line:\\n\"\n",
    "    \"\\\\boxed{NONNEGATIVE_INTEGER}\\n\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"- Output NOTHING else outside the tool block.\\n\"\n",
    "    \"- Final answer must be an integer in [0, 99999].\\n\"\n",
    "    \"- Prefer verifying with python when possible.\\n\"\n",
    ")\n",
    "\n",
    "SYSTEM_VERIFY = (\n",
    "    \"You are a strict verifier.\\n\"\n",
    "    \"Given a problem and proposed integer answer A, DISPROVE it quickly.\\n\"\n",
    "    \"Use python checks when possible:\\n\"\n",
    "    \"- parity constraints\\n\"\n",
    "    \"- modular constraints\\n\"\n",
    "    \"- substitution / brute force small cases / random tests\\n\\n\"\n",
    "    \"Protocol:\\n\"\n",
    "    \"- You may output <tool:python>...</tool:python> blocks.\\n\"\n",
    "    \"- Then output EXACTLY one final line: PASS or FAIL or UNKNOWN\\n\"\n",
    "    \"- No extra text.\\n\"\n",
    ")\n",
    "\n",
    "SYSTEM_SELECT = (\n",
    "    \"You are a selector.\\n\"\n",
    "    \"Pick the most reliable candidate answer based on evidence.\\n\"\n",
    "    \"Output EXACTLY one line: \\\\boxed{NONNEGATIVE_INTEGER}\\n\"\n",
    "    \"No extra text.\\n\"\n",
    ")\n",
    "\n",
    "HINTS = [\n",
    "    \"Tool-first: explore small cases in python, infer pattern, verify, then output boxed.\",\n",
    "    \"Proof-first: derive symbolic structure, then minimal python verification, output boxed.\",\n",
    "    \"Number-theory: use modular constraints/parity/gcd; python to test; output boxed.\",\n",
    "    \"Comb/Prob: use invariants or counting; python to validate small n; output boxed.\",\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fef92ac9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T17:10:06.944056Z",
     "iopub.status.busy": "2026-01-01T17:10:06.943910Z",
     "iopub.status.idle": "2026-01-01T17:10:06.947733Z",
     "shell.execute_reply": "2026-01-01T17:10:06.947377Z"
    },
    "papermill": {
     "duration": 0.007083,
     "end_time": "2026-01-01T17:10:06.948458",
     "exception": false,
     "start_time": "2026-01-01T17:10:06.941375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 7/13 — PROMPT BUILDER (chat template)  [FIX: chronological order]\n",
    "# =========================\n",
    "from typing import List, Tuple\n",
    "\n",
    "class PromptBuilder:\n",
    "    def __init__(self, tok):\n",
    "        self.tok = tok\n",
    "\n",
    "    def render(self, system: str, user: str, history: List[Tuple[str, str]]) -> str:\n",
    "        # IMPORTANT:\n",
    "        # Correct timeline must be:\n",
    "        # system -> history (old turns) -> current user (problem)\n",
    "        msgs = [{\"role\": \"system\", \"content\": system}]\n",
    "        for r, c in history:\n",
    "            # keep only valid roles\n",
    "            rr = \"assistant\" if r == \"assistant\" else \"user\"\n",
    "            msgs.append({\"role\": rr, \"content\": c})\n",
    "        msgs.append({\"role\": \"user\", \"content\": user})\n",
    "\n",
    "        try:\n",
    "            return self.tok.apply_chat_template(\n",
    "                msgs,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        except Exception:\n",
    "            # fallback raw text prompt\n",
    "            out = [f\"[SYSTEM]\\n{system}\\n\"]\n",
    "            for r, c in history:\n",
    "                out.append(f\"[{r.upper()}]\\n{c}\\n\")\n",
    "            out.append(f\"[USER]\\n{user}\\n\")\n",
    "            out.append(\"[ASSISTANT]\\n\")\n",
    "            return \"\\n\".join(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ca0d1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T17:10:06.953461Z",
     "iopub.status.busy": "2026-01-01T17:10:06.953321Z",
     "iopub.status.idle": "2026-01-01T17:10:06.962283Z",
     "shell.execute_reply": "2026-01-01T17:10:06.961878Z"
    },
    "papermill": {
     "duration": 0.012462,
     "end_time": "2026-01-01T17:10:06.963089",
     "exception": false,
     "start_time": "2026-01-01T17:10:06.950627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 8/13 — PYTHON TOOL POOL (Subprocess, Kaggle-stable) [FIXED]\n",
    "# - One ToolPool only\n",
    "# - Has .run(wid, code, timeout_s) -> (out, ok) to match engine\n",
    "# =========================\n",
    "import os, sys, json, queue, uuid, subprocess, threading\n",
    "from contextlib import contextmanager\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "_WORKER_SRC = r\"\"\"\n",
    "import sys, json, traceback, io, contextlib, math, random, itertools\n",
    "try:\n",
    "    import sympy as sp\n",
    "except Exception:\n",
    "    sp = None\n",
    "\n",
    "G = {\"math\": math, \"random\": random, \"itertools\": itertools, \"sp\": sp}\n",
    "\n",
    "def handle(req):\n",
    "    code = req.get(\"code\", \"\")\n",
    "    out_io = io.StringIO()\n",
    "    ok = True\n",
    "    with contextlib.redirect_stdout(out_io), contextlib.redirect_stderr(out_io):\n",
    "        try:\n",
    "            exec(compile(code, \"<tool>\", \"exec\"), G, G)\n",
    "        except Exception:\n",
    "            ok = False\n",
    "            traceback.print_exc(limit=3)\n",
    "\n",
    "    txt = out_io.getvalue().strip()\n",
    "    if not txt:\n",
    "        for k in (\"__result__\", \"result\", \"ans\", \"_\"):\n",
    "            if k in G:\n",
    "                try:\n",
    "                    txt = str(G[k])\n",
    "                    break\n",
    "                except Exception:\n",
    "                    pass\n",
    "    if not txt:\n",
    "        txt = \"[WARN] No output. Use print().\"\n",
    "    if len(txt) > 2000:\n",
    "        txt = txt[:2000] + \"\\n[...TRUNCATED...]\"\n",
    "    return {\"id\": req.get(\"id\"), \"ok\": ok, \"out\": txt}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    try:\n",
    "        req = json.loads(line)\n",
    "    except Exception:\n",
    "        continue\n",
    "    resp = handle(req)\n",
    "    sys.stdout.write(json.dumps(resp, ensure_ascii=False) + \"\\n\")\n",
    "    sys.stdout.flush()\n",
    "\"\"\"\n",
    "\n",
    "class SubprocessToolWorker:\n",
    "    def __init__(self):\n",
    "        self.proc: Optional[subprocess.Popen] = None\n",
    "        self._pending: Dict[str, \"queue.Queue[dict]\"] = {}\n",
    "        self._lock = threading.Lock()\n",
    "        self._reader_thread: Optional[threading.Thread] = None\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        self.stop()\n",
    "        env = dict(os.environ)\n",
    "        env[\"PYTHONUNBUFFERED\"] = \"1\"\n",
    "        self.proc = subprocess.Popen(\n",
    "            [sys.executable, \"-u\", \"-c\", _WORKER_SRC],\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            env=env,\n",
    "        )\n",
    "\n",
    "        def _reader():\n",
    "            assert self.proc is not None and self.proc.stdout is not None\n",
    "            for line in self.proc.stdout:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    msg = json.loads(line)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                rid = msg.get(\"id\")\n",
    "                if not rid:\n",
    "                    continue\n",
    "                q = self._pending.pop(rid, None)\n",
    "                if q is not None:\n",
    "                    q.put(msg)\n",
    "\n",
    "        self._reader_thread = threading.Thread(target=_reader, daemon=True)\n",
    "        self._reader_thread.start()\n",
    "\n",
    "    def is_alive(self) -> bool:\n",
    "        return self.proc is not None and (self.proc.poll() is None)\n",
    "\n",
    "    def stop(self):\n",
    "        if self.proc is not None:\n",
    "            try:\n",
    "                self.proc.kill()\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                self.proc.wait(timeout=1)\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.proc = None\n",
    "        self._pending.clear()\n",
    "\n",
    "    def execute(self, code: str, timeout_s: float) -> Tuple[str, bool]:\n",
    "        if not self.is_alive():\n",
    "            self.start()\n",
    "        assert self.proc is not None and self.proc.stdin is not None\n",
    "\n",
    "        rid = uuid.uuid4().hex\n",
    "        q: \"queue.Queue[dict]\" = queue.Queue(maxsize=1)\n",
    "        self._pending[rid] = q\n",
    "\n",
    "        payload = {\"id\": rid, \"code\": code}\n",
    "        with self._lock:\n",
    "            try:\n",
    "                self.proc.stdin.write(json.dumps(payload, ensure_ascii=False) + \"\\n\")\n",
    "                self.proc.stdin.flush()\n",
    "            except Exception:\n",
    "                self.start()\n",
    "                return \"[PYTHON_ERROR] worker write failed\", False\n",
    "\n",
    "        try:\n",
    "            msg = q.get(timeout=timeout_s)\n",
    "        except queue.Empty:\n",
    "            self.start()\n",
    "            return \"[PYTHON_TIMEOUT]\", False\n",
    "\n",
    "        out = (msg.get(\"out\", \"\") or \"\").strip() or \"[WARN] No output. Use print().\"\n",
    "        ok = bool(msg.get(\"ok\", False))\n",
    "        return out, ok\n",
    "\n",
    "class ToolPool:\n",
    "    def __init__(self, size: int):\n",
    "        self.size = max(1, int(size))\n",
    "        self.workers: List[SubprocessToolWorker] = [SubprocessToolWorker() for _ in range(self.size)]\n",
    "\n",
    "    def run(self, wid: int, code: str, timeout_s: float) -> Tuple[str, bool]:\n",
    "        w = self.workers[int(wid) % self.size]\n",
    "        return w.execute(code, timeout_s)\n",
    "\n",
    "    def close(self):\n",
    "        for w in self.workers:\n",
    "            w.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96e570ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T17:10:06.968158Z",
     "iopub.status.busy": "2026-01-01T17:10:06.967992Z",
     "iopub.status.idle": "2026-01-01T17:12:37.344559Z",
     "shell.execute_reply": "2026-01-01T17:12:37.344114Z"
    },
    "papermill": {
     "duration": 150.380164,
     "end_time": "2026-01-01T17:12:37.345446",
     "exception": false,
     "start_time": "2026-01-01T17:10:06.965282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48664cfc50042af8251e9d3ab068cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[backend] loaded hf | USE_FLASH_ATTN=0 | GEN_BATCH_SIZE=8 | MAX_MODEL_LEN=12288 | DTYPE=bfloat16\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 9/13 — BACKEND (HF) + tokenizer  [FIX FP8 crash]\n",
    "# FIX:\n",
    "# - use torch_dtype=... (NOT dtype=...)\n",
    "# - prefer full model on GPU0 to avoid CPU offload -> fp32 -> FP8 autocast ValueError\n",
    "# - left padding for decoder-only\n",
    "# - safe sampling clamp\n",
    "# - remove duplicate InfNanRemoveLogitsProcessor warning by NOT passing custom logits_processor\n",
    "# =========================\n",
    "import os, importlib.util\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class BackendBase:\n",
    "    def generate(self, prompts: List[str], *, temperature: float, top_p: float, max_tokens: int) -> List[str]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def _dtype_from_str(s: str) -> torch.dtype:\n",
    "    s = (s or \"\").lower()\n",
    "    if \"fp16\" in s or \"float16\" in s:\n",
    "        return torch.float16\n",
    "    if \"bf16\" in s or \"bfloat\" in s:\n",
    "        return torch.bfloat16\n",
    "    return torch.bfloat16\n",
    "\n",
    "class HFBackend(BackendBase):\n",
    "    def __init__(self, model_path: str, max_model_len: int):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        use_flash = os.getenv(\"USE_FLASH_ATTN\", \"0\") == \"1\"\n",
    "        has_flash = importlib.util.find_spec(\"flash_attn\") is not None\n",
    "        attn_impl = \"flash_attention_2\" if (use_flash and has_flash) else \"sdpa\"\n",
    "\n",
    "        torch_dtype = _dtype_from_str(DTYPE)\n",
    "\n",
    "        common_kwargs = dict(\n",
    "            torch_dtype=torch_dtype,          # IMPORTANT: must be torch_dtype\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "\n",
    "        # Prefer full model on GPU 0 (avoid CPU offload -> fp32 -> FP8 wrapper crash)\n",
    "        try:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                device_map={\"\": 0},\n",
    "                attn_implementation=attn_impl,\n",
    "                **common_kwargs,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[hf] device_map={{'':0}} load failed -> fallback device_map='auto': {type(e).__name__}: {e}\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                device_map=\"auto\",\n",
    "                attn_implementation=attn_impl,\n",
    "                **common_kwargs,\n",
    "            )\n",
    "\n",
    "        self.model.eval()\n",
    "        self.max_model_len = int(max_model_len)\n",
    "\n",
    "        self.eos_id = self.tokenizer.eos_token_id\n",
    "        self.pad_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.eos_id\n",
    "        self.bs = int(GEN_BATCH_SIZE)\n",
    "\n",
    "        # generation safety knobs (if supported)\n",
    "        try:\n",
    "            self.model.generation_config.remove_invalid_values = True\n",
    "            self.model.generation_config.renormalize_logits = True\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # where inputs should live (embed device)\n",
    "        try:\n",
    "            self.input_device = self.model.get_input_embeddings().weight.device\n",
    "        except Exception:\n",
    "            self.input_device = next(self.model.parameters()).device\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompts: List[str], *, temperature: float, top_p: float, max_tokens: int) -> List[str]:\n",
    "        outs: List[str] = []\n",
    "\n",
    "        # clamp sampling params to avoid weirdness\n",
    "        t = float(temperature) if temperature is not None else 0.0\n",
    "        p = float(top_p) if top_p is not None else 1.0\n",
    "        t = 0.0 if t < 1e-6 else min(1.2, max(0.05, t))\n",
    "        p = min(1.0, max(0.05, p))\n",
    "        do_sample = t > 1e-6\n",
    "\n",
    "        i = 0\n",
    "        while i < len(prompts):\n",
    "            bs = max(1, int(self.bs))\n",
    "            batch = prompts[i:i+bs]\n",
    "\n",
    "            enc = self.tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_model_len,\n",
    "            )\n",
    "            enc = {k: v.to(self.input_device) for k, v in enc.items()}\n",
    "            input_len = int(enc[\"input_ids\"].shape[1])\n",
    "\n",
    "            gen_kwargs = dict(\n",
    "                **enc,\n",
    "                max_new_tokens=int(max_tokens),\n",
    "                use_cache=True,\n",
    "                pad_token_id=self.pad_id,\n",
    "                eos_token_id=self.eos_id,\n",
    "                do_sample=do_sample,\n",
    "            )\n",
    "            if do_sample:\n",
    "                gen_kwargs[\"temperature\"] = t\n",
    "                gen_kwargs[\"top_p\"] = p\n",
    "\n",
    "            # Prefer built-in sanitizers (avoid duplicate InfNanRemoveLogitsProcessor warning)\n",
    "            try:\n",
    "                gen_kwargs[\"remove_invalid_values\"] = True\n",
    "                gen_kwargs[\"renormalize_logits\"] = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                gen = self.model.generate(**gen_kwargs)\n",
    "            except RuntimeError as e:\n",
    "                msg = str(e).lower()\n",
    "                if \"out of memory\" in msg or \"cuda out of memory\" in msg:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    self.bs = max(1, self.bs // 2)\n",
    "                    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "                        print(f\"[hf] OOM -> backoff GEN_BATCH_SIZE to {self.bs}\")\n",
    "                    continue\n",
    "                raise\n",
    "\n",
    "            for j in range(gen.shape[0]):\n",
    "                tail = gen[j, input_len:]\n",
    "                outs.append(self.tokenizer.decode(tail, skip_special_tokens=True))\n",
    "\n",
    "            i += bs\n",
    "\n",
    "        return outs\n",
    "\n",
    "_backend = HFBackend(MODEL_PATH, max_model_len=MAX_MODEL_LEN)\n",
    "_backend_name = \"hf\"\n",
    "print(f\"[backend] loaded {_backend_name} | USE_FLASH_ATTN={os.getenv('USE_FLASH_ATTN','0')} | GEN_BATCH_SIZE={GEN_BATCH_SIZE} | MAX_MODEL_LEN={MAX_MODEL_LEN} | DTYPE={DTYPE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0e960a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T17:12:37.351544Z",
     "iopub.status.busy": "2026-01-01T17:12:37.351076Z",
     "iopub.status.idle": "2026-01-01T17:12:37.355216Z",
     "shell.execute_reply": "2026-01-01T17:12:37.354785Z"
    },
    "papermill": {
     "duration": 0.008129,
     "end_time": "2026-01-01T17:12:37.356081",
     "exception": false,
     "start_time": "2026-01-01T17:12:37.347952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 10/13 — TIME MANAGER\n",
    "# =========================\n",
    "class TimeManager:\n",
    "    def __init__(self, hard_wall_s: int, total_questions: int):\n",
    "        self.start = time.time()\n",
    "        self.deadline = self.start + int(hard_wall_s)\n",
    "        self.total = max(1, int(total_questions))\n",
    "        self.done = 0\n",
    "\n",
    "    def remaining(self) -> float:\n",
    "        return max(0.0, self.deadline - time.time())\n",
    "\n",
    "    def budget(self, weight: float) -> float:\n",
    "        rem = self.remaining()\n",
    "        left = max(1, self.total - self.done)\n",
    "        base = rem / left\n",
    "        return clamp(base * float(weight), MIN_BUDGET_S, MAX_BUDGET_S)\n",
    "\n",
    "    def mark_done(self) -> None:\n",
    "        self.done += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1507730",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T17:12:37.361581Z",
     "iopub.status.busy": "2026-01-01T17:12:37.361432Z",
     "iopub.status.idle": "2026-01-01T17:12:37.366896Z",
     "shell.execute_reply": "2026-01-01T17:12:37.366499Z"
    },
    "papermill": {
     "duration": 0.009154,
     "end_time": "2026-01-01T17:12:37.367703",
     "exception": false,
     "start_time": "2026-01-01T17:12:37.358549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 11/13 — CANDIDATES + WEIGHTED VOTE\n",
    "# =========================\n",
    "@dataclass\n",
    "class Candidate:\n",
    "    answer: Optional[int]\n",
    "    raw: str\n",
    "    tool_calls: int\n",
    "    tool_errors: int\n",
    "    elapsed: float\n",
    "    stage: int\n",
    "    verified: Optional[bool] = None  # PASS->True, FAIL->False, UNKNOWN->None\n",
    "\n",
    "def cand_weight(c: Candidate) -> float:\n",
    "    if c.answer is None:\n",
    "        return 0.0\n",
    "    w = 1.0\n",
    "    if c.tool_calls > 0 and c.tool_errors == 0:\n",
    "        w += 0.9\n",
    "    w -= 0.75 * c.tool_errors\n",
    "    w += max(0.0, 0.35 - 0.015 * c.elapsed)\n",
    "    if c.verified is True:\n",
    "        w += 2.25\n",
    "    if c.verified is False:\n",
    "        w -= 2.25\n",
    "    return max(0.0, w)\n",
    "\n",
    "def weighted_vote(cands: List[Candidate]) -> Tuple[Optional[int], float, Dict[int, float]]:\n",
    "    scores: Dict[int, float] = {}\n",
    "    total = 0.0\n",
    "    for c in cands:\n",
    "        if c.answer is None:\n",
    "            continue\n",
    "        w = cand_weight(c)\n",
    "        total += w\n",
    "        scores[c.answer] = scores.get(c.answer, 0.0) + w\n",
    "    if not scores or total <= 1e-9:\n",
    "        return None, 0.0, {}\n",
    "    best = max(scores.items(), key=lambda kv: kv[1])[0]\n",
    "    ratio = scores[best] / total\n",
    "    return best, ratio, scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdffbf69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T17:12:37.373253Z",
     "iopub.status.busy": "2026-01-01T17:12:37.373099Z",
     "iopub.status.idle": "2026-01-01T17:12:37.389885Z",
     "shell.execute_reply": "2026-01-01T17:12:37.389507Z"
    },
    "papermill": {
     "duration": 0.020701,
     "end_time": "2026-01-01T17:12:37.390709",
     "exception": false,
     "start_time": "2026-01-01T17:12:37.370008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 12/13 — TIR ENGINE + VERIFIER + SELECTOR  [FIXED PARSING]\n",
    "# =========================\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "@dataclass\n",
    "class TIRState:\n",
    "    history: List[Tuple[str, str]]\n",
    "    hint: str\n",
    "    worker_id: int\n",
    "    done: bool = False\n",
    "    answer: Optional[int] = None\n",
    "    tool_calls: int = 0\n",
    "    tool_errors: int = 0\n",
    "    raw_last: str = \"\"\n",
    "\n",
    "class TIRBatchEngine:\n",
    "    def __init__(self, backend: BackendBase, pb: PromptBuilder, tools: ToolPool):\n",
    "        self.backend = backend\n",
    "        self.pb = pb\n",
    "        self.tools = tools\n",
    "\n",
    "    def _turn(\n",
    "        self,\n",
    "        problem: str,\n",
    "        states: List[TIRState],\n",
    "        *,\n",
    "        temperature: float,\n",
    "        top_p: float,\n",
    "        max_tokens: int,\n",
    "        time_left_s: float\n",
    "    ) -> None:\n",
    "        active = [i for i, s in enumerate(states) if not s.done]\n",
    "        if not active:\n",
    "            return\n",
    "\n",
    "        prompts: List[str] = []\n",
    "        for i in active:\n",
    "            s = states[i]\n",
    "            user = f\"{problem}\\n\\nHint: {s.hint}\"\n",
    "            prompts.append(self.pb.render(SYSTEM_TIR, user, s.history))\n",
    "\n",
    "        outs = self.backend.generate(prompts, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
    "\n",
    "        tool_jobs: List[Tuple[int, int, str]] = []  # (state_idx, worker_id, code)\n",
    "\n",
    "        for idx, raw in zip(active, outs):\n",
    "            st = states[idx]\n",
    "            st.raw_last = raw  # IMPORTANT: store RAW output (may include tool/boxed inside <think>)\n",
    "\n",
    "            ans = parse_boxed_int(raw)\n",
    "            if ans is not None:\n",
    "                st.done = True\n",
    "                st.answer = ans\n",
    "                continue\n",
    "\n",
    "            code = parse_tool_code(raw)\n",
    "            if code:\n",
    "                st.tool_calls += 1\n",
    "                tool_jobs.append((idx, st.worker_id, code))\n",
    "                continue\n",
    "\n",
    "            # Non-conforming output -> add clean snippet to history and remind protocol\n",
    "            snippet = clean_for_history(raw, limit=800)\n",
    "            if snippet:\n",
    "                st.history.append((\"assistant\", snippet))\n",
    "            st.history.append((\"user\", \"Output ONLY either a <tool:python> block OR one line \\\\boxed{integer}.\"))\n",
    "            st.history = trim_history(st.history)\n",
    "\n",
    "        if not tool_jobs:\n",
    "            return\n",
    "\n",
    "        def run_one(job: Tuple[int, int, str]) -> Tuple[int, str, str, bool]:\n",
    "            i, wid, code = job\n",
    "            py_out, ok = self.tools.run(\n",
    "                wid,\n",
    "                code,\n",
    "                timeout_s=min(TOOL_TIMEOUT_S, max(1.0, time_left_s))\n",
    "            )\n",
    "            return i, code, py_out, ok\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=min(len(tool_jobs), TOOL_THREAD_WORKERS)) as ex:\n",
    "            futs = [ex.submit(run_one, j) for j in tool_jobs]\n",
    "            for fut in as_completed(futs):\n",
    "                i, code, py_out, ok = fut.result()\n",
    "                st = states[i]\n",
    "                if (not ok) or (\"PYTHON_TIMEOUT\" in py_out) or (\"Traceback\" in py_out):\n",
    "                    st.tool_errors += 1\n",
    "                st.history.append((\"assistant\", f\"<tool:python>\\n{code}\\n</tool:python>\"))\n",
    "                st.history.append((\"user\", f\"Python output:\\n{py_out}\"))\n",
    "                st.history = trim_history(st.history)\n",
    "\n",
    "    def run_progressive(\n",
    "        self,\n",
    "        problem: str,\n",
    "        *,\n",
    "        max_k: int,\n",
    "        batch_k: int,\n",
    "        budget_s: float,\n",
    "        stage: int,\n",
    "        max_tokens: int,\n",
    "        temperature: float,\n",
    "        top_p: float,\n",
    "        early_stop_ratio: float,\n",
    "    ) -> List[Candidate]:\n",
    "        t0 = time.time()\n",
    "        cands: List[Candidate] = []\n",
    "        created = 0\n",
    "\n",
    "        while created < max_k and (time.time() - t0) < budget_s:\n",
    "            add = min(batch_k, max_k - created)\n",
    "            states = [\n",
    "                TIRState(history=[], hint=HINTS[(created + i) % len(HINTS)], worker_id=(created + i) % TOOL_POOL_SIZE)\n",
    "                for i in range(add)\n",
    "            ]\n",
    "            created += add\n",
    "\n",
    "            for _ in range(MAX_TURNS):\n",
    "                if time.time() - t0 >= budget_s:\n",
    "                    break\n",
    "                self._turn(\n",
    "                    problem,\n",
    "                    states,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    max_tokens=max_tokens,\n",
    "                    time_left_s=budget_s - (time.time() - t0),\n",
    "                )\n",
    "                if all(s.done for s in states):\n",
    "                    break\n",
    "\n",
    "            elapsed = time.time() - t0\n",
    "            for s in states:\n",
    "                ans = s.answer\n",
    "                if ans is None:\n",
    "                    # try boxed/tool parsing on RAW output; fallback last int\n",
    "                    ans = parse_boxed_int(s.raw_last) or fallback_last_int(s.raw_last)\n",
    "\n",
    "                cands.append(\n",
    "                    Candidate(\n",
    "                        answer=ans,\n",
    "                        raw=s.raw_last,\n",
    "                        tool_calls=s.tool_calls,\n",
    "                        tool_errors=s.tool_errors,\n",
    "                        elapsed=elapsed,\n",
    "                        stage=stage,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            best, ratio, _ = weighted_vote(cands)\n",
    "            if best is not None and ratio >= early_stop_ratio:\n",
    "                break\n",
    "\n",
    "        return cands\n",
    "\n",
    "class Verifier:\n",
    "    def __init__(self, backend: BackendBase, pb: PromptBuilder, tools: ToolPool):\n",
    "        self.backend = backend\n",
    "        self.pb = pb\n",
    "        self.tools = tools\n",
    "\n",
    "    def _extract_verdict(self, raw: str) -> Optional[bool]:\n",
    "        \"\"\"\n",
    "        Return True for PASS, False for FAIL, None for UNKNOWN/invalid.\n",
    "        Robust even if verdict appears inside <think>.\n",
    "        \"\"\"\n",
    "        t = remove_think_tags(raw).strip()\n",
    "        if not t:\n",
    "            return None\n",
    "        lines = [ln.strip().upper() for ln in t.splitlines() if ln.strip()]\n",
    "        if not lines:\n",
    "            return None\n",
    "        last = lines[-1]\n",
    "        if last == \"PASS\":\n",
    "            return True\n",
    "        if last == \"FAIL\":\n",
    "            return False\n",
    "        if last == \"UNKNOWN\":\n",
    "            return None\n",
    "        return None\n",
    "\n",
    "    def verify(self, problem: str, answer: int, budget_s: float) -> Optional[bool]:\n",
    "        t0 = time.time()\n",
    "        history: List[Tuple[str, str]] = []\n",
    "        user = f\"Problem:\\n{problem}\\n\\nProposed answer A = {answer}\\n\"\n",
    "\n",
    "        for _ in range(6):\n",
    "            if time.time() - t0 >= budget_s:\n",
    "                return None\n",
    "\n",
    "            prompt = self.pb.render(SYSTEM_VERIFY, user, history)\n",
    "            raw = self.backend.generate([prompt], temperature=0.0, top_p=1.0, max_tokens=650)[0]\n",
    "\n",
    "            code = parse_tool_code(raw)\n",
    "            if code:\n",
    "                py_out, ok = self.tools.run(\n",
    "                    0,\n",
    "                    code,\n",
    "                    timeout_s=min(TOOL_TIMEOUT_S, max(1.0, budget_s - (time.time() - t0)))\n",
    "                )\n",
    "                history.append((\"assistant\", f\"<tool:python>\\n{code}\\n</tool:python>\"))\n",
    "                history.append((\"user\", f\"Python output:\\n{py_out}\"))\n",
    "                history = trim_history(history, 10)\n",
    "                continue\n",
    "\n",
    "            verdict = self._extract_verdict(raw)\n",
    "            if verdict is True:\n",
    "                return True\n",
    "            if verdict is False:\n",
    "                return False\n",
    "\n",
    "            # If not cleanly formatted, push back\n",
    "            snippet = clean_for_history(raw, limit=800)\n",
    "            if snippet:\n",
    "                history.append((\"assistant\", snippet))\n",
    "            history.append((\"user\", \"Return ONLY one final line: PASS or FAIL or UNKNOWN.\"))\n",
    "            history = trim_history(history, 10)\n",
    "\n",
    "        return None\n",
    "\n",
    "class Selector:\n",
    "    def __init__(self, backend: BackendBase, pb: PromptBuilder):\n",
    "        self.backend = backend\n",
    "        self.pb = pb\n",
    "\n",
    "    def select(self, problem: str, scores: Dict[int, float]) -> Optional[int]:\n",
    "        if not scores:\n",
    "            return None\n",
    "        items = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[:8]\n",
    "        evidence = \"\\n\".join([f\"- {a}: score={s:.3f}\" for a, s in items])\n",
    "        prompt = self.pb.render(SYSTEM_SELECT, f\"Problem:\\n{problem}\\n\\nCandidate scores:\\n{evidence}\\n\", [])\n",
    "        raw = self.backend.generate([prompt], temperature=0.0, top_p=1.0, max_tokens=220)[0]\n",
    "        return parse_boxed_int(raw) or fallback_last_int(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0047ae7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T17:12:37.396255Z",
     "iopub.status.busy": "2026-01-01T17:12:37.395922Z",
     "iopub.status.idle": "2026-01-01T18:35:11.484566Z",
     "shell.execute_reply": "2026-01-01T18:35:11.484073Z"
    },
    "papermill": {
     "duration": 4954.095308,
     "end_time": "2026-01-01T18:35:11.488204",
     "exception": false,
     "start_time": "2026-01-01T17:12:37.392896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[solver] backend = hf\n",
      "[dev] solver loaded. Running dev_eval() on reference.csv ...\n",
      "[dev_eval] 5/10  elapsed=1872.4s  acc=0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dev_eval] 10/10  elapsed=4953.6s  acc=0.0%\n",
      "[dev_eval] FINAL: 0/10 = 0.0%  | time=4953.6s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 13/13 — SOLVER + DEV TEST + KAGGLE HOOK\n",
    "# =========================\n",
    "class AIMO3Solver:\n",
    "    def __init__(self):\n",
    "        self.pb = PromptBuilder(tokenizer)\n",
    "        self.tools = ToolPool(TOOL_POOL_SIZE)\n",
    "        self.backend = _backend\n",
    "        self.tm = TimeManager(HARD_WALL_SECONDS, TOTAL_QUESTIONS)\n",
    "        self.engine = TIRBatchEngine(self.backend, self.pb, self.tools)\n",
    "        self.verifier = Verifier(self.backend, self.pb, self.tools)\n",
    "        self.selector = Selector(self.backend, self.pb)\n",
    "\n",
    "        if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "            print(f\"[solver] backend = {_backend_name}\")\n",
    "\n",
    "    def close(self):\n",
    "        self.tools.close()\n",
    "\n",
    "    def solve_problem(self, problem: str) -> int:\n",
    "        plan = route_problem(problem)\n",
    "        rem = self.tm.remaining()\n",
    "        if rem < 5.0:\n",
    "            return 0\n",
    "\n",
    "        budget = min(self.tm.budget(plan.budget_weight), rem)\n",
    "\n",
    "        # Stage 1\n",
    "        c1 = self.engine.run_progressive(\n",
    "            problem,\n",
    "            max_k=plan.stage1_max_k,\n",
    "            batch_k=STAGE1_BATCH,\n",
    "            budget_s=0.38 * budget,\n",
    "            stage=1,\n",
    "            max_tokens=plan.stage1_max_tokens,\n",
    "            temperature=plan.temp1,\n",
    "            top_p=plan.top_p1,\n",
    "            early_stop_ratio=CONFIDENT_RATIO,\n",
    "        )\n",
    "        best, ratio, scores = weighted_vote(c1)\n",
    "        if best is not None and ratio >= CONFIDENT_RATIO:\n",
    "            self.tm.mark_done()\n",
    "            return mod100000(best)\n",
    "\n",
    "        # Stage 2\n",
    "        c2 = self.engine.run_progressive(\n",
    "            problem,\n",
    "            max_k=plan.stage2_max_k,\n",
    "            batch_k=STAGE2_BATCH,\n",
    "            budget_s=0.50 * budget,\n",
    "            stage=2,\n",
    "            max_tokens=plan.stage2_max_tokens,\n",
    "            temperature=plan.temp2,\n",
    "            top_p=plan.top_p2,\n",
    "            early_stop_ratio=CONFIDENT_RATIO,\n",
    "        )\n",
    "\n",
    "        all_c = c1 + c2\n",
    "        best, ratio, scores = weighted_vote(all_c)\n",
    "\n",
    "        if best is None:\n",
    "            self.tm.mark_done()\n",
    "            return 0\n",
    "\n",
    "        # Verifier-on-uncertainty\n",
    "        if ratio < VERIFY_RATIO and budget >= 25.0:\n",
    "            top = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[:VERIFY_TOP_N]\n",
    "            per_verify_budget = 0.10 * budget / max(1, len(top))\n",
    "\n",
    "            for ans, _ in top:\n",
    "                verdict = self.verifier.verify(problem, ans, budget_s=per_verify_budget)\n",
    "                for c in all_c:\n",
    "                    if c.answer == ans:\n",
    "                        c.verified = verdict\n",
    "\n",
    "            best, ratio, scores = weighted_vote(all_c)\n",
    "\n",
    "        # Deterministic selector if still not confident\n",
    "        final = best\n",
    "        if ratio < 0.80 and (0.07 * budget) >= 3.0:\n",
    "            sel = self.selector.select(problem, scores)\n",
    "            if sel is not None:\n",
    "                final = sel\n",
    "\n",
    "        self.tm.mark_done()\n",
    "        return mod100000(final)\n",
    "\n",
    "solver = AIMO3Solver()\n",
    "\n",
    "def predict(id_: \"pl.Series\", problem: \"pl.Series\"):\n",
    "    if pl is not None and isinstance(id_, pl.Series):\n",
    "        pid = id_.item(0)\n",
    "        prob = problem.item(0)\n",
    "        ans = solver.solve_problem(prob)\n",
    "        return pl.DataFrame({\"id\": [pid], \"answer\": [ans]})\n",
    "    else:\n",
    "        pid = id_[0] if hasattr(id_, \"__len__\") else id_\n",
    "        prob = problem[0] if hasattr(problem, \"__len__\") else problem\n",
    "        ans = solver.solve_problem(prob)\n",
    "        return pd.DataFrame({\"id\": [pid], \"answer\": [ans]})\n",
    "\n",
    "# ---- DEV EVAL ----\n",
    "def _find_comp_file(fname: str) -> Optional[str]:\n",
    "    hits = glob.glob(f\"/kaggle/input/*/{fname}\")\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "def dev_eval(n: int = 30):\n",
    "    ref_path = _find_comp_file(\"reference.csv\")\n",
    "    if not ref_path:\n",
    "        print(\"[dev_eval] reference.csv not found in /kaggle/input/*/\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(ref_path)\n",
    "    if \"problem\" not in df.columns:\n",
    "        print(\"[dev_eval] reference.csv missing 'problem' column\")\n",
    "        return\n",
    "\n",
    "    has_gt = \"answer\" in df.columns\n",
    "    gt = df.set_index(\"id\")[\"answer\"].to_dict() if has_gt else None\n",
    "\n",
    "    n = min(n, len(df))\n",
    "    sub = df.iloc[:n].copy()\n",
    "\n",
    "    t0 = time.time()\n",
    "    correct = 0\n",
    "    done = 0\n",
    "\n",
    "    for _, row in sub.iterrows():\n",
    "        pid = row[\"id\"]\n",
    "        prob = row[\"problem\"]\n",
    "        ans = solver.solve_problem(prob)\n",
    "        done += 1\n",
    "        if has_gt and int(ans) == int(gt[pid]):\n",
    "            correct += 1\n",
    "\n",
    "        if done % 5 == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            if has_gt:\n",
    "                print(f\"[dev_eval] {done}/{n}  elapsed={elapsed:.1f}s  acc={100*correct/done:.1f}%\")\n",
    "            else:\n",
    "                print(f\"[dev_eval] {done}/{n}  elapsed={elapsed:.1f}s\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    if has_gt:\n",
    "        print(f\"[dev_eval] FINAL: {correct}/{n} = {100*correct/n:.1f}%  | time={elapsed:.1f}s\")\n",
    "    else:\n",
    "        print(f\"[dev_eval] FINAL: done {n} problems | time={elapsed:.1f}s | (no ground truth in reference.csv)\")\n",
    "\n",
    "# ---- Kaggle Inference Server ----\n",
    "import kaggle_evaluation.aimo_3_inference_server as aimo3\n",
    "inference_server = aimo3.AIMO3InferenceServer(predict)\n",
    "\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    print(\"[dev] solver loaded. Running dev_eval() on reference.csv ...\")\n",
    "    dev_eval(n=int(os.getenv(\"DEV_N\", \"30\")))\n",
    "\n",
    "    if os.getenv(\"DEV_RUN_GATEWAY\", \"0\") == \"1\":\n",
    "        ref_path = _find_comp_file(\"reference.csv\")\n",
    "        df = pd.read_csv(ref_path)\n",
    "        tmp = df[[\"id\", \"problem\"]].head(int(os.getenv(\"DEV_GATEWAY_N\", \"10\")))\n",
    "        tmp_path = \"ref_input_head.csv\"\n",
    "        tmp.to_csv(tmp_path, index=False)\n",
    "        print(f\"[dev] run_local_gateway on {tmp_path}\")\n",
    "        inference_server.run_local_gateway((tmp_path,))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaH100",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301526,
     "sourceId": 363148,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 396608,
     "sourceId": 499291,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 396626,
     "sourceId": 499313,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 422384,
     "modelInstanceId": 404485,
     "sourceId": 510391,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5214.988567,
   "end_time": "2026-01-01T18:35:14.206679",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-01T17:08:19.218112",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03b79192e21c45e49417eb67cf2c5851": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "175b8ebe84ad4764afdec73e3e44ba81": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2cdb58e43ab847ecbe82b068f1a0993d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_03b79192e21c45e49417eb67cf2c5851",
       "placeholder": "​",
       "style": "IPY_MODEL_ca969f8c1e5a4d0d87eb2ac7a80e0f92",
       "tabbable": null,
       "tooltip": null,
       "value": " 4/4 [02:16&lt;00:00, 27.99s/it]"
      }
     },
     "58f9080775334caa9e463652683fa0d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_175b8ebe84ad4764afdec73e3e44ba81",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_75eb131e130e44c9b2060902abd3d2da",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     },
     "6f86c8df7f354a5782db617d4d2015d2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "74c80f251dd84c9faa4ceaca149abe89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "75eb131e130e44c9b2060902abd3d2da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a9672ca8cc6d4b6895125baff6330fcc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca969f8c1e5a4d0d87eb2ac7a80e0f92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e48664cfc50042af8251e9d3ab068cab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e4d92c9615d646adafff7ae79321fc29",
        "IPY_MODEL_58f9080775334caa9e463652683fa0d9",
        "IPY_MODEL_2cdb58e43ab847ecbe82b068f1a0993d"
       ],
       "layout": "IPY_MODEL_6f86c8df7f354a5782db617d4d2015d2",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e4d92c9615d646adafff7ae79321fc29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a9672ca8cc6d4b6895125baff6330fcc",
       "placeholder": "​",
       "style": "IPY_MODEL_74c80f251dd84c9faa4ceaca149abe89",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
