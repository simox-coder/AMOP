{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":363148,"sourceType":"modelInstanceVersion","modelInstanceId":301526,"modelId":322000},{"sourceId":499291,"sourceType":"modelInstanceVersion","modelInstanceId":396608,"modelId":322000},{"sourceId":499313,"sourceType":"modelInstanceVersion","modelInstanceId":396626,"modelId":322000},{"sourceId":510391,"sourceType":"modelInstanceVersion","modelInstanceId":404485,"modelId":422384}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d31b2c1d-dff8-48a4-96e9-b04057bf99c8","cell_type":"code","source":"# ============================================================\n# AIMO3 TOP-ORIENTED SUBMISSION — Qwen-3 30B-A3B Thinking (H100)\n# - Strong TIR + python tool (NO jupyter KernelManager)\n# - Best-of-N + verifier-on-uncertainty + deterministic selector\n# - REAL dev evaluation on reference.csv (accuracy + latency)\n# ============================================================\n\nfrom __future__ import annotations\n\n# =========================\n# CELL 1/13 — CONFIG\n# =========================\nimport os, re, time, math, json, random, glob\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nMODEL_PATH = \"/kaggle/input/qwen-3/transformers/8b-fp8/1\"\nSEED = int(os.getenv(\"SEED\", \"42\"))\n\n# Warm/cache\nWARMUP_USR_LIB = os.getenv(\"WARMUP_USR_LIB\", \"0\") == \"1\"\nCACHE_MODEL_FILES = os.getenv(\"CACHE_MODEL_FILES\", \"1\") == \"1\"\nCACHE_CHUNK_MB = int(os.getenv(\"CACHE_CHUNK_MB\", \"512\"))\nCACHE_WORKERS = int(os.getenv(\"CACHE_WORKERS\", \"8\"))\nCACHE_EXTS = (\".safetensors\", \".bin\", \".pt\")\n\n# Time\nHARD_WALL_SECONDS = int(os.getenv(\"HARD_WALL_SECONDS\", str(5 * 60 * 60 - 5 * 60)))\nTOTAL_QUESTIONS = int(os.getenv(\"TOTAL_QUESTIONS\", \"110\"))\nMIN_BUDGET_S = float(os.getenv(\"MIN_BUDGET_S\", \"10\"))\nMAX_BUDGET_S = float(os.getenv(\"MAX_BUDGET_S\", \"420\"))\n\n# Tool loop\nTOOL_POOL_SIZE = int(os.getenv(\"TOOL_POOL_SIZE\", \"32\"))   # small & stable\nTOOL_TIMEOUT_S = float(os.getenv(\"TOOL_TIMEOUT_S\", \"4.0\"))\nMAX_TURNS = int(os.getenv(\"MAX_TURNS\", \"8\"))\n\n# Sampling (tune for accuracy vs time)\nSTAGE1_BATCH = int(os.getenv(\"STAGE1_BATCH\", \"3\"))\nSTAGE2_BATCH = int(os.getenv(\"STAGE2_BATCH\", \"4\"))\nCONFIDENT_RATIO = float(os.getenv(\"CONFIDENT_RATIO\", \"0.80\"))\nVERIFY_RATIO = float(os.getenv(\"VERIFY_RATIO\", \"0.70\"))\nVERIFY_TOP_N = int(os.getenv(\"VERIFY_TOP_N\", \"3\"))\n\n# Generation\nMAX_MODEL_LEN = int(os.getenv(\"MAX_MODEL_LEN\", \"16384\"))\nDTYPE = os.getenv(\"DTYPE\", \"bfloat16\")\nGPU_MEM_UTIL = float(os.getenv(\"GPU_MEM_UTIL\", \"0.92\"))\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:23:29.024792Z","iopub.execute_input":"2025-12-31T12:23:29.025008Z","iopub.status.idle":"2025-12-31T12:23:29.032447Z","shell.execute_reply.started":"2025-12-31T12:23:29.024992Z","shell.execute_reply":"2025-12-31T12:23:29.032028Z"}},"outputs":[],"execution_count":1},{"id":"2b7621d8-2cb5-4d6d-b4d5-8a7879598339","cell_type":"code","source":"# =========================\n# CELL 2/13 — WARMUP HELPERS\n# =========================\ndef warmup_usr_lib() -> None:\n    import subprocess\n    cmd = \"find /kaggle/usr/lib -type f -print0 | xargs -0 -P 32 -n 500 cat > /dev/null\"\n    subprocess.run(cmd, shell=True, check=False)\n\ndef cache_model(path: str, exts=CACHE_EXTS, num_workers: int = 8, chunk_mb: int = 256) -> None:\n    import multiprocessing\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n\n    def warmup_file(fpath: str) -> int:\n        chunk = chunk_mb * 1024 * 1024\n        total = 0\n        with open(fpath, \"rb\") as f:\n            while True:\n                b = f.read(chunk)\n                if not b:\n                    break\n                total += len(b)\n        return total\n\n    if not os.path.isdir(path):\n        return\n\n    files = [\n        os.path.join(root, name)\n        for root, _, names in os.walk(path)\n        for name in names\n        if name.endswith(exts)\n    ]\n    if not files:\n        return\n\n    try:\n        cpu = multiprocessing.cpu_count()\n    except Exception:\n        cpu = 4\n    num_workers = max(1, min(num_workers, cpu, 16))\n    files.sort(key=lambda f: os.path.getsize(f), reverse=True)\n\n    t0 = time.time()\n    total = 0\n    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n        futs = [ex.submit(warmup_file, f) for f in files]\n        for fut in as_completed(futs):\n            total += fut.result()\n\n    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n        gb = total / 1024**3\n        print(f\"[cache_model] warmed ~{gb:.2f} GB in {time.time()-t0:.1f}s\")\n\nif WARMUP_USR_LIB:\n    warmup_usr_lib()\nif CACHE_MODEL_FILES:\n    cache_model(MODEL_PATH, num_workers=CACHE_WORKERS, chunk_mb=CACHE_CHUNK_MB)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:23:29.033307Z","iopub.execute_input":"2025-12-31T12:23:29.033439Z","iopub.status.idle":"2025-12-31T12:23:31.467089Z","shell.execute_reply.started":"2025-12-31T12:23:29.033425Z","shell.execute_reply":"2025-12-31T12:23:31.466666Z"}},"outputs":[{"name":"stdout","text":"[cache_model] warmed ~8.79 GB in 2.4s\n","output_type":"stream"}],"execution_count":2},{"id":"05b3f8e3-4cee-4285-818f-e076e9fc5420","cell_type":"code","source":"# =========================\n# CELL 3/13 — IMPORTS\n# =========================\nimport numpy as np\nimport pandas as pd\n\ntry:\n    import polars as pl\nexcept Exception:\n    pl = None\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\nset_seed(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:23:31.467660Z","iopub.execute_input":"2025-12-31T12:23:31.467783Z","iopub.status.idle":"2025-12-31T12:23:36.612886Z","shell.execute_reply.started":"2025-12-31T12:23:31.467771Z","shell.execute_reply":"2025-12-31T12:23:36.612234Z"}},"outputs":[{"name":"stderr","text":"2025-12-31 12:23:35.042219: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767183815.056697     485 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767183815.061061     485 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767183815.072554     485 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767183815.072570     485 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767183815.072572     485 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767183815.072573     485 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":3},{"id":"65a2f017-fea9-4dc8-a094-7b241755d531","cell_type":"code","source":"# =========================\n# CELL 4/13 — PARSERS + UTILS\n# =========================\n_THINK_RE = re.compile(r\"<think>.*?</think>\", re.DOTALL)\n_BOXED_RE = re.compile(r\"\\\\boxed\\{([^}]*)\\}\")\n_TOOL_RE = re.compile(r\"<tool:python>\\s*(.*?)\\s*</tool:python>\", re.DOTALL)\n\ndef strip_think(text: str) -> str:\n    text = _THINK_RE.sub(\"\", text)\n    if \"</think>\" in text:\n        text = text.split(\"</think>\", 1)[-1]\n    return text.strip()\n\ndef parse_boxed_int(text: str) -> Optional[int]:\n    text = strip_think(text)\n    m = _BOXED_RE.search(text)\n    if not m:\n        return None\n    raw = m.group(1).strip()\n    if not re.fullmatch(r\"[+-]?\\d+\", raw):\n        return None\n    v = int(raw)\n    return v if 0 <= v <= 99999 else None\n\ndef parse_tool_code(text: str) -> Optional[str]:\n    text = strip_think(text)\n    m = _TOOL_RE.search(text)\n    return m.group(1).strip() if m else None\n\ndef fallback_last_int(text: str) -> Optional[int]:\n    text = strip_think(text)\n    nums = re.findall(r\"[-+]?\\d+\", text)\n    if not nums:\n        return None\n    try:\n        v = int(nums[-1])\n    except Exception:\n        return None\n    return v if 0 <= v <= 99999 else None\n\ndef mod100000(x: int) -> int:\n    return int(x) % 100000\n\ndef clamp(x: float, lo: float, hi: float) -> float:\n    return float(min(hi, max(lo, x)))\n\ndef trim_history(hist: List[Tuple[str, str]], max_items: int = 10) -> List[Tuple[str, str]]:\n    return hist if len(hist) <= max_items else hist[-max_items:]\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:23:36.613598Z","iopub.execute_input":"2025-12-31T12:23:36.613952Z","iopub.status.idle":"2025-12-31T12:23:36.619734Z","shell.execute_reply.started":"2025-12-31T12:23:36.613936Z","shell.execute_reply":"2025-12-31T12:23:36.619371Z"}},"outputs":[],"execution_count":4},{"id":"53964c8d-e6a1-4e7b-aa07-16dd557c6c3e","cell_type":"code","source":"# =========================\n# CELL 5/13 — DIFFICULTY ROUTER\n# =========================\n@dataclass(frozen=True)\nclass Plan:\n    tag: str\n    budget_weight: float\n    stage1_max_k: int\n    stage2_max_k: int\n    stage1_max_tokens: int\n    stage2_max_tokens: int\n    temp1: float\n    temp2: float\n    top_p1: float\n    top_p2: float\n\ndef route_problem(problem: str) -> Plan:\n    p = (problem or \"\").lower()\n\n    if any(k in p for k in [\"triangle\",\"circle\",\"radius\",\"angle\",\"tangent\",\"perpendicular\",\"circum\",\"inscribed\"]):\n        return Plan(\"GEO\", 1.20, 6, 14, 950, 1800, 0.55, 0.75, 0.92, 0.90)\n\n    if any(k in p for k in [\"mod\",\"congruen\",\"prime\",\"gcd\",\"lcm\",\"divis\",\"remainder\",\"coprime\",\"valuation\",\"phi(\"]):\n        return Plan(\"NT\", 1.30, 6, 16, 980, 1950, 0.55, 0.78, 0.92, 0.90)\n\n    if any(k in p for k in [\"f(\",\"functional\",\"for all real\",\"for all integers\",\"for all x\",\"for all n\",\"satisfies\"]):\n        return Plan(\"FUNC\", 1.25, 6, 16, 980, 1950, 0.55, 0.78, 0.92, 0.90)\n\n    if any(k in p for k in [\"probability\",\"expected\",\"random\",\"uniform\",\"dice\",\"coin\",\"distribution\"]):\n        return Plan(\"PROB\", 1.15, 6, 14, 950, 1800, 0.55, 0.75, 0.92, 0.90)\n\n    if any(k in p for k in [\"ways\",\"choose\",\"arrangements\",\"permutation\",\"combination\",\"graph\",\"color\",\"pigeonhole\",\"invariant\"]):\n        return Plan(\"COMB\", 1.20, 6, 16, 950, 1900, 0.55, 0.78, 0.92, 0.90)\n\n    return Plan(\"ALG\", 1.00, 6, 14, 900, 1700, 0.50, 0.72, 0.92, 0.90)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:23:36.620801Z","iopub.execute_input":"2025-12-31T12:23:36.620944Z","iopub.status.idle":"2025-12-31T12:23:36.635312Z","shell.execute_reply.started":"2025-12-31T12:23:36.620929Z","shell.execute_reply":"2025-12-31T12:23:36.634948Z"}},"outputs":[],"execution_count":5},{"id":"0ef98e5f-f935-4106-b31c-6b46e5e5048f","cell_type":"code","source":"# =========================\n# CELL 6/13 — PROMPTS\n# =========================\nSYSTEM_TIR = (\n    \"You are an olympiad math solver.\\n\"\n    \"You MUST follow this protocol:\\n\\n\"\n    \"If you need computation, output exactly:\\n\"\n    \"<tool:python>\\n\"\n    \"# python code\\n\"\n    \"</tool:python>\\n\\n\"\n    \"If you are ready to answer, output exactly ONE line:\\n\"\n    \"\\\\boxed{NONNEGATIVE_INTEGER}\\n\\n\"\n    \"Rules:\\n\"\n    \"- Output NOTHING else outside the tool block.\\n\"\n    \"- Final answer must be an integer in [0, 99999].\\n\"\n    \"- Prefer verifying with python when possible.\\n\"\n)\n\nSYSTEM_VERIFY = (\n    \"You are a strict verifier.\\n\"\n    \"Given a problem and proposed integer answer A, DISPROVE it quickly.\\n\"\n    \"Use python checks when possible:\\n\"\n    \"- parity constraints\\n\"\n    \"- modular constraints\\n\"\n    \"- substitution / brute force small cases / random tests\\n\\n\"\n    \"Protocol:\\n\"\n    \"- You may output <tool:python>...</tool:python> blocks.\\n\"\n    \"- Then output EXACTLY one final line: PASS or FAIL or UNKNOWN\\n\"\n    \"- No extra text.\\n\"\n)\n\nSYSTEM_SELECT = (\n    \"You are a selector.\\n\"\n    \"Pick the most reliable candidate answer based on evidence.\\n\"\n    \"Output EXACTLY one line: \\\\boxed{NONNEGATIVE_INTEGER}\\n\"\n    \"No extra text.\\n\"\n)\n\nHINTS = [\n    \"Tool-first: explore small cases in python, infer pattern, verify, then output boxed.\",\n    \"Proof-first: derive symbolic structure, then minimal python verification, output boxed.\",\n    \"Number-theory: use modular constraints/parity/gcd; python to test; output boxed.\",\n    \"Comb/Prob: use invariants or counting; python to validate small n; output boxed.\",\n]\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:23:36.635801Z","iopub.execute_input":"2025-12-31T12:23:36.635920Z","iopub.status.idle":"2025-12-31T12:23:36.648941Z","shell.execute_reply.started":"2025-12-31T12:23:36.635907Z","shell.execute_reply":"2025-12-31T12:23:36.648565Z"}},"outputs":[],"execution_count":6},{"id":"23994364-1012-4980-9464-6b68fa647214","cell_type":"code","source":"# =========================\n# CELL 7/13 — PROMPT BUILDER (chat template)\n# =========================\nclass PromptBuilder:\n    def __init__(self, tok):\n        self.tok = tok\n\n    def render(self, system: str, user: str, history: List[Tuple[str, str]]) -> str:\n        msgs = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n        for r, c in history:\n            msgs.append({\"role\": r, \"content\": c})\n        try:\n            return self.tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n        except Exception:\n            out = [f\"[SYSTEM]\\n{system}\\n\", f\"[USER]\\n{user}\\n\"]\n            for r, c in history:\n                out.append(f\"[{r.upper()}]\\n{c}\\n\")\n            out.append(\"[ASSISTANT]\\n\")\n            return \"\\n\".join(out)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:23:36.649451Z","iopub.execute_input":"2025-12-31T12:23:36.649583Z","iopub.status.idle":"2025-12-31T12:23:36.663633Z","shell.execute_reply.started":"2025-12-31T12:23:36.649569Z","shell.execute_reply":"2025-12-31T12:23:36.663285Z"}},"outputs":[],"execution_count":7},{"id":"c58e0df1-542e-4dd5-b8b3-504c9fe7e0e0","cell_type":"code","source":"# =========================\n# CELL 8/13 — PYTHON TOOL POOL (NO KernelManager, Kaggle-stable)\n# =========================\nimport multiprocessing as mp\nimport traceback, io, contextlib, threading\n\ndef _tool_worker_loop(conn):\n    # lazy imports inside process\n    import math, itertools, random\n    try:\n        import sympy as sp\n    except Exception:\n        sp = None\n\n    g = {\n        \"__builtins__\": __builtins__,\n        \"math\": math,\n        \"itertools\": itertools,\n        \"random\": random,\n        \"sp\": sp,\n    }\n    l = {}\n\n    while True:\n        msg = conn.recv()\n        cmd = msg.get(\"cmd\")\n        if cmd == \"close\":\n            break\n        if cmd == \"reset\":\n            l = {}\n            conn.send({\"ok\": True, \"out\": \"[RESET]\"})\n            continue\n        if cmd != \"exec\":\n            conn.send({\"ok\": False, \"out\": \"[UNKNOWN_CMD]\"})\n            continue\n\n        code = msg.get(\"code\", \"\")\n        buf = io.StringIO()\n        ok = True\n        try:\n            with contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):\n                exec(code, g, l)\n        except Exception:\n            ok = False\n            buf.write(traceback.format_exc(limit=3))\n\n        out = buf.getvalue().strip()\n        if not out and ok:\n            out = \"[WARN] No output. Use print().\"\n        conn.send({\"ok\": ok, \"out\": out})\n\nclass ToolWorker:\n    def __init__(self, ctx, wid: int):\n        self.ctx = ctx\n        self.wid = wid\n        self.lock = threading.Lock()\n        self._start()\n\n    def _start(self):\n        self.parent, child = self.ctx.Pipe()\n        self.proc = self.ctx.Process(target=_tool_worker_loop, args=(child,), daemon=True)\n        self.proc.start()\n\n    def restart(self):\n        try:\n            if self.proc.is_alive():\n                self.proc.terminate()\n                self.proc.join(timeout=1)\n        except Exception:\n            pass\n        self._start()\n\n    def exec(self, code: str, timeout_s: float) -> Tuple[str, bool]:\n        with self.lock:\n            try:\n                self.parent.send({\"cmd\": \"exec\", \"code\": code})\n                if self.parent.poll(timeout_s):\n                    resp = self.parent.recv()\n                    return resp.get(\"out\", \"\"), bool(resp.get(\"ok\", False))\n                else:\n                    self.restart()\n                    return \"[PYTHON_TIMEOUT]\", False\n            except Exception:\n                self.restart()\n                return \"[PYTHON_CRASH_RESTARTED]\", False\n\nclass ToolPool:\n    def __init__(self, size: int):\n        self.size = max(1, int(size))\n        self.ctx = mp.get_context(\"spawn\")  # avoid fork-after-cuda weirdness\n        self.workers = [ToolWorker(self.ctx, i) for i in range(self.size)]\n\n    def run(self, wid: int, code: str, timeout_s: float) -> Tuple[str, bool]:\n        wid = int(wid) % self.size\n        return self.workers[wid].exec(code, timeout_s)\n\n    def close(self):\n        for w in self.workers:\n            try:\n                w.parent.send({\"cmd\": \"close\"})\n            except Exception:\n                pass\n            try:\n                if w.proc.is_alive():\n                    w.proc.terminate()\n            except Exception:\n                pass\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:23:36.664155Z","iopub.execute_input":"2025-12-31T12:23:36.664299Z","iopub.status.idle":"2025-12-31T12:23:36.678758Z","shell.execute_reply.started":"2025-12-31T12:23:36.664286Z","shell.execute_reply":"2025-12-31T12:23:36.678394Z"}},"outputs":[],"execution_count":8},{"id":"8b30f7be-c787-44b1-987e-f0b058a79e53","cell_type":"code","source":"# =========================\n# CELL 8/13 — PYTHON TOOL (SubprocessToolPool) [KAGGLE-PROOF]\n# - No jupyter child kernels (avoid colab_kernel_launcher crash)\n# - No multiprocessing spawn pickling (avoid __main__ AttributeError)\n# - Stateful python workers via subprocess stdin/stdout (JSONL)\n# - Timeout => kill & restart worker\n# =========================\n\nimport os, sys, json, time, queue, uuid, subprocess, threading\nfrom contextlib import contextmanager\nfrom typing import List, Dict, Optional\n\n_WORKER_SRC = r\"\"\"\nimport sys, json, traceback, io, contextlib, math, random, itertools\ntry:\n    import sympy as sp\nexcept Exception:\n    sp = None\n\nG = {\"math\": math, \"random\": random, \"itertools\": itertools, \"sp\": sp}\n\ndef handle(req):\n    code = req.get(\"code\", \"\")\n    out_io = io.StringIO()\n    err_io = io.StringIO()\n    ok = True\n\n    with contextlib.redirect_stdout(out_io), contextlib.redirect_stderr(err_io):\n        try:\n            exec(compile(code, \"<tool>\", \"exec\"), G, G)\n        except Exception:\n            ok = False\n            traceback.print_exc()\n\n    txt = (out_io.getvalue() + err_io.getvalue()).strip()\n\n    # If no stdout/stderr, try common result variables\n    if not txt:\n        for k in (\"__result__\", \"result\", \"ans\", \"_\"):\n            if k in G:\n                try:\n                    txt = str(G[k])\n                    break\n                except Exception:\n                    pass\n\n    if not txt:\n        txt = \"[WARN] No output. Use print().\"\n\n    if len(txt) > 2000:\n        txt = txt[:2000] + \"\\n[...TRUNCATED...]\"\n\n    return {\"id\": req.get(\"id\"), \"ok\": ok, \"out\": txt}\n\nfor line in sys.stdin:\n    line = line.strip()\n    if not line:\n        continue\n    try:\n        req = json.loads(line)\n    except Exception:\n        continue\n    resp = handle(req)\n    sys.stdout.write(json.dumps(resp, ensure_ascii=False) + \"\\n\")\n    sys.stdout.flush()\n\"\"\"\n\nclass SubprocessToolWorker:\n    def __init__(self):\n        self.proc: Optional[subprocess.Popen] = None\n        self._pending: Dict[str, \"queue.Queue[dict]\"] = {}\n        self._lock = threading.Lock()\n        self._reader_thread: Optional[threading.Thread] = None\n        self.start()\n\n    def start(self):\n        self.stop()\n        env = dict(os.environ)\n        env[\"PYTHONUNBUFFERED\"] = \"1\"\n\n        self.proc = subprocess.Popen(\n            [sys.executable, \"-u\", \"-c\", _WORKER_SRC],\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            bufsize=1,\n            env=env,\n        )\n\n        def _reader():\n            assert self.proc is not None and self.proc.stdout is not None\n            for line in self.proc.stdout:\n                line = line.strip()\n                if not line:\n                    continue\n                try:\n                    msg = json.loads(line)\n                except Exception:\n                    continue\n                rid = msg.get(\"id\")\n                if not rid:\n                    continue\n                q = self._pending.pop(rid, None)\n                if q is not None:\n                    q.put(msg)\n\n        self._reader_thread = threading.Thread(target=_reader, daemon=True)\n        self._reader_thread.start()\n\n    def is_alive(self) -> bool:\n        return self.proc is not None and (self.proc.poll() is None)\n\n    def stop(self):\n        if self.proc is not None:\n            try:\n                self.proc.kill()\n            except Exception:\n                pass\n            try:\n                self.proc.wait(timeout=1)\n            except Exception:\n                pass\n        self.proc = None\n        self._pending.clear()\n\n    def execute(self, code: str, timeout_s: float) -> str:\n        if not self.is_alive():\n            self.start()\n\n        assert self.proc is not None and self.proc.stdin is not None\n        rid = uuid.uuid4().hex\n        q: \"queue.Queue[dict]\" = queue.Queue(maxsize=1)\n        self._pending[rid] = q\n\n        payload = {\"id\": rid, \"code\": code}\n        with self._lock:\n            try:\n                self.proc.stdin.write(json.dumps(payload, ensure_ascii=False) + \"\\n\")\n                self.proc.stdin.flush()\n            except Exception:\n                # restart and signal timeout-like failure\n                self.start()\n                return \"[PYTHON_ERROR] worker write failed\"\n\n        try:\n            msg = q.get(timeout=timeout_s)\n        except queue.Empty:\n            # timeout => restart worker\n            self.start()\n            return \"[PYTHON_TIMEOUT]\"\n\n        return msg.get(\"out\", \"\").strip() or \"[WARN] No output. Use print().\"\n\nclass ToolPool:\n    def __init__(self, size: int):\n        self.size = max(1, int(size))\n        self.q: \"queue.Queue[SubprocessToolWorker]\" = queue.Queue()\n        self.all: List[SubprocessToolWorker] = []\n        for _ in range(self.size):\n            w = SubprocessToolWorker()\n            self.q.put(w)\n            self.all.append(w)\n\n    @contextmanager\n    def acquire(self) -> SubprocessToolWorker:\n        w = self.q.get()\n        try:\n            if not w.is_alive():\n                w.start()\n            yield w\n        finally:\n            self.q.put(w)\n\n    def close(self):\n        while not self.q.empty():\n            try:\n                self.q.get_nowait()\n            except Exception:\n                break\n        for w in self.all:\n            w.stop()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:23:36.679330Z","iopub.execute_input":"2025-12-31T12:23:36.679470Z","iopub.status.idle":"2025-12-31T12:23:36.698523Z","shell.execute_reply.started":"2025-12-31T12:23:36.679456Z","shell.execute_reply":"2025-12-31T12:23:36.698144Z"}},"outputs":[],"execution_count":9},{"id":"65188a57-916f-4318-a3e5-34a4d2dff9d2","cell_type":"code","source":"# =========================\n# CELL 10/13 — TIME MANAGER\n# =========================\nclass TimeManager:\n    def __init__(self, hard_wall_s: int, total_questions: int):\n        self.start = time.time()\n        self.deadline = self.start + int(hard_wall_s)\n        self.total = max(1, int(total_questions))\n        self.done = 0\n\n    def remaining(self) -> float:\n        return max(0.0, self.deadline - time.time())\n\n    def budget(self, weight: float) -> float:\n        rem = self.remaining()\n        left = max(1, self.total - self.done)\n        base = rem / left\n        return clamp(base * float(weight), MIN_BUDGET_S, MAX_BUDGET_S)\n\n    def mark_done(self) -> None:\n        self.done += 1\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:23:36.699030Z","iopub.execute_input":"2025-12-31T12:23:36.699164Z","iopub.status.idle":"2025-12-31T12:23:36.710723Z","shell.execute_reply.started":"2025-12-31T12:23:36.699151Z","shell.execute_reply":"2025-12-31T12:23:36.710383Z"}},"outputs":[],"execution_count":10},{"id":"54906722-c28f-4dea-9006-7de074a5144d","cell_type":"code","source":"# =========================\n# CELL 11/13 — CANDIDATES + WEIGHTED VOTE\n# =========================\n@dataclass\nclass Candidate:\n    answer: Optional[int]\n    raw: str\n    tool_calls: int\n    tool_errors: int\n    elapsed: float\n    stage: int\n    verified: Optional[bool] = None  # PASS->True, FAIL->False, UNKNOWN->None\n\ndef cand_weight(c: Candidate) -> float:\n    if c.answer is None:\n        return 0.0\n    w = 1.0\n    if c.tool_calls > 0 and c.tool_errors == 0:\n        w += 0.9\n    w -= 0.75 * c.tool_errors\n    w += max(0.0, 0.35 - 0.015 * c.elapsed)\n    if c.verified is True:\n        w += 2.25\n    if c.verified is False:\n        w -= 2.25\n    return max(0.0, w)\n\ndef weighted_vote(cands: List[Candidate]) -> Tuple[Optional[int], float, Dict[int, float]]:\n    scores: Dict[int, float] = {}\n    total = 0.0\n    for c in cands:\n        if c.answer is None:\n            continue\n        w = cand_weight(c)\n        total += w\n        scores[c.answer] = scores.get(c.answer, 0.0) + w\n    if not scores or total <= 1e-9:\n        return None, 0.0, {}\n    best = max(scores.items(), key=lambda kv: kv[1])[0]\n    ratio = scores[best] / total\n    return best, ratio, scores\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:23:36.711216Z","iopub.execute_input":"2025-12-31T12:23:36.711337Z","iopub.status.idle":"2025-12-31T12:23:36.722171Z","shell.execute_reply.started":"2025-12-31T12:23:36.711324Z","shell.execute_reply":"2025-12-31T12:23:36.721799Z"}},"outputs":[],"execution_count":11},{"id":"3e829212-f307-4871-b0d5-2f61b434f195","cell_type":"code","source":"# =========================\n# CELL 12/13 — TIR ENGINE + VERIFIER + SELECTOR\n# =========================\n@dataclass\nclass TIRState:\n    history: List[Tuple[str, str]]\n    hint: str\n    worker_id: int\n    done: bool = False\n    answer: Optional[int] = None\n    tool_calls: int = 0\n    tool_errors: int = 0\n    raw_last: str = \"\"\n\nclass TIRBatchEngine:\n    def __init__(self, backend: BackendBase, pb: PromptBuilder, tools: ToolPool):\n        self.backend = backend\n        self.pb = pb\n        self.tools = tools\n\n    def _turn(self, problem: str, states: List[TIRState], *, temperature: float, top_p: float, max_tokens: int, time_left_s: float) -> None:\n        active = [i for i, s in enumerate(states) if not s.done]\n        if not active:\n            return\n\n        prompts: List[str] = []\n        for i in active:\n            s = states[i]\n            user = f\"{problem}\\n\\nHint: {s.hint}\"\n            prompts.append(self.pb.render(SYSTEM_TIR, user, s.history))\n\n        outs = self.backend.generate(prompts, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n\n        tool_jobs: List[Tuple[int, int, str]] = []  # (state_idx, worker_id, code)\n        for idx, out in zip(active, outs):\n            out = strip_think(out)\n            st = states[idx]\n            st.raw_last = out\n\n            ans = parse_boxed_int(out)\n            if ans is not None:\n                st.done = True\n                st.answer = ans\n                continue\n\n            code = parse_tool_code(out)\n            if code:\n                st.tool_calls += 1\n                tool_jobs.append((idx, st.worker_id, code))\n                continue\n\n            st.history.append((\"assistant\", out[:800]))\n            st.history.append((\"user\", \"Output ONLY either a <tool:python> block OR one line \\\\boxed{integer}.\"))\n            st.history = trim_history(st.history)\n\n        if not tool_jobs:\n            return\n\n        def run_one(job: Tuple[int, int, str]) -> Tuple[int, str, str, bool]:\n            i, wid, code = job\n            py_out, ok = self.tools.run(wid, code, timeout_s=min(TOOL_TIMEOUT_S, max(1.0, time_left_s)))\n            return i, code, py_out, ok\n\n        with ThreadPoolExecutor(max_workers=min(len(tool_jobs), TOOL_POOL_SIZE)) as ex:\n            futs = [ex.submit(run_one, j) for j in tool_jobs]\n            for fut in as_completed(futs):\n                i, code, py_out, ok = fut.result()\n                st = states[i]\n                if (not ok) or (\"PYTHON_TIMEOUT\" in py_out) or (\"Traceback\" in py_out):\n                    st.tool_errors += 1\n                st.history.append((\"assistant\", f\"<tool:python>\\n{code}\\n</tool:python>\"))\n                st.history.append((\"user\", f\"Python output:\\n{py_out}\"))\n                st.history = trim_history(st.history)\n\n    def run_progressive(\n        self,\n        problem: str,\n        *,\n        max_k: int,\n        batch_k: int,\n        budget_s: float,\n        stage: int,\n        max_tokens: int,\n        temperature: float,\n        top_p: float,\n        early_stop_ratio: float,\n    ) -> List[Candidate]:\n        t0 = time.time()\n        cands: List[Candidate] = []\n        created = 0\n\n        while created < max_k and (time.time() - t0) < budget_s:\n            add = min(batch_k, max_k - created)\n            states = [\n                TIRState(history=[], hint=HINTS[(created + i) % len(HINTS)], worker_id=(created + i) % TOOL_POOL_SIZE)\n                for i in range(add)\n            ]\n            created += add\n\n            for _ in range(MAX_TURNS):\n                if time.time() - t0 >= budget_s:\n                    break\n                self._turn(\n                    problem, states,\n                    temperature=temperature, top_p=top_p,\n                    max_tokens=max_tokens,\n                    time_left_s=budget_s - (time.time() - t0),\n                )\n                if all(s.done for s in states):\n                    break\n\n            elapsed = time.time() - t0\n            for s in states:\n                ans = s.answer\n                if ans is None:\n                    ans = parse_boxed_int(s.raw_last) or fallback_last_int(s.raw_last)\n                cands.append(Candidate(\n                    answer=ans,\n                    raw=s.raw_last,\n                    tool_calls=s.tool_calls,\n                    tool_errors=s.tool_errors,\n                    elapsed=elapsed,\n                    stage=stage,\n                ))\n\n            best, ratio, _ = weighted_vote(cands)\n            if best is not None and ratio >= early_stop_ratio:\n                break\n\n        return cands\n\nclass Verifier:\n    def __init__(self, backend: BackendBase, pb: PromptBuilder, tools: ToolPool):\n        self.backend = backend\n        self.pb = pb\n        self.tools = tools\n\n    def verify(self, problem: str, answer: int, budget_s: float) -> Optional[bool]:\n        t0 = time.time()\n        history: List[Tuple[str, str]] = []\n        user = f\"Problem:\\n{problem}\\n\\nProposed answer A = {answer}\\n\"\n\n        for _ in range(6):\n            if time.time() - t0 >= budget_s:\n                return None\n\n            prompt = self.pb.render(SYSTEM_VERIFY, user, history)\n            out = strip_think(self.backend.generate([prompt], temperature=0.0, top_p=1.0, max_tokens=650)[0])\n\n            code = parse_tool_code(out)\n            if code:\n                py_out, ok = self.tools.run(0, code, timeout_s=min(TOOL_TIMEOUT_S, max(1.0, budget_s - (time.time() - t0))))\n                history.append((\"assistant\", f\"<tool:python>\\n{code}\\n</tool:python>\"))\n                history.append((\"user\", f\"Python output:\\n{py_out}\"))\n                history = trim_history(history, 10)\n                continue\n\n            last = out.strip().splitlines()[-1].strip().upper() if out.strip() else \"\"\n            if last == \"PASS\":\n                return True\n            if last == \"FAIL\":\n                return False\n            if last == \"UNKNOWN\":\n                return None\n\n            history.append((\"assistant\", out[:800]))\n            history.append((\"user\", \"Return ONLY one final line: PASS or FAIL or UNKNOWN.\"))\n            history = trim_history(history, 10)\n\n        return None\n\nclass Selector:\n    def __init__(self, backend: BackendBase, pb: PromptBuilder):\n        self.backend = backend\n        self.pb = pb\n\n    def select(self, problem: str, scores: Dict[int, float]) -> Optional[int]:\n        if not scores:\n            return None\n        items = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[:8]\n        evidence = \"\\n\".join([f\"- {a}: score={s:.3f}\" for a, s in items])\n        prompt = self.pb.render(SYSTEM_SELECT, f\"Problem:\\n{problem}\\n\\nCandidate scores:\\n{evidence}\\n\", [])\n        out = strip_think(self.backend.generate([prompt], temperature=0.0, top_p=1.0, max_tokens=220)[0])\n        return parse_boxed_int(out) or fallback_last_int(out)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:23:36.722722Z","iopub.execute_input":"2025-12-31T12:23:36.722854Z","iopub.status.idle":"2025-12-31T12:23:36.751916Z","shell.execute_reply.started":"2025-12-31T12:23:36.722840Z","shell.execute_reply":"2025-12-31T12:23:36.751538Z"}},"outputs":[],"execution_count":12},{"id":"4eab14fd-1a35-442e-a826-aba4637f2b75","cell_type":"code","source":"# =========================\n# CELL 13/13 — SOLVER + DEV TEST + KAGGLE HOOK\n# =========================\nclass AIMO3Solver:\n    def __init__(self):\n        self.pb = PromptBuilder(tokenizer)\n        self.tools = ToolPool(TOOL_POOL_SIZE)\n        self.backend = _backend\n        self.tm = TimeManager(HARD_WALL_SECONDS, TOTAL_QUESTIONS)\n        self.engine = TIRBatchEngine(self.backend, self.pb, self.tools)\n        self.verifier = Verifier(self.backend, self.pb, self.tools)\n        self.selector = Selector(self.backend, self.pb)\n\n        if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n            print(f\"[solver] backend = {_backend_name}\")\n    \n    def close(self):\n        self.pool.close()\n\n\n    def solve_problem(self, problem: str) -> int:\n        plan = route_problem(problem)\n        rem = self.tm.remaining()\n        if rem < 5.0:\n            return 0\n\n        budget = min(self.tm.budget(plan.budget_weight), rem)\n\n        # Stage 1\n        c1 = self.engine.run_progressive(\n            problem,\n            max_k=plan.stage1_max_k,\n            batch_k=STAGE1_BATCH,\n            budget_s=0.38 * budget,\n            stage=1,\n            max_tokens=plan.stage1_max_tokens,\n            temperature=plan.temp1,\n            top_p=plan.top_p1,\n            early_stop_ratio=CONFIDENT_RATIO,\n        )\n        best, ratio, scores = weighted_vote(c1)\n        if best is not None and ratio >= CONFIDENT_RATIO:\n            self.tm.mark_done()\n            return mod100000(best)\n\n        # Stage 2\n        c2 = self.engine.run_progressive(\n            problem,\n            max_k=plan.stage2_max_k,\n            batch_k=STAGE2_BATCH,\n            budget_s=0.50 * budget,\n            stage=2,\n            max_tokens=plan.stage2_max_tokens,\n            temperature=plan.temp2,\n            top_p=plan.top_p2,\n            early_stop_ratio=CONFIDENT_RATIO,\n        )\n\n        all_c = c1 + c2\n        best, ratio, scores = weighted_vote(all_c)\n\n        if best is None:\n            self.tm.mark_done()\n            return 0\n\n        # Verifier-on-uncertainty\n        if ratio < VERIFY_RATIO and budget >= 25.0:\n            top = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[:VERIFY_TOP_N]\n            per_verify_budget = 0.10 * budget / max(1, len(top))\n\n            for ans, _ in top:\n                verdict = self.verifier.verify(problem, ans, budget_s=per_verify_budget)\n                for c in all_c:\n                    if c.answer == ans:\n                        c.verified = verdict\n\n            best, ratio, scores = weighted_vote(all_c)\n\n        final = best\n        if ratio < 0.80 and (0.07 * budget) >= 3.0:\n            sel = self.selector.select(problem, scores)\n            if sel is not None:\n                final = sel\n\n        self.tm.mark_done()\n        return mod100000(final)\n\nsolver = AIMO3Solver()\n\ndef predict(id_: \"pl.Series\", problem: \"pl.Series\"):\n    if pl is not None and isinstance(id_, pl.Series):\n        pid = id_.item(0)\n        prob = problem.item(0)\n        ans = solver.solve_problem(prob)\n        return pl.DataFrame({\"id\": [pid], \"answer\": [ans]})\n    else:\n        pid = id_[0] if hasattr(id_, \"__len__\") else id_\n        prob = problem[0] if hasattr(problem, \"__len__\") else problem\n        ans = solver.solve_problem(prob)\n        return pd.DataFrame({\"id\": [pid], \"answer\": [ans]})\n\n# ---- DEV EVAL (this is what you were missing) ----\ndef _find_comp_file(fname: str) -> Optional[str]:\n    # competition bundle usually: /kaggle/input/<comp-slug>/fname\n    hits = glob.glob(f\"/kaggle/input/*/{fname}\")\n    return hits[0] if hits else None\n\ndef dev_eval(n: int = 30):\n    ref_path = _find_comp_file(\"reference.csv\")\n    if not ref_path:\n        print(\"[dev_eval] reference.csv not found in /kaggle/input/*/\")\n        return\n\n    df = pd.read_csv(ref_path)\n    if \"problem\" not in df.columns:\n        print(\"[dev_eval] reference.csv missing 'problem' column\")\n        return\n\n    has_gt = \"answer\" in df.columns\n    if has_gt:\n        gt = df.set_index(\"id\")[\"answer\"].to_dict()\n\n    n = min(n, len(df))\n    sub = df.iloc[:n].copy()\n\n    t0 = time.time()\n    correct = 0\n    for i, row in sub.iterrows():\n        pid = row[\"id\"]\n        prob = row[\"problem\"]\n        ans = solver.solve_problem(prob)\n        if has_gt:\n            if int(ans) == int(gt[pid]):\n                correct += 1\n        if (i - sub.index[0] + 1) % 5 == 0:\n            elapsed = time.time() - t0\n            done = i - sub.index[0] + 1\n            acc = (correct / done) * 100 if has_gt else None\n            print(f\"[dev_eval] {done}/{n}  elapsed={elapsed:.1f}s\" + (f\"  acc={acc:.1f}%\" if acc is not None else \"\"))\n\n    elapsed = time.time() - t0\n    if has_gt:\n        print(f\"[dev_eval] FINAL: {correct}/{n} = {100*correct/n:.1f}%  | time={elapsed:.1f}s\")\n    else:\n        print(f\"[dev_eval] FINAL: done {n} problems | time={elapsed:.1f}s | (no ground truth in reference.csv)\")\n\n# ---- Kaggle Inference Server ----\nimport kaggle_evaluation.aimo_3_inference_server as aimo3\ninference_server = aimo3.AIMO3InferenceServer(predict)\n\nif os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    inference_server.serve()\nelse:\n    print(\"[dev] solver loaded. Running dev_eval() on reference.csv ...\")\n    dev_eval(n=int(os.getenv(\"DEV_N\", \"30\")))\n\n    # Optional: simulate gateway on a small csv (closer to “Kaggle chấm”)\n    if os.getenv(\"DEV_RUN_GATEWAY\", \"0\") == \"1\":\n        ref_path = _find_comp_file(\"reference.csv\")\n        df = pd.read_csv(ref_path)\n        tmp = df[[\"id\",\"problem\"]].head(int(os.getenv(\"DEV_GATEWAY_N\",\"10\")))\n        tmp_path = \"ref_input_head.csv\"\n        tmp.to_csv(tmp_path, index=False)\n        print(f\"[dev] run_local_gateway on {tmp_path}\")\n        inference_server.run_local_gateway((tmp_path,))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:23:36.752461Z","iopub.execute_input":"2025-12-31T12:23:36.752585Z","iopub.status.idle":"2025-12-31T12:23:36.770013Z","shell.execute_reply.started":"2025-12-31T12:23:36.752572Z","shell.execute_reply":"2025-12-31T12:23:36.769576Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_485/347432387.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmod100000\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAIMO3Solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Series\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Series\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_485/347432387.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAIMO3Solver\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mToolPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTOOL_POOL_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"],"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error"}],"execution_count":13}]}