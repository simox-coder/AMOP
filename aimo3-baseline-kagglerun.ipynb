{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":363148,"sourceType":"modelInstanceVersion","modelInstanceId":301526,"modelId":322000},{"sourceId":499291,"sourceType":"modelInstanceVersion","modelInstanceId":396608,"modelId":322000},{"sourceId":499313,"sourceType":"modelInstanceVersion","modelInstanceId":396626,"modelId":322000},{"sourceId":510391,"sourceType":"modelInstanceVersion","modelInstanceId":404485,"modelId":422384}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d31b2c1d-dff8-48a4-96e9-b04057bf99c8","cell_type":"code","source":"# ============================================================\n# AIMO3 TOP-ORIENTED SUBMISSION — Qwen-3 30B-A3B Thinking (H100)\n# FIXES:\n# - proper left-padding setup downstream\n# - separate TOOL_POOL_SIZE (python workers) vs TOOL_THREAD_WORKERS (threads)\n# - add GEN_BATCH_SIZE for GPU utilization\n# ============================================================\n\nfrom __future__ import annotations\n\nimport os, re, time, math, json, random, glob\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nMODEL_PATH = \"/kaggle/input/qwen-3/transformers/30b-a3b-thinking-2507-fp8/1\"\nSEED = int(os.getenv(\"SEED\", \"42\"))\n\n# Warm/cache\nWARMUP_USR_LIB = os.getenv(\"WARMUP_USR_LIB\", \"0\") == \"1\"\nCACHE_MODEL_FILES = os.getenv(\"CACHE_MODEL_FILES\", \"1\") == \"1\"\nCACHE_CHUNK_MB = int(os.getenv(\"CACHE_CHUNK_MB\", \"512\"))\nCACHE_WORKERS = int(os.getenv(\"CACHE_WORKERS\", \"8\"))\nCACHE_EXTS = (\".safetensors\", \".bin\", \".pt\")\n\n# Time (mặc định 4h55m giống style V22 notebook, tránh chết sát giờ)\nHARD_WALL_SECONDS = int(os.getenv(\"HARD_WALL_SECONDS\", str((4 * 60 + 55) * 60)))\nTOTAL_QUESTIONS = int(os.getenv(\"TOTAL_QUESTIONS\", \"110\"))\nMIN_BUDGET_S = float(os.getenv(\"MIN_BUDGET_S\", \"10\"))\nMAX_BUDGET_S = float(os.getenv(\"MAX_BUDGET_S\", \"420\"))\n\n# Tool loop (CPU)\nTOOL_POOL_SIZE = int(os.getenv(\"TOOL_POOL_SIZE\", \"12\"))  # số python subprocess workers\nTOOL_THREAD_WORKERS = int(os.getenv(\"TOOL_THREAD_WORKERS\", str(min(12, TOOL_POOL_SIZE))))\nTOOL_TIMEOUT_S = float(os.getenv(\"TOOL_TIMEOUT_S\", \"6.0\"))\nMAX_TURNS = int(os.getenv(\"MAX_TURNS\", \"10\"))\n\n# Sampling (accuracy vs time)\nSTAGE1_BATCH = int(os.getenv(\"STAGE1_BATCH\", \"2\"))\nSTAGE2_BATCH = int(os.getenv(\"STAGE2_BATCH\", \"3\"))\nCONFIDENT_RATIO = float(os.getenv(\"CONFIDENT_RATIO\", \"0.78\"))\nVERIFY_RATIO = float(os.getenv(\"VERIFY_RATIO\", \"0.66\"))\nVERIFY_TOP_N = int(os.getenv(\"VERIFY_TOP_N\", \"3\"))\n\n# Generation (GPU)\nMAX_MODEL_LEN = int(os.getenv(\"MAX_MODEL_LEN\", \"12288\"))   # 16k ok, nhưng 12k thường nhanh hơn\nDTYPE = os.getenv(\"DTYPE\", \"bfloat16\")\nGPU_MEM_UTIL = float(os.getenv(\"GPU_MEM_UTIL\", \"0.92\"))\n\n# Batch size cho HF backend (đẩy GPU util lên)\nGEN_BATCH_SIZE = int(os.getenv(\"GEN_BATCH_SIZE\", \"8\"))  # H100 + 30B fp8 thường chịu 6-10\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T13:51:27.633101Z","iopub.execute_input":"2025-12-31T13:51:27.633327Z","iopub.status.idle":"2025-12-31T13:51:27.641852Z","shell.execute_reply.started":"2025-12-31T13:51:27.633309Z","shell.execute_reply":"2025-12-31T13:51:27.641413Z"}},"outputs":[],"execution_count":1},{"id":"2b7621d8-2cb5-4d6d-b4d5-8a7879598339","cell_type":"code","source":"# =========================\n# CELL 2/13 — WARMUP HELPERS\n# =========================\ndef warmup_usr_lib() -> None:\n    import subprocess\n    cmd = \"find /kaggle/usr/lib -type f -print0 | xargs -0 -P 32 -n 500 cat > /dev/null\"\n    subprocess.run(cmd, shell=True, check=False)\n\ndef cache_model(path: str, exts=CACHE_EXTS, num_workers: int = 8, chunk_mb: int = 256) -> None:\n    import multiprocessing\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n\n    def warmup_file(fpath: str) -> int:\n        chunk = chunk_mb * 1024 * 1024\n        total = 0\n        with open(fpath, \"rb\") as f:\n            while True:\n                b = f.read(chunk)\n                if not b:\n                    break\n                total += len(b)\n        return total\n\n    if not os.path.isdir(path):\n        return\n\n    files = [\n        os.path.join(root, name)\n        for root, _, names in os.walk(path)\n        for name in names\n        if name.endswith(exts)\n    ]\n    if not files:\n        return\n\n    try:\n        cpu = multiprocessing.cpu_count()\n    except Exception:\n        cpu = 4\n    num_workers = max(1, min(num_workers, cpu, 16))\n    files.sort(key=lambda f: os.path.getsize(f), reverse=True)\n\n    t0 = time.time()\n    total = 0\n    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n        futs = [ex.submit(warmup_file, f) for f in files]\n        for fut in as_completed(futs):\n            total += fut.result()\n\n    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n        gb = total / 1024**3\n        print(f\"[cache_model] warmed ~{gb:.2f} GB in {time.time()-t0:.1f}s\")\n\nif WARMUP_USR_LIB:\n    warmup_usr_lib()\nif CACHE_MODEL_FILES:\n    cache_model(MODEL_PATH, num_workers=CACHE_WORKERS, chunk_mb=CACHE_CHUNK_MB)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T13:51:27.644326Z","iopub.execute_input":"2025-12-31T13:51:27.644510Z","iopub.status.idle":"2025-12-31T13:52:45.320503Z","shell.execute_reply.started":"2025-12-31T13:51:27.644494Z","shell.execute_reply":"2025-12-31T13:52:45.320026Z"}},"outputs":[{"name":"stdout","text":"[cache_model] warmed ~29.03 GB in 77.7s\n","output_type":"stream"}],"execution_count":2},{"id":"05b3f8e3-4cee-4285-818f-e076e9fc5420","cell_type":"code","source":"# =========================\n# CELL 3/13 — IMPORTS\n# =========================\nimport numpy as np\nimport pandas as pd\n\ntry:\n    import polars as pl\nexcept Exception:\n    pl = None\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\nset_seed(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T13:52:45.321368Z","iopub.execute_input":"2025-12-31T13:52:45.321532Z","iopub.status.idle":"2025-12-31T13:53:02.746108Z","shell.execute_reply.started":"2025-12-31T13:52:45.321519Z","shell.execute_reply":"2025-12-31T13:53:02.745683Z"}},"outputs":[{"name":"stderr","text":"2025-12-31 13:52:54.467482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767189174.585913     105 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767189174.619138     105 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767189174.913344     105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767189174.913368     105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767189174.913371     105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767189174.913372     105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":3},{"id":"65a2f017-fea9-4dc8-a094-7b241755d531","cell_type":"code","source":"# =========================\n# CELL 4/13 — PARSERS + UTILS\n# =========================\n_THINK_RE = re.compile(r\"<think>.*?</think>\", re.DOTALL)\n_BOXED_RE = re.compile(r\"\\\\boxed\\{([^}]*)\\}\")\n_TOOL_RE = re.compile(r\"<tool:python>\\s*(.*?)\\s*</tool:python>\", re.DOTALL)\n\ndef strip_think(text: str) -> str:\n    text = _THINK_RE.sub(\"\", text)\n    if \"</think>\" in text:\n        text = text.split(\"</think>\", 1)[-1]\n    return text.strip()\n\ndef parse_boxed_int(text: str) -> Optional[int]:\n    text = strip_think(text)\n    m = _BOXED_RE.search(text)\n    if not m:\n        return None\n    raw = m.group(1).strip()\n    if not re.fullmatch(r\"[+-]?\\d+\", raw):\n        return None\n    v = int(raw)\n    return v if 0 <= v <= 99999 else None\n\ndef parse_tool_code(text: str) -> Optional[str]:\n    text = strip_think(text)\n    m = _TOOL_RE.search(text)\n    return m.group(1).strip() if m else None\n\ndef fallback_last_int(text: str) -> Optional[int]:\n    text = strip_think(text)\n    nums = re.findall(r\"[-+]?\\d+\", text)\n    if not nums:\n        return None\n    try:\n        v = int(nums[-1])\n    except Exception:\n        return None\n    return v if 0 <= v <= 99999 else None\n\ndef mod100000(x: int) -> int:\n    return int(x) % 100000\n\ndef clamp(x: float, lo: float, hi: float) -> float:\n    return float(min(hi, max(lo, x)))\n\ndef trim_history(hist: List[Tuple[str, str]], max_items: int = 10) -> List[Tuple[str, str]]:\n    return hist if len(hist) <= max_items else hist[-max_items:]\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T13:53:02.746738Z","iopub.execute_input":"2025-12-31T13:53:02.747094Z","iopub.status.idle":"2025-12-31T13:53:02.752806Z","shell.execute_reply.started":"2025-12-31T13:53:02.747078Z","shell.execute_reply":"2025-12-31T13:53:02.752437Z"}},"outputs":[],"execution_count":4},{"id":"53964c8d-e6a1-4e7b-aa07-16dd557c6c3e","cell_type":"code","source":"# =========================\n# CELL 5/13 — DIFFICULTY ROUTER\n# =========================\n@dataclass(frozen=True)\nclass Plan:\n    tag: str\n    budget_weight: float\n    stage1_max_k: int\n    stage2_max_k: int\n    stage1_max_tokens: int\n    stage2_max_tokens: int\n    temp1: float\n    temp2: float\n    top_p1: float\n    top_p2: float\n\ndef route_problem(problem: str) -> Plan:\n    p = (problem or \"\").lower()\n\n    if any(k in p for k in [\"triangle\",\"circle\",\"radius\",\"angle\",\"tangent\",\"perpendicular\",\"circum\",\"inscribed\"]):\n        return Plan(\"GEO\", 1.20, 6, 14, 950, 1800, 0.55, 0.75, 0.92, 0.90)\n\n    if any(k in p for k in [\"mod\",\"congruen\",\"prime\",\"gcd\",\"lcm\",\"divis\",\"remainder\",\"coprime\",\"valuation\",\"phi(\"]):\n        return Plan(\"NT\", 1.30, 6, 16, 980, 1950, 0.55, 0.78, 0.92, 0.90)\n\n    if any(k in p for k in [\"f(\",\"functional\",\"for all real\",\"for all integers\",\"for all x\",\"for all n\",\"satisfies\"]):\n        return Plan(\"FUNC\", 1.25, 6, 16, 980, 1950, 0.55, 0.78, 0.92, 0.90)\n\n    if any(k in p for k in [\"probability\",\"expected\",\"random\",\"uniform\",\"dice\",\"coin\",\"distribution\"]):\n        return Plan(\"PROB\", 1.15, 6, 14, 950, 1800, 0.55, 0.75, 0.92, 0.90)\n\n    if any(k in p for k in [\"ways\",\"choose\",\"arrangements\",\"permutation\",\"combination\",\"graph\",\"color\",\"pigeonhole\",\"invariant\"]):\n        return Plan(\"COMB\", 1.20, 6, 16, 950, 1900, 0.55, 0.78, 0.92, 0.90)\n\n    return Plan(\"ALG\", 1.00, 6, 14, 900, 1700, 0.50, 0.72, 0.92, 0.90)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T13:53:02.753673Z","iopub.execute_input":"2025-12-31T13:53:02.753821Z","iopub.status.idle":"2025-12-31T13:53:02.773326Z","shell.execute_reply.started":"2025-12-31T13:53:02.753806Z","shell.execute_reply":"2025-12-31T13:53:02.772926Z"}},"outputs":[],"execution_count":5},{"id":"0ef98e5f-f935-4106-b31c-6b46e5e5048f","cell_type":"code","source":"# =========================\n# CELL 6/13 — PROMPTS\n# =========================\nSYSTEM_TIR = (\n    \"You are an olympiad math solver.\\n\"\n    \"You MUST follow this protocol:\\n\\n\"\n    \"If you need computation, output exactly:\\n\"\n    \"<tool:python>\\n\"\n    \"# python code\\n\"\n    \"</tool:python>\\n\\n\"\n    \"If you are ready to answer, output exactly ONE line:\\n\"\n    \"\\\\boxed{NONNEGATIVE_INTEGER}\\n\\n\"\n    \"Rules:\\n\"\n    \"- Output NOTHING else outside the tool block.\\n\"\n    \"- Final answer must be an integer in [0, 99999].\\n\"\n    \"- Prefer verifying with python when possible.\\n\"\n)\n\nSYSTEM_VERIFY = (\n    \"You are a strict verifier.\\n\"\n    \"Given a problem and proposed integer answer A, DISPROVE it quickly.\\n\"\n    \"Use python checks when possible:\\n\"\n    \"- parity constraints\\n\"\n    \"- modular constraints\\n\"\n    \"- substitution / brute force small cases / random tests\\n\\n\"\n    \"Protocol:\\n\"\n    \"- You may output <tool:python>...</tool:python> blocks.\\n\"\n    \"- Then output EXACTLY one final line: PASS or FAIL or UNKNOWN\\n\"\n    \"- No extra text.\\n\"\n)\n\nSYSTEM_SELECT = (\n    \"You are a selector.\\n\"\n    \"Pick the most reliable candidate answer based on evidence.\\n\"\n    \"Output EXACTLY one line: \\\\boxed{NONNEGATIVE_INTEGER}\\n\"\n    \"No extra text.\\n\"\n)\n\nHINTS = [\n    \"Tool-first: explore small cases in python, infer pattern, verify, then output boxed.\",\n    \"Proof-first: derive symbolic structure, then minimal python verification, output boxed.\",\n    \"Number-theory: use modular constraints/parity/gcd; python to test; output boxed.\",\n    \"Comb/Prob: use invariants or counting; python to validate small n; output boxed.\",\n]\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T13:53:02.774008Z","iopub.execute_input":"2025-12-31T13:53:02.774243Z","iopub.status.idle":"2025-12-31T13:53:02.782091Z","shell.execute_reply.started":"2025-12-31T13:53:02.774229Z","shell.execute_reply":"2025-12-31T13:53:02.781754Z"}},"outputs":[],"execution_count":6},{"id":"23994364-1012-4980-9464-6b68fa647214","cell_type":"code","source":"# =========================\n# CELL 7/13 — PROMPT BUILDER (chat template)\n# =========================\nclass PromptBuilder:\n    def __init__(self, tok):\n        self.tok = tok\n\n    def render(self, system: str, user: str, history: List[Tuple[str, str]]) -> str:\n        msgs = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n        for r, c in history:\n            msgs.append({\"role\": r, \"content\": c})\n        try:\n            return self.tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n        except Exception:\n            out = [f\"[SYSTEM]\\n{system}\\n\", f\"[USER]\\n{user}\\n\"]\n            for r, c in history:\n                out.append(f\"[{r.upper()}]\\n{c}\\n\")\n            out.append(\"[ASSISTANT]\\n\")\n            return \"\\n\".join(out)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T13:53:02.782593Z","iopub.execute_input":"2025-12-31T13:53:02.782712Z","iopub.status.idle":"2025-12-31T13:53:02.790939Z","shell.execute_reply.started":"2025-12-31T13:53:02.782700Z","shell.execute_reply":"2025-12-31T13:53:02.790486Z"}},"outputs":[],"execution_count":7},{"id":"c58e0df1-542e-4dd5-b8b3-504c9fe7e0e0","cell_type":"code","source":"# =========================\n# CELL 8/13 — PYTHON TOOL POOL (Subprocess, Kaggle-stable) [FIXED]\n# - One ToolPool only\n# - Has .run(wid, code, timeout_s) -> (out, ok) to match engine\n# =========================\nimport os, sys, json, queue, uuid, subprocess, threading\nfrom contextlib import contextmanager\nfrom typing import List, Dict, Optional, Tuple\n\n_WORKER_SRC = r\"\"\"\nimport sys, json, traceback, io, contextlib, math, random, itertools\ntry:\n    import sympy as sp\nexcept Exception:\n    sp = None\n\nG = {\"math\": math, \"random\": random, \"itertools\": itertools, \"sp\": sp}\n\ndef handle(req):\n    code = req.get(\"code\", \"\")\n    out_io = io.StringIO()\n    ok = True\n    with contextlib.redirect_stdout(out_io), contextlib.redirect_stderr(out_io):\n        try:\n            exec(compile(code, \"<tool>\", \"exec\"), G, G)\n        except Exception:\n            ok = False\n            traceback.print_exc(limit=3)\n\n    txt = out_io.getvalue().strip()\n    if not txt:\n        for k in (\"__result__\", \"result\", \"ans\", \"_\"):\n            if k in G:\n                try:\n                    txt = str(G[k])\n                    break\n                except Exception:\n                    pass\n    if not txt:\n        txt = \"[WARN] No output. Use print().\"\n    if len(txt) > 2000:\n        txt = txt[:2000] + \"\\n[...TRUNCATED...]\"\n    return {\"id\": req.get(\"id\"), \"ok\": ok, \"out\": txt}\n\nfor line in sys.stdin:\n    line = line.strip()\n    if not line:\n        continue\n    try:\n        req = json.loads(line)\n    except Exception:\n        continue\n    resp = handle(req)\n    sys.stdout.write(json.dumps(resp, ensure_ascii=False) + \"\\n\")\n    sys.stdout.flush()\n\"\"\"\n\nclass SubprocessToolWorker:\n    def __init__(self):\n        self.proc: Optional[subprocess.Popen] = None\n        self._pending: Dict[str, \"queue.Queue[dict]\"] = {}\n        self._lock = threading.Lock()\n        self._reader_thread: Optional[threading.Thread] = None\n        self.start()\n\n    def start(self):\n        self.stop()\n        env = dict(os.environ)\n        env[\"PYTHONUNBUFFERED\"] = \"1\"\n        self.proc = subprocess.Popen(\n            [sys.executable, \"-u\", \"-c\", _WORKER_SRC],\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            bufsize=1,\n            env=env,\n        )\n\n        def _reader():\n            assert self.proc is not None and self.proc.stdout is not None\n            for line in self.proc.stdout:\n                line = line.strip()\n                if not line:\n                    continue\n                try:\n                    msg = json.loads(line)\n                except Exception:\n                    continue\n                rid = msg.get(\"id\")\n                if not rid:\n                    continue\n                q = self._pending.pop(rid, None)\n                if q is not None:\n                    q.put(msg)\n\n        self._reader_thread = threading.Thread(target=_reader, daemon=True)\n        self._reader_thread.start()\n\n    def is_alive(self) -> bool:\n        return self.proc is not None and (self.proc.poll() is None)\n\n    def stop(self):\n        if self.proc is not None:\n            try:\n                self.proc.kill()\n            except Exception:\n                pass\n            try:\n                self.proc.wait(timeout=1)\n            except Exception:\n                pass\n        self.proc = None\n        self._pending.clear()\n\n    def execute(self, code: str, timeout_s: float) -> Tuple[str, bool]:\n        if not self.is_alive():\n            self.start()\n        assert self.proc is not None and self.proc.stdin is not None\n\n        rid = uuid.uuid4().hex\n        q: \"queue.Queue[dict]\" = queue.Queue(maxsize=1)\n        self._pending[rid] = q\n\n        payload = {\"id\": rid, \"code\": code}\n        with self._lock:\n            try:\n                self.proc.stdin.write(json.dumps(payload, ensure_ascii=False) + \"\\n\")\n                self.proc.stdin.flush()\n            except Exception:\n                self.start()\n                return \"[PYTHON_ERROR] worker write failed\", False\n\n        try:\n            msg = q.get(timeout=timeout_s)\n        except queue.Empty:\n            self.start()\n            return \"[PYTHON_TIMEOUT]\", False\n\n        out = (msg.get(\"out\", \"\") or \"\").strip() or \"[WARN] No output. Use print().\"\n        ok = bool(msg.get(\"ok\", False))\n        return out, ok\n\nclass ToolPool:\n    def __init__(self, size: int):\n        self.size = max(1, int(size))\n        self.workers: List[SubprocessToolWorker] = [SubprocessToolWorker() for _ in range(self.size)]\n\n    def run(self, wid: int, code: str, timeout_s: float) -> Tuple[str, bool]:\n        w = self.workers[int(wid) % self.size]\n        return w.execute(code, timeout_s)\n\n    def close(self):\n        for w in self.workers:\n            w.stop()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T13:53:02.791597Z","iopub.execute_input":"2025-12-31T13:53:02.791795Z","iopub.status.idle":"2025-12-31T13:53:02.803745Z","shell.execute_reply.started":"2025-12-31T13:53:02.791778Z","shell.execute_reply":"2025-12-31T13:53:02.803315Z"}},"outputs":[],"execution_count":8},{"id":"8b30f7be-c787-44b1-987e-f0b058a79e53","cell_type":"code","source":"# =========================\n# CELL 9/13 — BACKEND (HF) + tokenizer  [REPLACE THIS CELL]\n# FIX:\n# - left padding for decoder-only\n# - remove_invalid_values + InfNanRemoveLogitsProcessor\n# - renormalize_logits\n# - safe sampling clamp\n# =========================\nimport os, importlib.util\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\ntokenizer.padding_side = \"left\"\ntokenizer.truncation_side = \"left\"\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nclass BackendBase:\n    def generate(self, prompts: List[str], *, temperature: float, top_p: float, max_tokens: int) -> List[str]:\n        raise NotImplementedError\n\ndef _dtype_from_str(s: str):\n    s = (s or \"\").lower()\n    if \"bf16\" in s or \"bfloat\" in s:\n        return torch.bfloat16\n    if \"fp16\" in s or \"float16\" in s:\n        return torch.float16\n    return torch.bfloat16\n\n# logits sanitizers (robust across transformers versions)\ndef _get_logits_sanitizer():\n    try:\n        from transformers.generation.logits_process import LogitsProcessorList, InfNanRemoveLogitsProcessor\n        return LogitsProcessorList([InfNanRemoveLogitsProcessor()])\n    except Exception:\n        return None\n\n_SANITIZER = _get_logits_sanitizer()\n\nclass HFBackend(BackendBase):\n    def __init__(self, model_path: str, max_model_len: int):\n        self.tokenizer = tokenizer\n\n        # stability-first: sdpa tends to be safer than flash for some fp8 stacks\n        # if you insist speed: set USE_FLASH_ATTN=1\n        use_flash = os.getenv(\"USE_FLASH_ATTN\", \"0\") == \"1\"\n        has_flash = importlib.util.find_spec(\"flash_attn\") is not None\n        attn_impl = \"flash_attention_2\" if (use_flash and has_flash) else \"sdpa\"\n\n        base_kwargs = dict(\n            dtype=_dtype_from_str(DTYPE),\n            device_map=\"auto\",\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n        )\n\n        try:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                attn_implementation=attn_impl,\n                **base_kwargs,\n            )\n        except Exception:\n            self.model = AutoModelForCausalLM.from_pretrained(model_path, **base_kwargs)\n\n        self.model.eval()\n        self.max_model_len = int(max_model_len)\n\n        self.eos_id = self.tokenizer.eos_token_id\n        self.pad_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.eos_id\n\n        self.bs = int(GEN_BATCH_SIZE)\n\n        # generation config safety knobs\n        try:\n            self.model.generation_config.remove_invalid_values = True\n            self.model.generation_config.renormalize_logits = True\n        except Exception:\n            pass\n\n    @torch.inference_mode()\n    def generate(self, prompts: List[str], *, temperature: float, top_p: float, max_tokens: int) -> List[str]:\n        outs: List[str] = []\n\n        # clamp sampling params to avoid weirdness\n        t = float(temperature) if temperature is not None else 0.0\n        p = float(top_p) if top_p is not None else 1.0\n        t = 0.0 if t < 1e-6 else min(1.2, max(0.05, t))\n        p = min(1.0, max(0.05, p))\n        do_sample = t > 1e-6\n\n        i = 0\n        while i < len(prompts):\n            bs = max(1, int(self.bs))\n            batch = prompts[i:i+bs]\n\n            enc = self.tokenizer(\n                batch,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=self.max_model_len,\n            )\n            enc = {k: v.to(self.model.device) for k, v in enc.items()}\n            input_len = int(enc[\"input_ids\"].shape[1])\n\n            gen_kwargs = dict(\n                **enc,\n                max_new_tokens=int(max_tokens),\n                use_cache=True,\n                pad_token_id=self.pad_id,\n                eos_token_id=self.eos_id,\n                do_sample=do_sample,\n                temperature=(t if do_sample else None),\n                top_p=(p if do_sample else None),\n                remove_invalid_values=True,   # key fix\n                renormalize_logits=True,     # key fix\n            )\n            if _SANITIZER is not None:\n                gen_kwargs[\"logits_processor\"] = _SANITIZER\n\n            try:\n                gen = self.model.generate(**gen_kwargs)\n            except RuntimeError as e:\n                msg = str(e).lower()\n                if \"out of memory\" in msg or \"cuda out of memory\" in msg:\n                    torch.cuda.empty_cache()\n                    self.bs = max(1, self.bs // 2)\n                    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n                        print(f\"[hf] OOM -> backoff GEN_BATCH_SIZE to {self.bs}\")\n                    continue\n                raise\n\n            for j in range(gen.shape[0]):\n                tail = gen[j, input_len:]\n                outs.append(self.tokenizer.decode(tail, skip_special_tokens=True))\n\n            i += bs\n\n        return outs\n\n_backend = HFBackend(MODEL_PATH, max_model_len=MAX_MODEL_LEN)\n_backend_name = \"hf\"\nprint(f\"[backend] loaded {_backend_name} | attn={os.getenv('USE_FLASH_ATTN','0')} | GEN_BATCH_SIZE={GEN_BATCH_SIZE} | MAX_MODEL_LEN={MAX_MODEL_LEN}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T13:59:44.115688Z","iopub.execute_input":"2025-12-31T13:59:44.116266Z","iopub.status.idle":"2025-12-31T14:03:12.954005Z","shell.execute_reply.started":"2025-12-31T13:59:44.116247Z","shell.execute_reply":"2025-12-31T14:03:12.953506Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c08cd33a0974dfdb401db6f8fb3cbed"}},"metadata":{}},{"name":"stdout","text":"[backend] loaded hf | attn=0 | GEN_BATCH_SIZE=8 | MAX_MODEL_LEN=12288\n","output_type":"stream"}],"execution_count":14},{"id":"65188a57-916f-4318-a3e5-34a4d2dff9d2","cell_type":"code","source":"# =========================\n# CELL 10/13 — TIME MANAGER\n# =========================\nclass TimeManager:\n    def __init__(self, hard_wall_s: int, total_questions: int):\n        self.start = time.time()\n        self.deadline = self.start + int(hard_wall_s)\n        self.total = max(1, int(total_questions))\n        self.done = 0\n\n    def remaining(self) -> float:\n        return max(0.0, self.deadline - time.time())\n\n    def budget(self, weight: float) -> float:\n        rem = self.remaining()\n        left = max(1, self.total - self.done)\n        base = rem / left\n        return clamp(base * float(weight), MIN_BUDGET_S, MAX_BUDGET_S)\n\n    def mark_done(self) -> None:\n        self.done += 1\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:03:12.954852Z","iopub.execute_input":"2025-12-31T14:03:12.955005Z","iopub.status.idle":"2025-12-31T14:03:12.958850Z","shell.execute_reply.started":"2025-12-31T14:03:12.954990Z","shell.execute_reply":"2025-12-31T14:03:12.958469Z"}},"outputs":[],"execution_count":15},{"id":"54906722-c28f-4dea-9006-7de074a5144d","cell_type":"code","source":"# =========================\n# CELL 11/13 — CANDIDATES + WEIGHTED VOTE\n# =========================\n@dataclass\nclass Candidate:\n    answer: Optional[int]\n    raw: str\n    tool_calls: int\n    tool_errors: int\n    elapsed: float\n    stage: int\n    verified: Optional[bool] = None  # PASS->True, FAIL->False, UNKNOWN->None\n\ndef cand_weight(c: Candidate) -> float:\n    if c.answer is None:\n        return 0.0\n    w = 1.0\n    if c.tool_calls > 0 and c.tool_errors == 0:\n        w += 0.9\n    w -= 0.75 * c.tool_errors\n    w += max(0.0, 0.35 - 0.015 * c.elapsed)\n    if c.verified is True:\n        w += 2.25\n    if c.verified is False:\n        w -= 2.25\n    return max(0.0, w)\n\ndef weighted_vote(cands: List[Candidate]) -> Tuple[Optional[int], float, Dict[int, float]]:\n    scores: Dict[int, float] = {}\n    total = 0.0\n    for c in cands:\n        if c.answer is None:\n            continue\n        w = cand_weight(c)\n        total += w\n        scores[c.answer] = scores.get(c.answer, 0.0) + w\n    if not scores or total <= 1e-9:\n        return None, 0.0, {}\n    best = max(scores.items(), key=lambda kv: kv[1])[0]\n    ratio = scores[best] / total\n    return best, ratio, scores\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:03:12.959370Z","iopub.execute_input":"2025-12-31T14:03:12.959539Z","iopub.status.idle":"2025-12-31T14:03:12.967948Z","shell.execute_reply.started":"2025-12-31T14:03:12.959526Z","shell.execute_reply":"2025-12-31T14:03:12.967550Z"}},"outputs":[],"execution_count":16},{"id":"3e829212-f307-4871-b0d5-2f61b434f195","cell_type":"code","source":"# =========================\n# CELL 12/13 — TIR ENGINE + VERIFIER + SELECTOR\n# =========================\n@dataclass\nclass TIRState:\n    history: List[Tuple[str, str]]\n    hint: str\n    worker_id: int\n    done: bool = False\n    answer: Optional[int] = None\n    tool_calls: int = 0\n    tool_errors: int = 0\n    raw_last: str = \"\"\n\nclass TIRBatchEngine:\n    def __init__(self, backend: BackendBase, pb: PromptBuilder, tools: ToolPool):\n        self.backend = backend\n        self.pb = pb\n        self.tools = tools\n\n    def _turn(self, problem: str, states: List[TIRState], *, temperature: float, top_p: float, max_tokens: int, time_left_s: float) -> None:\n        active = [i for i, s in enumerate(states) if not s.done]\n        if not active:\n            return\n\n        prompts: List[str] = []\n        for i in active:\n            s = states[i]\n            user = f\"{problem}\\n\\nHint: {s.hint}\"\n            prompts.append(self.pb.render(SYSTEM_TIR, user, s.history))\n\n        outs = self.backend.generate(prompts, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n\n        tool_jobs: List[Tuple[int, int, str]] = []  # (state_idx, worker_id, code)\n        for idx, out in zip(active, outs):\n            out = strip_think(out)\n            st = states[idx]\n            st.raw_last = out\n\n            ans = parse_boxed_int(out)\n            if ans is not None:\n                st.done = True\n                st.answer = ans\n                continue\n\n            code = parse_tool_code(out)\n            if code:\n                st.tool_calls += 1\n                tool_jobs.append((idx, st.worker_id, code))\n                continue\n\n            st.history.append((\"assistant\", out[:800]))\n            st.history.append((\"user\", \"Output ONLY either a <tool:python> block OR one line \\\\boxed{integer}.\"))\n            st.history = trim_history(st.history)\n\n        if not tool_jobs:\n            return\n\n        def run_one(job: Tuple[int, int, str]) -> Tuple[int, str, str, bool]:\n            i, wid, code = job\n            py_out, ok = self.tools.run(wid, code, timeout_s=min(TOOL_TIMEOUT_S, max(1.0, time_left_s)))\n            return i, code, py_out, ok\n\n        with ThreadPoolExecutor(max_workers=min(len(tool_jobs), TOOL_THREAD_WORKERS)) as ex:\n            futs = [ex.submit(run_one, j) for j in tool_jobs]\n            for fut in as_completed(futs):\n                i, code, py_out, ok = fut.result()\n                st = states[i]\n                if (not ok) or (\"PYTHON_TIMEOUT\" in py_out) or (\"Traceback\" in py_out):\n                    st.tool_errors += 1\n                st.history.append((\"assistant\", f\"<tool:python>\\n{code}\\n</tool:python>\"))\n                st.history.append((\"user\", f\"Python output:\\n{py_out}\"))\n                st.history = trim_history(st.history)\n\n    def run_progressive(\n        self,\n        problem: str,\n        *,\n        max_k: int,\n        batch_k: int,\n        budget_s: float,\n        stage: int,\n        max_tokens: int,\n        temperature: float,\n        top_p: float,\n        early_stop_ratio: float,\n    ) -> List[Candidate]:\n        t0 = time.time()\n        cands: List[Candidate] = []\n        created = 0\n\n        while created < max_k and (time.time() - t0) < budget_s:\n            add = min(batch_k, max_k - created)\n            states = [\n                TIRState(history=[], hint=HINTS[(created + i) % len(HINTS)], worker_id=(created + i) % TOOL_POOL_SIZE)\n                for i in range(add)\n            ]\n            created += add\n\n            for _ in range(MAX_TURNS):\n                if time.time() - t0 >= budget_s:\n                    break\n                self._turn(\n                    problem, states,\n                    temperature=temperature, top_p=top_p,\n                    max_tokens=max_tokens,\n                    time_left_s=budget_s - (time.time() - t0),\n                )\n                if all(s.done for s in states):\n                    break\n\n            elapsed = time.time() - t0\n            for s in states:\n                ans = s.answer\n                if ans is None:\n                    ans = parse_boxed_int(s.raw_last) or fallback_last_int(s.raw_last)\n                cands.append(Candidate(\n                    answer=ans,\n                    raw=s.raw_last,\n                    tool_calls=s.tool_calls,\n                    tool_errors=s.tool_errors,\n                    elapsed=elapsed,\n                    stage=stage,\n                ))\n\n            best, ratio, _ = weighted_vote(cands)\n            if best is not None and ratio >= early_stop_ratio:\n                break\n\n        return cands\n\nclass Verifier:\n    def __init__(self, backend: BackendBase, pb: PromptBuilder, tools: ToolPool):\n        self.backend = backend\n        self.pb = pb\n        self.tools = tools\n\n    def verify(self, problem: str, answer: int, budget_s: float) -> Optional[bool]:\n        t0 = time.time()\n        history: List[Tuple[str, str]] = []\n        user = f\"Problem:\\n{problem}\\n\\nProposed answer A = {answer}\\n\"\n\n        for _ in range(6):\n            if time.time() - t0 >= budget_s:\n                return None\n\n            prompt = self.pb.render(SYSTEM_VERIFY, user, history)\n            out = strip_think(self.backend.generate([prompt], temperature=0.0, top_p=1.0, max_tokens=650)[0])\n\n            code = parse_tool_code(out)\n            if code:\n                py_out, ok = self.tools.run(0, code, timeout_s=min(TOOL_TIMEOUT_S, max(1.0, budget_s - (time.time() - t0))))\n                history.append((\"assistant\", f\"<tool:python>\\n{code}\\n</tool:python>\"))\n                history.append((\"user\", f\"Python output:\\n{py_out}\"))\n                history = trim_history(history, 10)\n                continue\n\n            last = out.strip().splitlines()[-1].strip().upper() if out.strip() else \"\"\n            if last == \"PASS\":\n                return True\n            if last == \"FAIL\":\n                return False\n            if last == \"UNKNOWN\":\n                return None\n\n            history.append((\"assistant\", out[:800]))\n            history.append((\"user\", \"Return ONLY one final line: PASS or FAIL or UNKNOWN.\"))\n            history = trim_history(history, 10)\n\n        return None\n\nclass Selector:\n    def __init__(self, backend: BackendBase, pb: PromptBuilder):\n        self.backend = backend\n        self.pb = pb\n\n    def select(self, problem: str, scores: Dict[int, float]) -> Optional[int]:\n        if not scores:\n            return None\n        items = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[:8]\n        evidence = \"\\n\".join([f\"- {a}: score={s:.3f}\" for a, s in items])\n        prompt = self.pb.render(SYSTEM_SELECT, f\"Problem:\\n{problem}\\n\\nCandidate scores:\\n{evidence}\\n\", [])\n        out = strip_think(self.backend.generate([prompt], temperature=0.0, top_p=1.0, max_tokens=220)[0])\n        return parse_boxed_int(out) or fallback_last_int(out)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:03:12.968874Z","iopub.execute_input":"2025-12-31T14:03:12.969039Z","iopub.status.idle":"2025-12-31T14:03:12.986005Z","shell.execute_reply.started":"2025-12-31T14:03:12.969025Z","shell.execute_reply":"2025-12-31T14:03:12.985601Z"}},"outputs":[],"execution_count":17},{"id":"4eab14fd-1a35-442e-a826-aba4637f2b75","cell_type":"code","source":"# =========================\n# CELL 13/13 — SOLVER + DEV TEST + KAGGLE HOOK\n# =========================\nclass AIMO3Solver:\n    def __init__(self):\n        self.pb = PromptBuilder(tokenizer)\n        self.tools = ToolPool(TOOL_POOL_SIZE)\n        self.backend = _backend\n        self.tm = TimeManager(HARD_WALL_SECONDS, TOTAL_QUESTIONS)\n        self.engine = TIRBatchEngine(self.backend, self.pb, self.tools)\n        self.verifier = Verifier(self.backend, self.pb, self.tools)\n        self.selector = Selector(self.backend, self.pb)\n\n        if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n            print(f\"[solver] backend = {_backend_name}\")\n\n    def close(self):\n        self.tools.close()\n\n    def solve_problem(self, problem: str) -> int:\n        plan = route_problem(problem)\n        rem = self.tm.remaining()\n        if rem < 5.0:\n            return 0\n\n        budget = min(self.tm.budget(plan.budget_weight), rem)\n\n        # Stage 1\n        c1 = self.engine.run_progressive(\n            problem,\n            max_k=plan.stage1_max_k,\n            batch_k=STAGE1_BATCH,\n            budget_s=0.38 * budget,\n            stage=1,\n            max_tokens=plan.stage1_max_tokens,\n            temperature=plan.temp1,\n            top_p=plan.top_p1,\n            early_stop_ratio=CONFIDENT_RATIO,\n        )\n        best, ratio, scores = weighted_vote(c1)\n        if best is not None and ratio >= CONFIDENT_RATIO:\n            self.tm.mark_done()\n            return mod100000(best)\n\n        # Stage 2\n        c2 = self.engine.run_progressive(\n            problem,\n            max_k=plan.stage2_max_k,\n            batch_k=STAGE2_BATCH,\n            budget_s=0.50 * budget,\n            stage=2,\n            max_tokens=plan.stage2_max_tokens,\n            temperature=plan.temp2,\n            top_p=plan.top_p2,\n            early_stop_ratio=CONFIDENT_RATIO,\n        )\n\n        all_c = c1 + c2\n        best, ratio, scores = weighted_vote(all_c)\n\n        if best is None:\n            self.tm.mark_done()\n            return 0\n\n        # Verifier-on-uncertainty\n        if ratio < VERIFY_RATIO and budget >= 25.0:\n            top = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[:VERIFY_TOP_N]\n            per_verify_budget = 0.10 * budget / max(1, len(top))\n\n            for ans, _ in top:\n                verdict = self.verifier.verify(problem, ans, budget_s=per_verify_budget)\n                for c in all_c:\n                    if c.answer == ans:\n                        c.verified = verdict\n\n            best, ratio, scores = weighted_vote(all_c)\n\n        # Deterministic selector if still not confident\n        final = best\n        if ratio < 0.80 and (0.07 * budget) >= 3.0:\n            sel = self.selector.select(problem, scores)\n            if sel is not None:\n                final = sel\n\n        self.tm.mark_done()\n        return mod100000(final)\n\nsolver = AIMO3Solver()\n\ndef predict(id_: \"pl.Series\", problem: \"pl.Series\"):\n    if pl is not None and isinstance(id_, pl.Series):\n        pid = id_.item(0)\n        prob = problem.item(0)\n        ans = solver.solve_problem(prob)\n        return pl.DataFrame({\"id\": [pid], \"answer\": [ans]})\n    else:\n        pid = id_[0] if hasattr(id_, \"__len__\") else id_\n        prob = problem[0] if hasattr(problem, \"__len__\") else problem\n        ans = solver.solve_problem(prob)\n        return pd.DataFrame({\"id\": [pid], \"answer\": [ans]})\n\n# ---- DEV EVAL ----\ndef _find_comp_file(fname: str) -> Optional[str]:\n    hits = glob.glob(f\"/kaggle/input/*/{fname}\")\n    return hits[0] if hits else None\n\ndef dev_eval(n: int = 30):\n    ref_path = _find_comp_file(\"reference.csv\")\n    if not ref_path:\n        print(\"[dev_eval] reference.csv not found in /kaggle/input/*/\")\n        return\n\n    df = pd.read_csv(ref_path)\n    if \"problem\" not in df.columns:\n        print(\"[dev_eval] reference.csv missing 'problem' column\")\n        return\n\n    has_gt = \"answer\" in df.columns\n    gt = df.set_index(\"id\")[\"answer\"].to_dict() if has_gt else None\n\n    n = min(n, len(df))\n    sub = df.iloc[:n].copy()\n\n    t0 = time.time()\n    correct = 0\n    done = 0\n\n    for _, row in sub.iterrows():\n        pid = row[\"id\"]\n        prob = row[\"problem\"]\n        ans = solver.solve_problem(prob)\n        done += 1\n        if has_gt and int(ans) == int(gt[pid]):\n            correct += 1\n\n        if done % 5 == 0:\n            elapsed = time.time() - t0\n            if has_gt:\n                print(f\"[dev_eval] {done}/{n}  elapsed={elapsed:.1f}s  acc={100*correct/done:.1f}%\")\n            else:\n                print(f\"[dev_eval] {done}/{n}  elapsed={elapsed:.1f}s\")\n\n    elapsed = time.time() - t0\n    if has_gt:\n        print(f\"[dev_eval] FINAL: {correct}/{n} = {100*correct/n:.1f}%  | time={elapsed:.1f}s\")\n    else:\n        print(f\"[dev_eval] FINAL: done {n} problems | time={elapsed:.1f}s | (no ground truth in reference.csv)\")\n\n# ---- Kaggle Inference Server ----\nimport kaggle_evaluation.aimo_3_inference_server as aimo3\ninference_server = aimo3.AIMO3InferenceServer(predict)\n\nif os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    inference_server.serve()\nelse:\n    print(\"[dev] solver loaded. Running dev_eval() on reference.csv ...\")\n    dev_eval(n=int(os.getenv(\"DEV_N\", \"30\")))\n\n    if os.getenv(\"DEV_RUN_GATEWAY\", \"0\") == \"1\":\n        ref_path = _find_comp_file(\"reference.csv\")\n        df = pd.read_csv(ref_path)\n        tmp = df[[\"id\", \"problem\"]].head(int(os.getenv(\"DEV_GATEWAY_N\", \"10\")))\n        tmp_path = \"ref_input_head.csv\"\n        tmp.to_csv(tmp_path, index=False)\n        print(f\"[dev] run_local_gateway on {tmp_path}\")\n        inference_server.run_local_gateway((tmp_path,))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:03:12.986589Z","iopub.execute_input":"2025-12-31T14:03:12.986723Z","iopub.status.idle":"2025-12-31T14:03:13.141043Z","shell.execute_reply.started":"2025-12-31T14:03:12.986709Z","shell.execute_reply":"2025-12-31T14:03:13.139836Z"}},"outputs":[{"name":"stderr","text":"A custom logits processor of type <class 'transformers.generation.logits_process.InfNanRemoveLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.InfNanRemoveLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.InfNanRemoveLogitsProcessor'> to see related `.generate()` flags.\n","output_type":"stream"},{"name":"stdout","text":"[solver] backend = hf\n[dev] solver loaded. Running dev_eval() on reference.csv ...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_105/3975538817.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[dev] solver loaded. Running dev_eval() on reference.csv ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mdev_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DEV_N\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"30\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DEV_RUN_GATEWAY\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_105/3975538817.py\u001b[0m in \u001b[0;36mdev_eval\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"problem\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve_problem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_gt\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_105/3975538817.py\u001b[0m in \u001b[0;36msolve_problem\u001b[0;34m(self, problem)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Stage 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         c1 = self.engine.run_progressive(\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mmax_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage1_max_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_105/3627310513.py\u001b[0m in \u001b[0;36mrun_progressive\u001b[0;34m(self, problem, max_k, batch_k, budget_s, stage, max_tokens, temperature, top_p, early_stop_ratio)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbudget_s\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 self._turn(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_105/3627310513.py\u001b[0m in \u001b[0;36m_turn\u001b[0;34m(self, problem, states, temperature, top_p, max_tokens, time_left_s)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprompts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSYSTEM_TIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtool_jobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (state_idx, worker_id, code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_105/1782810897.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, temperature, top_p, max_tokens)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mgen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3_moe/modeling_qwen3_moe.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_router_logits, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         outputs: MoeModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3_moe/modeling_qwen3_moe.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3_moe/modeling_qwen3_moe.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         hidden_states, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3_moe/modeling_qwen3_moe.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mhidden_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/finegrained_fp8.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mdevice_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_accelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_accelerator_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mtorch_accelerator_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch_accelerator_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m                 \u001b[0mqinput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                 output = w8a8_block_fp8_matmul_triton(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/_utils.py\u001b[0m in \u001b[0;36m_get_device_index\u001b[0;34m(device, optional, allow_cpu)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected a cuda or cpu device, but got: {device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected a cuda device, but got: {device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected a cuda device, but got: cpu"],"ename":"ValueError","evalue":"Expected a cuda device, but got: cpu","output_type":"error"}],"execution_count":18}]}