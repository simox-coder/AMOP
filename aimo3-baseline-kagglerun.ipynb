{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":499291,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":396608,"modelId":322000},{"sourceId":510391,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":404485,"modelId":422384},{"sourceId":363148,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":301526,"modelId":322000}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"8f3c8458-0b44-421a-8b44-f67c51f12ba3","cell_type":"markdown","source":"# AIMO3 Baseline Notebook\n## AI Mathematical Olympiad – Progress Prize 3\n\n### Quick Start Guide\n1. **RUN_MODE Selection**:\n   - `\"local_ref\"`: Debug mode - runs evaluation on `reference.csv` (10 problems)\n   - `\"submit_auto\"`: Kaggle submission mode - uses `kaggle_evaluation` API\n\n2. **Model Setup (Kaggle)**:\n   - Add your model as a Kaggle Dataset/Model input\n   - Update `MODEL_PATH` in CONFIG to point to `/kaggle/input/your-model-name`\n   - Or use Kaggle's built-in models\n\n3. **Telemetry**:\n   - Logs saved to `/kaggle/working/aimo3_telemetry.jsonl`\n   - Download after submission for analysis","metadata":{}},{"id":"67a7fccb-029b-4741-ad8c-a97ee3d1e827","cell_type":"markdown","source":"## CELL A — CONFIG (Constants)","metadata":{}},{"id":"3097b11f-bc9c-43b8-8af2-3b3a7987a999","cell_type":"code","source":"# ============================================================\n# AIMO3 NOTEBOOK — NEW SOLVER (from scratch) — PART 1/3\n# Cells: A → D\n# ============================================================\n\n# ============================================\n# CELL A — CONFIG\n# ============================================\n\nimport os\nK_BASE = 6\nTEMPERATURE_BASE = 0.6\nMAX_NEW_TOKENS = 2048\nTOP_P = 0.95\nTOP_K = 50\nAGENT_MAX_RETRIES = 3\nEXEC_TIMEOUT_SEC = 10\n\n# Modes:\n# - \"submit\": run Kaggle inference server\n# - \"local_ref\": run small local debug on reference.csv\nRUN_MODE = os.getenv(\"RUN_MODE\", \"submit\")\n\n# Model (you said: start fresh with 8B on H100)\nMODEL_PATH = os.getenv(\"MODEL_PATH\", \"/kaggle/input/qwen-3/transformers/8b-fp8/1\")\nMODEL_ID   = os.getenv(\"MODEL_ID\",   MODEL_PATH)\n\n# Generation budget (start conservative; dynamic escalation happens later)\nK_BASE = int(os.getenv(\"K_BASE\", \"4\"))                 # base samples per stage (batch)\nTEMPERATURE_BASE = float(os.getenv(\"TEMPERATURE\", \"0.6\"))\nTOP_P = float(os.getenv(\"TOP_P\", \"0.95\"))\nTOP_K = int(os.getenv(\"TOP_K\", \"50\"))\n\n# Token budgets by stage (short → longer)\nMAX_NEW_TOKENS_DIRECT = int(os.getenv(\"MAX_NEW_TOKENS_DIRECT\", \"256\"))\nMAX_NEW_TOKENS_CODE   = int(os.getenv(\"MAX_NEW_TOKENS_CODE\",   \"768\"))\nMAX_NEW_TOKENS_HEAVY  = int(os.getenv(\"MAX_NEW_TOKENS_HEAVY\",  \"1400\"))\n\n# Agent retries (per stage)\nAGENT_MAX_RETRIES = int(os.getenv(\"AGENT_MAX_RETRIES\", \"2\"))\n\n# Python execution timeout (seconds) — will be dynamically adjusted later too\nEXEC_TIMEOUT_SEC_BASE = int(os.getenv(\"EXEC_TIMEOUT_SEC\", \"10\"))\n\n# Overall logging\nVERBOSE = int(os.getenv(\"VERBOSE\", \"1\")) == 1\n\n# Paths\nREF_CSV_PATH = os.getenv(\n    \"REF_CSV_PATH\",\n    \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\",\n)\n\nprint(\"============================================================\")\nprint(\"AIMO3 NOTEBOOK — NEW SOLVER (Transformers) — PART 1/3\")\nprint(\"============================================================\")\nprint(f\"RUN_MODE: {RUN_MODE}\")\nprint(f\"MODEL_PATH: {MODEL_PATH}\")\nprint(f\"K_BASE={K_BASE}, TEMP={TEMPERATURE_BASE}, TOP_P={TOP_P}, TOP_K={TOP_K}\")\nprint(f\"MAX_NEW_TOKENS: direct={MAX_NEW_TOKENS_DIRECT}, code={MAX_NEW_TOKENS_CODE}, heavy={MAX_NEW_TOKENS_HEAVY}\")\nprint(f\"AGENT_MAX_RETRIES={AGENT_MAX_RETRIES}, EXEC_TIMEOUT_SEC_BASE={EXEC_TIMEOUT_SEC_BASE}\")\nprint(\"============================================================\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:50:41.698019Z","iopub.execute_input":"2025-12-30T20:50:41.698604Z","iopub.status.idle":"2025-12-30T20:50:41.703440Z","shell.execute_reply.started":"2025-12-30T20:50:41.698586Z","shell.execute_reply":"2025-12-30T20:50:41.703056Z"}},"outputs":[{"name":"stdout","text":"============================================================\nAIMO3 NOTEBOOK — NEW SOLVER (Transformers) — PART 1/3\n============================================================\nRUN_MODE: submit\nMODEL_PATH: /kaggle/input/qwen-3/transformers/8b-fp8/1\nK_BASE=4, TEMP=0.6, TOP_P=0.95, TOP_K=50\nMAX_NEW_TOKENS: direct=256, code=768, heavy=1400\nAGENT_MAX_RETRIES=2, EXEC_TIMEOUT_SEC_BASE=10\n============================================================\n","output_type":"stream"}],"execution_count":11},{"id":"e8524dbd-485a-411d-8c32-2b6dc53ba207","cell_type":"markdown","source":"## CELL B — IMPORTS + SEED CONTROL","metadata":{}},{"id":"eca0abe7-86c3-4104-ae6f-e59c0bd174d0","cell_type":"code","source":"# ============================================\n# CELL B — IMPORTS + UTILS\n# ============================================\n\nimport re\nimport time\nimport json\nimport math\nimport random\nimport tempfile\nimport subprocess\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Speed / determinism knobs\ntorch.set_float32_matmul_precision(\"high\")\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\ndef set_seed(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\ndef vprint(*args, **kwargs):\n    if VERBOSE:\n        print(*args, **kwargs)\n\ndef now_s() -> float:\n    return time.time()\n\ndef safe_int(x: Any) -> Optional[int]:\n    try:\n        if isinstance(x, bool):\n            return None\n        if isinstance(x, (int, np.integer)):\n            return int(x)\n        s = str(x).strip()\n        if re.fullmatch(r\"[+-]?\\d+\", s):\n            return int(s)\n        return None\n    except Exception:\n        return None\n\ndef clamp_abs_int(x: int, limit: int = 10**18) -> int:\n    if x > limit: return limit\n    if x < -limit: return -limit\n    return x\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:50:41.704099Z","iopub.execute_input":"2025-12-30T20:50:41.704232Z","iopub.status.idle":"2025-12-30T20:50:41.722323Z","shell.execute_reply.started":"2025-12-30T20:50:41.704218Z","shell.execute_reply":"2025-12-30T20:50:41.721973Z"}},"outputs":[],"execution_count":12},{"id":"f5b39b9c-fb9b-47c2-8a40-cf505cb2ea01","cell_type":"markdown","source":"## CELL C — LLM BACKEND (Transformers)","metadata":{}},{"id":"28a46843-ee10-4e61-8875-eaae2678d40d","cell_type":"code","source":"# ============================================\n# CELL C — LLM BACKEND (Transformers)\n# ============================================\n\n_TOKENIZER: Optional[AutoTokenizer] = None\n_MODEL: Optional[AutoModelForCausalLM] = None\n_DEVICE: Optional[torch.device] = None\n\ndef load_llm() -> Tuple[AutoTokenizer, AutoModelForCausalLM, torch.device]:\n    \"\"\"\n    Loads tokenizer + model once.\n    This cell is designed to be stable for Kaggle / Transformers.\n    \"\"\"\n    global _TOKENIZER, _MODEL, _DEVICE\n    if _TOKENIZER is not None and _MODEL is not None and _DEVICE is not None:\n        return _TOKENIZER, _MODEL, _DEVICE\n\n    vprint(\"[LLM] Loading tokenizer...\")\n    tok = AutoTokenizer.from_pretrained(\n        MODEL_ID,\n        trust_remote_code=True,\n        use_fast=True,\n    )\n\n    vprint(\"[LLM] Loading model...\")\n    # NOTE: use torch_dtype=\"auto\" because your Kaggle weights may be FP8/FP16/BF16 packaged.\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_PATH,\n        torch_dtype=\"auto\",\n        device_map=\"auto\",\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n    )\n    model.eval()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _TOKENIZER, _MODEL, _DEVICE = tok, model, device\n\n    vprint(\"[LLM] Loaded. device:\", device, \"| dtype:\", getattr(next(model.parameters(), None), \"dtype\", None))\n    return tok, model, device\n\ndef _apply_chat_template(tokenizer: AutoTokenizer, messages: List[Dict[str, str]]) -> str:\n    \"\"\"\n    Robust chat formatting: prefers tokenizer.apply_chat_template if available.\n    Falls back to simple role-prefixed transcript.\n    \"\"\"\n    if hasattr(tokenizer, \"apply_chat_template\"):\n        try:\n            return tokenizer.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True,\n            )\n        except Exception:\n            pass\n\n    # Fallback (works with most causal LMs, less optimal)\n    parts = []\n    for m in messages:\n        role = m.get(\"role\", \"user\")\n        content = m.get(\"content\", \"\")\n        parts.append(f\"{role.upper()}:\\n{content}\\n\")\n    parts.append(\"ASSISTANT:\\n\")\n    return \"\\n\".join(parts)\n\n@torch.inference_mode()\ndef llm_generate_texts(\n    batch_messages: List[List[Dict[str, str]]],\n    max_new_tokens: int,\n    temperature: float,\n    top_p: float,\n    top_k: int,\n) -> List[str]:\n    \"\"\"\n    Batch generation for higher GPU utilization.\n    batch_messages: list of chat message lists (len = batch size).\n    Returns: list of decoded assistant continuations (len = batch size).\n    \"\"\"\n    tokenizer, model, _ = load_llm()\n\n    prompts = [_apply_chat_template(tokenizer, msgs) for msgs in batch_messages]\n    enc = tokenizer(\n        prompts,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n    )\n    enc = {k: v.to(model.device) for k, v in enc.items()}\n\n    do_sample = temperature > 1e-6\n    gen = model.generate(\n        **enc,\n        do_sample=do_sample,\n        temperature=temperature if do_sample else None,\n        top_p=top_p if do_sample else None,\n        top_k=top_k if do_sample else None,\n        max_new_tokens=max_new_tokens,\n        pad_token_id=tokenizer.eos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n        use_cache=True,\n    )\n\n    # Slice off the prompt part (per-row prompt lengths differ because of padding)\n    # We decode full then remove prompt prefix via lengths.\n    input_lens = enc[\"input_ids\"].shape[1]\n    # NOTE: this is safe because we padded to same length; model output includes that padded prompt.\n    outs = tokenizer.batch_decode(gen[:, input_lens:], skip_special_tokens=True)\n    return [o.strip() for o in outs]\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:50:41.724809Z","iopub.execute_input":"2025-12-30T20:50:41.724941Z","iopub.status.idle":"2025-12-30T20:50:41.752098Z","shell.execute_reply.started":"2025-12-30T20:50:41.724928Z","shell.execute_reply":"2025-12-30T20:50:41.751735Z"}},"outputs":[],"execution_count":13},{"id":"28e9ed89-8771-46b6-a17d-9cf93afb18c7","cell_type":"markdown","source":"## CELL D — SAFE PYTHON EXECUTOR + PROMPTS","metadata":{}},{"id":"4af419da-bdb0-4478-88a0-5194e4432dcc","cell_type":"code","source":"# ============================================\n# CELL D — PROMPT FACTORY (diverse archetypes)\n# ============================================\n\nSYSTEM_SOLVER = (\n    \"You are a competition mathematician.\\n\"\n    \"Return FINAL_ANSWER: <integer> when you are done.\\n\"\n    \"If you use Python, put code inside a single ```python ... ``` block and ensure it prints ONE integer.\\n\"\n    \"Do NOT include multiple candidate answers.\\n\"\n)\n\n# 4 “archetypes” (diversity by design, not just random seeds)\nARCHETYPES = {\n    \"direct\": (\n        \"Solve succinctly. If the problem is computational, derive a clean formula and compute.\\n\"\n        \"End with FINAL_ANSWER: <integer>.\\n\"\n    ),\n    \"sympy_first\": (\n        \"Use Python + sympy early. Convert the problem to equations / constraints, solve via sympy.\\n\"\n        \"Print only the final integer.\\n\"\n    ),\n    \"bruteforce_constraints\": (\n        \"Prefer brute force / search under constraints if feasible.\\n\"\n        \"Write Python to search intelligently (pruning).\\n\"\n        \"Print only the final integer.\\n\"\n    ),\n    \"number_theory_style\": (\n        \"Use modular arithmetic / invariants / parity / divisibility reasoning.\\n\"\n        \"If any computation is needed, write Python.\\n\"\n        \"Print only the final integer.\\n\"\n    ),\n}\n\ndef build_messages(problem: str, archetype: str, extra_hint: str = \"\") -> List[Dict[str, str]]:\n    if archetype not in ARCHETYPES:\n        archetype = \"direct\"\n    user = (\n        f\"PROBLEM:\\n{problem}\\n\\n\"\n        f\"STYLE:\\n{ARCHETYPES[archetype]}\\n\"\n    )\n    if extra_hint.strip():\n        user += f\"\\nEXTRA_HINT:\\n{extra_hint.strip()}\\n\"\n    return [\n        {\"role\": \"system\", \"content\": SYSTEM_SOLVER},\n        {\"role\": \"user\", \"content\": user},\n    ]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:50:41.752767Z","iopub.execute_input":"2025-12-30T20:50:41.752896Z","iopub.status.idle":"2025-12-30T20:50:41.769790Z","shell.execute_reply.started":"2025-12-30T20:50:41.752884Z","shell.execute_reply":"2025-12-30T20:50:41.769434Z"}},"outputs":[],"execution_count":14},{"id":"8a30367c-007d-433a-9d58-123b088cd390","cell_type":"markdown","source":"## CELL E — ANSWER EXTRACTION (Strict)","metadata":{}},{"id":"4f2685e5-d029-4d25-9acc-613602046363","cell_type":"code","source":"# ============================================================\n# CELL E — PARSING / EXTRACTION / VOTING UTILS\n# ============================================================\n\nimport re\nimport math\nimport time\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Tuple\n\nFINAL_PATTERNS = [\n    re.compile(r\"FINAL_ANSWER\\s*[:=]\\s*(-?\\d+)\", re.IGNORECASE),\n    re.compile(r\"\\\\boxed\\{\\s*(-?\\d+)\\s*\\}\"),\n    re.compile(r\"(?<!\\w)Answer\\s*[:=]\\s*(-?\\d+)\", re.IGNORECASE),\n]\n\nINT_ONLY_LINE = re.compile(r\"^\\s*(-?\\d+)\\s*$\")\n\nCODE_BLOCK_PATTERNS = [\n    re.compile(r\"```python\\s*(.*?)```\", re.DOTALL | re.IGNORECASE),\n    re.compile(r\"```py\\s*(.*?)```\", re.DOTALL | re.IGNORECASE),\n    re.compile(r\"```\\s*(.*?)```\", re.DOTALL),  # fallback (dangerous; use last)\n]\n\ndef strip_think(text: str) -> str:\n    # remove common thinking tags without breaking content\n    text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL | re.IGNORECASE)\n    text = re.sub(r\"<analysis>.*?</analysis>\", \"\", text, flags=re.DOTALL | re.IGNORECASE)\n    return text\n\ndef extract_final_answer(text: str) -> Optional[int]:\n    if not text:\n        return None\n    t = strip_think(text)\n\n    # Priority 1: explicit patterns (take last occurrence)\n    for pat in FINAL_PATTERNS:\n        matches = list(pat.finditer(t))\n        if matches:\n            return int(matches[-1].group(1))\n\n    # Priority 2: last line is integer-only\n    lines = [ln.strip() for ln in t.strip().splitlines() if ln.strip()]\n    if lines:\n        m = INT_ONLY_LINE.match(lines[-1])\n        if m:\n            return int(m.group(1))\n\n    # Priority 3: \"best-effort\" — find integers near the end (last ~300 chars)\n    tail = t[-300:]\n    ints = re.findall(r\"(-?\\d+)\", tail)\n    if ints:\n        try:\n            return int(ints[-1])\n        except Exception:\n            return None\n    return None\n\ndef extract_python_code(text: str) -> Optional[str]:\n    if not text:\n        return None\n    t = strip_think(text)\n\n    best = None\n    for pat in CODE_BLOCK_PATTERNS:\n        blocks = list(pat.finditer(t))\n        if not blocks:\n            continue\n        # prefer the last block (often the final corrected version)\n        best = blocks[-1].group(1).strip()\n        if best:\n            break\n\n    if not best:\n        return None\n\n    # remove leading language hints / stray backticks inside\n    best = best.strip().strip(\"`\").strip()\n\n    # If the model includes \"FINAL_ANSWER:\" inside code as a comment, keep it fine.\n    return best if len(best) >= 8 else None\n\ndef parse_int_from_stdout(stdout: str) -> Optional[int]:\n    if not stdout:\n        return None\n    lines = [ln.strip() for ln in stdout.strip().splitlines() if ln.strip()]\n    if not lines:\n        return None\n\n    # Prefer last integer-only line\n    for ln in reversed(lines):\n        m = INT_ONLY_LINE.match(ln)\n        if m:\n            return int(m.group(1))\n\n    # Otherwise, parse last FINAL_ANSWER / boxed / Answer patterns in stdout\n    return extract_final_answer(stdout)\n\n@dataclass\nclass Candidate:\n    answer: int\n    source: str  # \"direct\", \"boxed\", \"code\", \"verifier\", etc.\n    weight: float\n    meta: Dict[str, Any] = field(default_factory=dict)\n\ndef weighted_vote(cands: List[Candidate]) -> Tuple[Optional[int], Dict[int, float]]:\n    if not cands:\n        return None, {}\n    score: Dict[int, float] = {}\n    best_single: Dict[int, float] = {}\n    for c in cands:\n        score[c.answer] = score.get(c.answer, 0.0) + float(c.weight)\n        best_single[c.answer] = max(best_single.get(c.answer, 0.0), float(c.weight))\n    # rank by total weight, then best single, then answer (stable tie-break)\n    best = sorted(score.items(), key=lambda kv: (kv[1], best_single.get(kv[0], 0.0), -abs(kv[0])), reverse=True)[0][0]\n    return best, score\n\ndef consensus_strength(score_map: Dict[int, float]) -> float:\n    if not score_map:\n        return 0.0\n    vals = sorted(score_map.values(), reverse=True)\n    if len(vals) == 1:\n        return 1.0\n    # ratio top / (top + second)\n    top, second = vals[0], vals[1]\n    denom = top + second\n    return float(top / denom) if denom > 0 else 0.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:50:41.848608Z","iopub.execute_input":"2025-12-30T20:50:41.849091Z","iopub.status.idle":"2025-12-30T20:50:41.858781Z","shell.execute_reply.started":"2025-12-30T20:50:41.849075Z","shell.execute_reply":"2025-12-30T20:50:41.858386Z"}},"outputs":[],"execution_count":15},{"id":"bea3a388-a88b-4668-8198-290410e969af","cell_type":"markdown","source":"## CELL F — AGENTIC SOLVER (PoT + Retry + Voting)","metadata":{}},{"id":"ea1210b5-302f-4d66-a3a6-5382a736aec4","cell_type":"code","source":"# ============================================================\n# CELL F — PYTHON EXECUTOR (SAFE-ISH) + TIME BUDGET\n# ============================================================\n\nimport os\nimport sys\nimport tempfile\nimport subprocess\nfrom pathlib import Path\n\ndef _executor_env() -> Dict[str, str]:\n    env = dict(os.environ)\n    # reduce noisy behaviors\n    env[\"PYTHONHASHSEED\"] = \"0\"\n    env[\"PYTHONWARNINGS\"] = \"ignore\"\n    env[\"OMP_NUM_THREADS\"] = \"1\"\n    env[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n    env[\"MKL_NUM_THREADS\"] = \"1\"\n    env[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n    return env\n\ndef run_python_sandbox(code: str, timeout_sec: int = 10) -> Tuple[bool, str, str]:\n    \"\"\"\n    Runs code in a temp dir using a temp .py file.\n    Returns: (success, stdout, stderr)\n    \"\"\"\n    if not code or not code.strip():\n        return False, \"\", \"Empty code\"\n\n    # Hard guard: prevent obvious shell / file deletion attempts (not perfect)\n    dangerous = [\"os.system(\", \"subprocess.\", \"shutil.rmtree\", \"rm -rf\", \"pip install\", \"wget \", \"curl \"]\n    lowered = code.lower()\n    for d in dangerous:\n        if d in lowered:\n            return False, \"\", f\"Blocked dangerous token: {d}\"\n\n    with tempfile.TemporaryDirectory() as td:\n        td_path = Path(td)\n        script_path = td_path / \"main.py\"\n\n        prelude = (\n            \"import sys\\n\"\n            \"import math\\n\"\n            \"import itertools\\n\"\n            \"import functools\\n\"\n            \"import fractions\\n\"\n            \"from math import *\\n\"\n            \"try:\\n\"\n            \"    import sympy as sp\\n\"\n            \"except Exception:\\n\"\n            \"    sp = None\\n\"\n            \"try:\\n\"\n            \"    import numpy as np\\n\"\n            \"except Exception:\\n\"\n            \"    np = None\\n\"\n            \"sys.setrecursionlimit(10**6)\\n\"\n        )\n\n        # If code doesn't print, try to encourage a single print by wrapping? No: keep pure.\n        script_path.write_text(prelude + \"\\n\\n\" + code + \"\\n\", encoding=\"utf-8\")\n\n        try:\n            proc = subprocess.run(\n                [sys.executable, str(script_path)],\n                cwd=str(td_path),\n                capture_output=True,\n                text=True,\n                timeout=int(timeout_sec),\n                env=_executor_env(),\n            )\n            ok = (proc.returncode == 0)\n            return ok, (proc.stdout or \"\"), (proc.stderr or \"\")\n        except subprocess.TimeoutExpired:\n            return False, \"\", \"Execution timed out\"\n        except Exception as e:\n            return False, \"\", f\"Executor error: {e}\"\n\nclass DeadlineManager:\n    def __init__(self, total_budget_sec: int = 5 * 60 * 60 - 60, hard_stop_margin_sec: int = 20):\n        self.t0 = time.time()\n        self.total_budget_sec = int(total_budget_sec)\n        self.hard_stop_margin_sec = int(hard_stop_margin_sec)\n\n    def elapsed(self) -> float:\n        return time.time() - self.t0\n\n    def remaining(self) -> float:\n        return max(0.0, self.total_budget_sec - self.elapsed())\n\n    def can_continue(self) -> bool:\n        return self.remaining() > self.hard_stop_margin_sec\n\n    def per_problem_budget(self, problems_left: int, min_sec: int = 10, max_sec: int = 180) -> int:\n        if problems_left <= 0:\n            return min_sec\n        rem = self.remaining()\n        # fair-share with cap\n        fair = rem / float(problems_left)\n        return int(max(min_sec, min(max_sec, fair)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:50:41.859541Z","iopub.execute_input":"2025-12-30T20:50:41.859670Z","iopub.status.idle":"2025-12-30T20:50:41.878656Z","shell.execute_reply.started":"2025-12-30T20:50:41.859656Z","shell.execute_reply":"2025-12-30T20:50:41.878309Z"}},"outputs":[],"execution_count":16},{"id":"e0dfb3f6-efdc-46d2-bac7-d1c571ba2fd0","cell_type":"markdown","source":"## CELL G — TELEMETRY LOGGER","metadata":{}},{"id":"e4def158-b8d0-4c60-982e-ae12c05dab2f","cell_type":"code","source":"# ============================================================\n# CELL G — PROMPTS (DIVERSE ARCHETYPES) + GENERATION HELPERS\n# ============================================================\n\n# Requires llm_generate_texts from CELL C.\n\nSYSTEM_SOLVER = (\n    \"You are a math contest solver. \"\n    \"You MUST output the final integer answer in the format: FINAL_ANSWER: <integer> \"\n    \"Do not include any other numbers on the last line.\"\n)\n\nSYSTEM_CODE = (\n    \"You are a math contest solver that uses Python to compute exactly. \"\n    \"Return ONLY a Python code block ```python ... ```.\\n\"\n    \"Rules:\\n\"\n    \"- The code must print ONLY one line: the integer answer.\\n\"\n    \"- No extra prints.\\n\"\n    \"- Use sympy/numpy if helpful.\\n\"\n)\n\nSYSTEM_VERIFY = (\n    \"You are a strict verifier. \"\n    \"Given a problem and a proposed integer answer, check for constraint/boundary traps. \"\n    \"Return either:\\n\"\n    \"VERDICT: OK\\n\"\n    \"or\\n\"\n    \"VERDICT: FAIL\\n\"\n    \"If FAIL and you are confident, also provide: FINAL_ANSWER: <integer> on a new line.\"\n)\n\ndef user_prompt_direct(problem: str, style_hint: str = \"\") -> str:\n    hint = f\"\\nStyle hint: {style_hint}\\n\" if style_hint else \"\\n\"\n    return (\n        f\"Problem:\\n{problem}\\n\"\n        f\"{hint}\"\n        \"Solve carefully. Put the final result as the last line exactly like:\\n\"\n        \"FINAL_ANSWER: 123\\n\"\n    )\n\ndef user_prompt_code(problem: str, approach_hint: str = \"\") -> str:\n    hint = f\"\\nApproach hint: {approach_hint}\\n\" if approach_hint else \"\\n\"\n    return (\n        f\"Problem:\\n{problem}\\n\"\n        f\"{hint}\"\n        \"Write Python that computes the exact integer answer and prints ONLY that integer.\\n\"\n    )\n\ndef user_prompt_verify(problem: str, answer: int) -> str:\n    return (\n        f\"Problem:\\n{problem}\\n\\n\"\n        f\"Proposed answer: {answer}\\n\\n\"\n        \"Check quickly for missed constraints (integer, positivity, bounds, divisibility, off-by-one, etc.).\"\n    )\n\nARCHETYPES_DIRECT = [\n    \"Algebra / simplify first\",\n    \"Number theory / modular constraints\",\n    \"Brute force reasoning (small search if implied)\",\n    \"Invariant / parity / extremal thinking\",\n]\n\nARCHETYPES_CODE = [\n    \"Use sympy solve/simplify; then print integer\",\n    \"Try brute force/search over integer constraints\",\n    \"Translate to equations and compute exactly\",\n    \"If geometry: coordinate bash / compute exactly\",\n]\n\ndef _batch_messages(system: str, user: str, n: int) -> List[List[Dict[str, str]]]:\n    return [[{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}] for _ in range(int(n))]\n\ndef generate_n_direct(problem: str, n: int, temperature: float, max_new_tokens: int) -> List[str]:\n    # diversify by cycling archetype hints\n    msgs = []\n    for i in range(n):\n        hint = ARCHETYPES_DIRECT[i % len(ARCHETYPES_DIRECT)]\n        msgs.append([\n            {\"role\": \"system\", \"content\": SYSTEM_SOLVER},\n            {\"role\": \"user\", \"content\": user_prompt_direct(problem, hint)},\n        ])\n    return llm_generate_texts(\n        msgs,\n        temperature=float(temperature),\n        top_p=float(TOP_P),\n        top_k=int(TOP_K),\n        max_new_tokens=int(max_new_tokens),\n    )\n\ndef generate_n_code(problem: str, n: int, temperature: float, max_new_tokens: int) -> List[str]:\n    msgs = []\n    for i in range(n):\n        hint = ARCHETYPES_CODE[i % len(ARCHETYPES_CODE)]\n        msgs.append([\n            {\"role\": \"system\", \"content\": SYSTEM_CODE},\n            {\"role\": \"user\", \"content\": user_prompt_code(problem, hint)},\n        ])\n    return llm_generate_texts(\n        msgs,\n        temperature=float(temperature),\n        top_p=float(TOP_P),\n        top_k=int(TOP_K),\n        max_new_tokens=int(max_new_tokens),\n    )\n\ndef generate_verify(problem: str, answer: int, temperature: float = 0.0, max_new_tokens: int = 256) -> str:\n    msgs = [[\n        {\"role\": \"system\", \"content\": SYSTEM_VERIFY},\n        {\"role\": \"user\", \"content\": user_prompt_verify(problem, answer)},\n    ]]\n    outs = llm_generate_texts(\n        msgs,\n        temperature=float(temperature),\n        top_p=1.0,\n        top_k=0,\n        max_new_tokens=int(max_new_tokens),\n    )\n    return outs[0] if outs else \"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:50:41.879355Z","iopub.execute_input":"2025-12-30T20:50:41.879477Z","iopub.status.idle":"2025-12-30T20:50:41.907203Z","shell.execute_reply.started":"2025-12-30T20:50:41.879464Z","shell.execute_reply":"2025-12-30T20:50:41.906850Z"}},"outputs":[],"execution_count":17},{"id":"7d4bc6b9-57cd-482e-bb5e-da89189bf993","cell_type":"markdown","source":"## CELL H — LOCAL HARNESS (Reference CSV Regression)","metadata":{}},{"id":"5b10eec2-3181-4b51-8ba0-a7ae37eb00df","cell_type":"code","source":"# ============================================================\n# CELL H — SOLVER (TWO-STAGE + WEIGHTED VOTING + RETRY)\n# ============================================================\n\ndef build_candidates_from_direct(outputs: List[str]) -> List[Candidate]:\n    cands: List[Candidate] = []\n    for t in outputs:\n        ans = extract_final_answer(t)\n        if ans is None:\n            continue\n        # Weight heuristics: explicit FINAL_ANSWER / boxed gets higher\n        w = 0.6\n        if re.search(r\"FINAL_ANSWER\\s*[:=]\\s*-?\\d+\", t, re.IGNORECASE):\n            w = 0.9\n        if re.search(r\"\\\\boxed\\{\\s*-?\\d+\\s*\\}\", t):\n            w = max(w, 0.85)\n        cands.append(Candidate(answer=int(ans), source=\"direct\", weight=float(w), meta={\"raw\": t[-500:]}))\n    return cands\n\ndef build_candidates_from_code(outputs: List[str], exec_timeout_sec: int, max_retries: int) -> List[Candidate]:\n    cands: List[Candidate] = []\n    for raw in outputs:\n        code = extract_python_code(raw)\n        if not code:\n            # fallback: if it still gave FINAL_ANSWER\n            ans = extract_final_answer(raw)\n            if ans is not None:\n                cands.append(Candidate(answer=int(ans), source=\"direct_fallback\", weight=0.55, meta={\"raw\": raw[-500:]}))\n            continue\n\n        cur_code = code\n        last_err = None\n        for attempt in range(max_retries):\n            ok, out, err = run_python_sandbox(cur_code, timeout_sec=exec_timeout_sec)\n            if ok:\n                ans = parse_int_from_stdout(out)\n                if ans is not None:\n                    cands.append(Candidate(\n                        answer=int(ans),\n                        source=\"code_exec\",\n                        weight=2.0,\n                        meta={\"stdout_tail\": out[-300:], \"attempt\": attempt, \"raw\": raw[-400:]},\n                    ))\n                else:\n                    # ran but output not parseable => low weight (likely noisy prints)\n                    a2 = extract_final_answer(out)\n                    if a2 is not None:\n                        cands.append(Candidate(answer=int(a2), source=\"code_exec_loose\", weight=1.0, meta={\"stdout_tail\": out[-300:]}))\n                break\n\n            last_err = (err or \"\").strip()\n            # If retry available, ask model to fix code using error feedback (single-shot in-place)\n            if attempt < max_retries - 1:\n                fix_msgs = [[\n                    {\"role\": \"system\", \"content\": SYSTEM_CODE},\n                    {\"role\": \"user\", \"content\": (\n                        \"Fix the Python code below. It failed with an error.\\n\"\n                        \"Return ONLY a corrected ```python``` code block that prints ONLY the integer answer.\\n\\n\"\n                        f\"ERROR:\\n{last_err}\\n\\n\"\n                        f\"CODE:\\n```python\\n{cur_code}\\n```\"\n                    )},\n                ]]\n                fixed = llm_generate_texts(\n                    fix_msgs,\n                    temperature=0.2,\n                    top_p=float(TOP_P),\n                    top_k=int(TOP_K),\n                    max_new_tokens=1024,\n                )\n                if fixed:\n                    new_code = extract_python_code(fixed[0])\n                    if new_code:\n                        cur_code = new_code\n                        continue\n            # no more retries or couldn't fix\n            break\n\n        if last_err and (not cands or cands[-1].meta.get(\"raw\") != raw[-400:]):\n            # Keep a trace in meta? skip (no candidate)\n            pass\n\n    return cands\n\ndef maybe_verify(problem: str, top_answer: int, budget_ok: bool) -> List[Candidate]:\n    if not budget_ok:\n        return []\n    txt = generate_verify(problem, top_answer, temperature=0.0, max_new_tokens=256)\n    t = strip_think(txt)\n    if \"VERDICT: OK\" in t:\n        return [Candidate(answer=int(top_answer), source=\"verifier_ok\", weight=1.2, meta={\"raw\": t[-400:]})]\n    if \"VERDICT: FAIL\" in t:\n        alt = extract_final_answer(t)\n        if alt is not None and int(alt) != int(top_answer):\n            return [Candidate(answer=int(alt), source=\"verifier_fix\", weight=1.0, meta={\"raw\": t[-400:]})]\n    return []\n\ndef solve_one(problem: str, dm: Optional[DeadlineManager] = None, problems_left: int = 50) -> int:\n    \"\"\"\n    Fresh solver: Two-stage gating + weighted voting + optional verification.\n    \"\"\"\n    if dm is None:\n        dm = DeadlineManager()\n\n    # Dynamic per-problem budget (soft; we still rely on token + executor limits)\n    per_budget = dm.per_problem_budget(problems_left=problems_left, min_sec=12, max_sec=180)\n\n    t_start = time.time()\n    cands: List[Candidate] = []\n\n    # -----------------------\n    # STAGE A: cheap direct\n    # -----------------------\n    K1 = max(2, min(K_BASE, 4))\n    direct_outs = generate_n_direct(problem, n=K1, temperature=float(TEMPERATURE_BASE), max_new_tokens=min(768, int(MAX_NEW_TOKENS)))\n    cands += build_candidates_from_direct(direct_outs)\n\n    best, score_map = weighted_vote(cands)\n    strength = consensus_strength(score_map)\n\n    # If strong enough, verify quickly then return\n    if best is not None and strength >= 0.72:\n        cands += maybe_verify(problem, best, budget_ok=(dm.can_continue() and (time.time() - t_start) < per_budget))\n        best2, _ = weighted_vote(cands)\n        return int(best2 if best2 is not None else best)\n\n    # -----------------------\n    # STAGE B: code (TIR)\n    # -----------------------\n    # Spend time only if within per_budget\n    if (time.time() - t_start) < per_budget and dm.can_continue():\n        # Fewer but heavier attempts; code stage weights dominate when successful.\n        K2 = max(1, min(K_BASE, 3))\n        code_outs = generate_n_code(problem, n=K2, temperature=0.2, max_new_tokens=int(MAX_NEW_TOKENS))\n        cands += build_candidates_from_code(code_outs, exec_timeout_sec=int(EXEC_TIMEOUT_SEC), max_retries=int(AGENT_MAX_RETRIES))\n\n    best, score_map = weighted_vote(cands)\n    strength = consensus_strength(score_map)\n\n    # If still weak, expand direct a bit (diverse) within budget\n    if best is not None and strength < 0.68 and (time.time() - t_start) < per_budget and dm.can_continue():\n        K3 = max(2, min(K_BASE, 6))\n        direct_outs2 = generate_n_direct(problem, n=K3, temperature=min(0.9, float(TEMPERATURE_BASE) + 0.2), max_new_tokens=min(1024, int(MAX_NEW_TOKENS)))\n        cands += build_candidates_from_direct(direct_outs2)\n        best, score_map = weighted_vote(cands)\n        strength = consensus_strength(score_map)\n\n    # Optional verify if we have *any* answer and budget allows\n    if best is not None and (time.time() - t_start) < per_budget and dm.can_continue():\n        cands += maybe_verify(problem, best, budget_ok=True)\n        best2, _ = weighted_vote(cands)\n        return int(best2 if best2 is not None else best)\n\n    # Fallback: if nothing parseable, return 0\n    if best is None:\n        return 0\n    return int(best)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:50:41.907691Z","iopub.execute_input":"2025-12-30T20:50:41.907802Z","iopub.status.idle":"2025-12-30T20:50:41.926541Z","shell.execute_reply.started":"2025-12-30T20:50:41.907789Z","shell.execute_reply":"2025-12-30T20:50:41.926184Z"}},"outputs":[],"execution_count":18},{"id":"b6e86ef0-2988-4c4c-ac0b-44bc6afff24a","cell_type":"markdown","source":"## CELL I — SUBMISSION GLUE (Kaggle Evaluation API)","metadata":{}},{"id":"1865c1bb-fb0e-4f3e-be36-12324bada8f7","cell_type":"code","source":"# ============================================================\n# CELL I — TOOLING: PARSING / SAFE EXEC / VOTING / BUDGET\n# ============================================================\n\nimport re, sys, os, time, math, tempfile, subprocess\nfrom collections import Counter, defaultdict\n\nFINAL_ANSWER_RE = re.compile(r\"FINAL_ANSWER\\s*:\\s*([-+]?\\d+)\", re.IGNORECASE)\nBOXED_RE = re.compile(r\"\\\\boxed\\{([^}]*)\\}\")\nINT_RE = re.compile(r\"[-+]?\\d+\")\n\ndef _strip_think(text: str) -> str:\n    # remove <think>...</think> if present\n    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL | re.IGNORECASE)\n\ndef extract_final_answer_int(text: str):\n    if not text:\n        return None\n    m = FINAL_ANSWER_RE.search(text)\n    if m:\n        try:\n            return int(m.group(1))\n        except:\n            return None\n    return None\n\ndef extract_boxed_int(text: str):\n    if not text:\n        return None\n    m = BOXED_RE.search(text)\n    if not m:\n        return None\n    inside = m.group(1)\n    ints = INT_RE.findall(inside)\n    if not ints:\n        return None\n    try:\n        return int(ints[-1])\n    except:\n        return None\n\ndef extract_last_int(text: str):\n    if not text:\n        return None\n    ints = INT_RE.findall(text)\n    if not ints:\n        return None\n    try:\n        return int(ints[-1])\n    except:\n        return None\n\ndef extract_python_blocks(text: str):\n    \"\"\"\n    Return list of code blocks in order. Handles ```python ... ``` and ``` ... ```.\n    \"\"\"\n    if not text:\n        return []\n    blocks = []\n    # Prefer python fenced blocks\n    for m in re.finditer(r\"```python\\s*(.*?)```\", text, flags=re.DOTALL | re.IGNORECASE):\n        blocks.append(m.group(1).strip())\n    # Fallback generic fenced blocks if no python blocks\n    if not blocks:\n        for m in re.finditer(r\"```\\s*(.*?)```\", text, flags=re.DOTALL):\n            blocks.append(m.group(1).strip())\n    return [b for b in blocks if b]\n\n_FORBIDDEN_PATTERNS = [\n    r\"\\bimport\\s+os\\b\", r\"\\bimport\\s+sys\\b\", r\"\\bimport\\s+subprocess\\b\",\n    r\"\\bfrom\\s+os\\b\", r\"\\bfrom\\s+sys\\b\", r\"\\bfrom\\s+subprocess\\b\",\n    r\"\\bopen\\s*\\(\", r\"\\beval\\s*\\(\", r\"\\bexec\\s*\\(\",\n    r\"\\b__import__\\b\", r\"\\bpathlib\\b\", r\"\\bshutil\\b\",\n    r\"\\bsocket\\b\", r\"\\brequests\\b\", r\"\\burllib\\b\"\n]\n\ndef sanitize_python(code: str) -> str:\n    \"\"\"\n    Soft-sanitize: comment out forbidden lines, keep the rest.\n    (We don't hard reject because LLM hay lỡ import os.)\n    \"\"\"\n    if not code:\n        return code\n    out_lines = []\n    for line in code.splitlines():\n        s = line.strip()\n        bad = any(re.search(p, s) for p in _FORBIDDEN_PATTERNS)\n        if bad:\n            out_lines.append(\"# [blocked] \" + line)\n        else:\n            out_lines.append(line)\n    return \"\\n\".join(out_lines)\n\ndef execute_python_sandbox(code: str, timeout_sec: int = 10):\n    \"\"\"\n    Run code in a subprocess, capture stdout/stderr, enforce timeout.\n    Return: (ok: bool, stdout: str, stderr: str)\n    \"\"\"\n    if not code or not code.strip():\n        return (False, \"\", \"empty_code\")\n\n    code = sanitize_python(code)\n\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n        f.write(code)\n        tmp_path = f.name\n\n    try:\n        r = subprocess.run(\n            [sys.executable, tmp_path],\n            capture_output=True,\n            text=True,\n            timeout=timeout_sec\n        )\n        ok = (r.returncode == 0)\n        return (ok, (r.stdout or \"\").strip(), (r.stderr or \"\").strip())\n    except subprocess.TimeoutExpired:\n        return (False, \"\", \"timeout\")\n    except Exception as e:\n        return (False, \"\", f\"exec_error: {e}\")\n    finally:\n        try:\n            os.remove(tmp_path)\n        except:\n            pass\n\ndef parse_int_from_stdout(stdout: str):\n    \"\"\"\n    Contract: we take LAST non-empty line, then extract last int on that line.\n    If still none, extract last int anywhere in stdout.\n    \"\"\"\n    if not stdout:\n        return None\n    lines = [ln.strip() for ln in stdout.splitlines() if ln.strip()]\n    if lines:\n        v = extract_last_int(lines[-1])\n        if v is not None:\n            return v\n    return extract_last_int(stdout)\n\ndef weighted_vote(cands):\n    \"\"\"\n    cands: list of dicts:\n      { \"answer\": int|None, \"weight\": float, \"meta\": ... }\n    Return best_answer or None.\n    \"\"\"\n    score = defaultdict(float)\n    for c in cands:\n        a = c.get(\"answer\", None)\n        w = float(c.get(\"weight\", 0.0))\n        if a is None:\n            continue\n        score[a] += w\n    if not score:\n        return None, {}\n    best = max(score.items(), key=lambda kv: kv[1])[0]\n    return best, dict(score)\n\ndef make_time_budget(num_problems: int, total_seconds: int = 5*60*60, safety_margin: float = 0.10):\n    \"\"\"\n    Very simple budget per problem for competition run.\n    \"\"\"\n    usable = int(total_seconds * (1.0 - safety_margin))\n    per = max(15, usable // max(1, num_problems))\n    return per\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:50:41.927279Z","iopub.execute_input":"2025-12-30T20:50:41.927415Z","iopub.status.idle":"2025-12-30T20:50:41.948018Z","shell.execute_reply.started":"2025-12-30T20:50:41.927402Z","shell.execute_reply":"2025-12-30T20:50:41.947645Z"}},"outputs":[],"execution_count":19},{"id":"378a80db-358a-4cf1-bce7-914ebef247f1","cell_type":"markdown","source":"## CELL J — SELF TESTS","metadata":{}},{"id":"e6a45bf5-b843-43bf-9baa-ee51a774758f","cell_type":"code","source":"# ============================================================\n# CELL J — AGENTIC SOLVER (8B-friendly) — NEW PIPELINE\n# ============================================================\n\ndef _truncate_problem(text: str, max_chars: int = 12000) -> str:\n    text = text or \"\"\n    text = text.strip()\n    if len(text) <= max_chars:\n        return text\n    # keep end part too (many problems place constraints late)\n    head = text[: int(max_chars*0.70)]\n    tail = text[-int(max_chars*0.30):]\n    return head + \"\\n...\\n\" + tail\n\ndef build_prompt_fast(problem: str) -> str:\n    # SINGLE LINE contract => parser won't catch random \"2, 3, 20...\"\n    return (\n        \"You are solving a math contest problem.\\n\"\n        \"Return ONLY one line in this exact format:\\n\"\n        \"FINAL_ANSWER: <integer>\\n\"\n        \"No other text.\\n\\n\"\n        f\"PROBLEM:\\n{problem}\\n\"\n    )\n\ndef build_prompt_tir(problem: str, feedback: str = \"\") -> str:\n    return (\n        \"You are an expert mathematician using Python as a calculator.\\n\"\n        \"You MUST solve by writing Python code.\\n\"\n        \"Rules:\\n\"\n        \"1) Provide exactly one python code block fenced as ```python ... ```.\\n\"\n        \"2) The code MUST print ONLY the integer answer (one line, no debug).\\n\"\n        \"3) Avoid file/network/system operations.\\n\"\n        \"4) After the code block, output exactly one line: FINAL_ANSWER: <integer>\\n\"\n        \"   (the integer must match what your code prints)\\n\"\n        f\"{('FEEDBACK_FROM_PREVIOUS_ATTEMPT:\\\\n' + feedback + '\\\\n') if feedback else ''}\\n\"\n        f\"PROBLEM:\\n{problem}\\n\"\n    )\n\nclass AgenticSolver8B:\n    def __init__(\n        self,\n        k_base: int = 6,\n        temp: float = 0.6,\n        top_p: float = 0.95,\n        top_k: int = 50,\n        max_new_tokens: int = 2048,\n        max_retries: int = 3,\n        exec_timeout_sec: int = 10,\n    ):\n        self.k_base = int(k_base)\n        self.temp = float(temp)\n        self.top_p = float(top_p)\n        self.top_k = int(top_k)\n        self.max_new_tokens = int(max_new_tokens)\n        self.max_retries = int(max_retries)\n        self.exec_timeout_sec = int(exec_timeout_sec)\n\n    def solve_one(self, problem_text: str, budget_sec: int = 120):\n        problem = _truncate_problem(problem_text)\n\n        candidates = []\n\n        # ---------- Stage A: FAST DIRECT (cheap) ----------\n        # Use small K early; if consensus -> done\n        k_fast = min(2, self.k_base)\n        fast_prompt = build_prompt_fast(problem)\n        fast_prompts = [fast_prompt] * max(1, k_fast)\n\n        fast_texts = llm_generate_texts(\n            fast_prompts,\n            max_new_tokens=min(256, self.max_new_tokens),\n            temperature=max(0.2, min(self.temp, 0.7)),\n            top_p=self.top_p,\n            top_k=self.top_k,\n        )\n\n        for t in fast_texts:\n            t = _strip_think(t)\n            a = extract_final_answer_int(t)\n            if a is None:\n                a = extract_boxed_int(t)\n            if a is None:\n                a = extract_last_int(t)\n            candidates.append({\"answer\": a, \"weight\": 0.8, \"meta\": {\"stage\": \"fast\"}})\n\n        best_a, scoremap = weighted_vote(candidates)\n        if best_a is not None:\n            # if strong consensus among fast answers, accept\n            counts = Counter([c[\"answer\"] for c in candidates if c[\"answer\"] is not None])\n            if counts and counts[best_a] >= 2:\n                return int(best_a), {\"stage\": \"fast_consensus\", \"scoremap\": scoremap}\n\n        # ---------- Stage B: TIR (python) with retries ----------\n        feedback = \"\"\n        start = time.time()\n\n        for attempt in range(self.max_retries):\n            # Budget-aware: if running out, reduce K\n            elapsed = time.time() - start\n            remaining = max(5.0, budget_sec - elapsed)\n            if remaining < 25:\n                k = 2\n            else:\n                k = self.k_base\n\n            tir_prompt = build_prompt_tir(problem, feedback=feedback)\n            tir_prompts = [tir_prompt] * max(1, k)\n\n            texts = llm_generate_texts(\n                tir_prompts,\n                max_new_tokens=self.max_new_tokens,\n                temperature=self.temp,\n                top_p=self.top_p,\n                top_k=self.top_k,\n            )\n\n            new_errs = []\n            for t in texts:\n                raw = _strip_think(t)\n\n                # 1) Try python execution first (highest weight)\n                blocks = extract_python_blocks(raw)\n                exec_ok = False\n                exec_ans = None\n                if blocks:\n                    ok, out, err = execute_python_sandbox(blocks[0], timeout_sec=self.exec_timeout_sec)\n                    exec_ok = ok\n                    if ok:\n                        exec_ans = parse_int_from_stdout(out)\n                        if exec_ans is not None:\n                            candidates.append({\"answer\": int(exec_ans), \"weight\": 3.5, \"meta\": {\"stage\": \"tir_exec_ok\"}})\n                        else:\n                            candidates.append({\"answer\": None, \"weight\": 0.0, \"meta\": {\"stage\": \"tir_exec_no_int\", \"out\": out[:200]}})\n                            new_errs.append(\"python_ran_but_no_integer_output\")\n                    else:\n                        candidates.append({\"answer\": None, \"weight\": 0.0, \"meta\": {\"stage\": \"tir_exec_fail\", \"err\": err[:200]}})\n                        new_errs.append(err[:200] if err else \"python_exec_failed\")\n\n                # 2) Fallback parse from contract line\n                if exec_ans is None:\n                    a = extract_final_answer_int(raw)\n                    if a is None:\n                        a = extract_boxed_int(raw)\n                    if a is None:\n                        a = extract_last_int(raw)\n                    candidates.append({\"answer\": a, \"weight\": 0.6, \"meta\": {\"stage\": \"tir_text_fallback\"}})\n\n            best, scoremap = weighted_vote(candidates)\n            if best is not None:\n                # if top score is comfortably above others, accept\n                sorted_scores = sorted(scoremap.items(), key=lambda kv: kv[1], reverse=True)\n                if len(sorted_scores) == 1 or sorted_scores[0][1] >= 1.5 * sorted_scores[1][1]:\n                    return int(best), {\"stage\": f\"tir_vote_attempt_{attempt}\", \"scoremap\": scoremap}\n\n            feedback = \" ; \".join(new_errs[:3])  # keep short\n\n        # Last fallback\n        best, scoremap = weighted_vote(candidates)\n        if best is not None:\n            return int(best), {\"stage\": \"final_vote_fallback\", \"scoremap\": scoremap}\n        return 0, {\"stage\": \"fail_all\"}\n\n# Instantiate solver (uses globals from CELL A: K_BASE, TEMPERATURE_BASE, TOP_P, TOP_K, MAX_NEW_TOKENS, AGENT_MAX_RETRIES, EXEC_TIMEOUT_SEC)\nsolver = AgenticSolver8B(\n    k_base=K_BASE,\n    temp=TEMPERATURE_BASE,\n    top_p=TOP_P,\n    top_k=TOP_K,\n    max_new_tokens=MAX_NEW_TOKENS,\n    max_retries=AGENT_MAX_RETRIES,\n    exec_timeout_sec=EXEC_TIMEOUT_SEC,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:50:41.998840Z","iopub.execute_input":"2025-12-30T20:50:41.998984Z","iopub.status.idle":"2025-12-30T20:50:42.009271Z","shell.execute_reply.started":"2025-12-30T20:50:41.998972Z","shell.execute_reply":"2025-12-30T20:50:42.008918Z"}},"outputs":[],"execution_count":20},{"id":"78e69cdc-573b-4c77-bea8-d66c5b8b258d","cell_type":"markdown","source":"## CELL K — MAIN EXECUTION","metadata":{}},{"id":"1e2dc222-b8eb-4444-a5a5-7ab43b9caa86","cell_type":"code","source":"# ================================\n# CELL K — SUBMISSION GLUE (Kaggle Evaluation API)\n# ================================\n\nimport os\nimport sys\nimport pandas as pd\nfrom typing import Union\n\n# Make kaggle_evaluation importable (robust across notebook/competition)\nfor p in [\"/kaggle/input/kaggle-evaluation\", \"/kaggle/input\", \".\", \"..\"]:\n    if os.path.exists(os.path.join(p, \"kaggle_evaluation\")):\n        sys.path.insert(0, p)\n        break\n\ndef predict(test_input: Union[pd.DataFrame, dict, pd.Series]) -> pd.DataFrame:\n    \"\"\"\n    Kaggle prediction endpoint.\n    Input must contain: id, problem\n    Output must contain: id, answer\n    \"\"\"\n    if isinstance(test_input, dict):\n        test_df = pd.DataFrame([test_input])\n    elif isinstance(test_input, pd.Series):\n        test_df = pd.DataFrame([test_input.to_dict()])\n    elif isinstance(test_input, pd.DataFrame):\n        test_df = test_input\n    else:\n        raise ValueError(f\"predict() expects DataFrame/dict/Series, got {type(test_input)}\")\n\n    required = {\"id\", \"problem\"}\n    missing = required - set(test_df.columns)\n    if missing:\n        raise ValueError(f\"Input missing required columns: {missing}\")\n\n    cfg = globals().get(\"GLOBAL_CONFIG\", {}) or {}\n\n    rows = []\n    for _, r in test_df.iterrows():\n        pid = str(r[\"id\"])\n        prob = str(r[\"problem\"])\n\n        # solve_problem + log_telemetry must be defined in earlier cells (A–J)\n        ans, telemetry = solve_problem(pid, prob, config=cfg)\n        try:\n            log_telemetry(telemetry)\n        except Exception:\n            pass\n\n        rows.append({\"id\": pid, \"answer\": int(ans)})\n\n    return pd.DataFrame(rows)\n\ndef setup_and_serve():\n    \"\"\"\n    Start Kaggle inference server.\n    - Competition rerun: serve()\n    - Local run: try run_local_gateway() if available\n    \"\"\"\n    try:\n        from kaggle_evaluation.aimo_3_inference_server import AIMO3InferenceServer\n\n        # Warm load model once (must exist in earlier cells)\n        load_model()\n\n        server = AIMO3InferenceServer(predict)\n\n        if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\") == \"1\":\n            print(\"Starting inference server (competition mode)...\")\n            server.serve()\n        else:\n            print(\"Attempting local gateway (debug mode)...\")\n            if hasattr(server, \"run_local_gateway\"):\n                try:\n                    server.run_local_gateway()\n                except Exception as e:\n                    print(f\"run_local_gateway failed: {e}\")\n                    print(\"Falling back to serve()...\")\n                    server.serve()\n            else:\n                print(\"run_local_gateway not available, using serve()...\")\n                server.serve()\n\n    except Exception as e:\n        print(f\"Could not start kaggle_evaluation server: {e}\")\n        print(\"If you're in local_ref/debug mode, this is OK.\")\n\nprint(\"Submission glue initialized.\")\nprint(\"Call setup_and_serve() to start the Kaggle evaluation server.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:50:42.009898Z","iopub.execute_input":"2025-12-30T20:50:42.010019Z","iopub.status.idle":"2025-12-30T20:50:42.033916Z","shell.execute_reply.started":"2025-12-30T20:50:42.010007Z","shell.execute_reply":"2025-12-30T20:50:42.033556Z"}},"outputs":[{"name":"stdout","text":"Submission glue initialized.\nCall setup_and_serve() to start the Kaggle evaluation server.\n","output_type":"stream"}],"execution_count":21}]}