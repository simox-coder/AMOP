{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":363148,"sourceType":"modelInstanceVersion","modelInstanceId":301526,"modelId":322000},{"sourceId":499291,"sourceType":"modelInstanceVersion","modelInstanceId":396608,"modelId":322000},{"sourceId":499313,"sourceType":"modelInstanceVersion","modelInstanceId":396626,"modelId":322000},{"sourceId":510391,"sourceType":"modelInstanceVersion","modelInstanceId":404485,"modelId":422384}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"1ddd52b7-8fb4-4dd6-a5b9-ea937a2d028f","cell_type":"code","source":"# ============================================================\n# AIMO3 CLEAN SUBMISSION — Qwen-3 30B-A3B Thinking (H100)\n# - Setup / warm libs (optional) + cache model files into OS page cache\n# - vLLM (Python API) + chat-template prompts\n# - TIR with <tool:python> blocks\n# - Jupyter kernel pool (stateful) + wall-clock timeout + interrupt\n# - Dynamic time manager\n# - Upgraded answer selection: weighted vote + verify-on-uncertainty + deterministic selector\n#\n# NOTE:\n# - Kaggle AIMO3 predict signature uses pl.Series and returns pl.DataFrame/pd.DataFrame\n# ============================================================\n\nfrom __future__ import annotations\n\nimport os\nimport re\nimport time\nimport math\nimport json\nimport queue\nimport threading\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom contextlib import contextmanager\n\n# =========================\n# CELL 1/13 — CONFIG\n# =========================\nfrom __future__ import annotations\n\nimport os, re, time, queue, threading\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple\nfrom contextlib import contextmanager\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nMODEL_PATH = \"/kaggle/input/qwen-3/transformers/30b-a3b-thinking-2507-fp8/1\"\nSEED = int(os.getenv(\"SEED\", \"42\"))\n\n# Warm/cache\nWARMUP_USR_LIB = os.getenv(\"WARMUP_USR_LIB\", \"0\") == \"1\"\nCACHE_MODEL_FILES = os.getenv(\"CACHE_MODEL_FILES\", \"1\") == \"1\"\nCACHE_CHUNK_MB = int(os.getenv(\"CACHE_CHUNK_MB\", \"512\"))\nCACHE_WORKERS = int(os.getenv(\"CACHE_WORKERS\", \"8\"))\nCACHE_EXTS = (\".safetensors\", \".bin\", \".pt\")\n\n# Time (5h - 5m default)\nHARD_WALL_SECONDS = int(os.getenv(\"HARD_WALL_SECONDS\", str(5 * 60 * 60 - 5 * 60)))\nTOTAL_QUESTIONS = int(os.getenv(\"TOTAL_QUESTIONS\", \"110\"))   # override if needed\nMIN_BUDGET_S = float(os.getenv(\"MIN_BUDGET_S\", \"12\"))\nMAX_BUDGET_S = float(os.getenv(\"MAX_BUDGET_S\", \"420\"))\n\n# Tool loop\nKERNEL_POOL_SIZE = int(os.getenv(\"KERNEL_POOL_SIZE\", \"8\"))\nTOOL_TIMEOUT_S = float(os.getenv(\"TOOL_TIMEOUT_S\", \"4.0\"))\nMAX_TURNS = int(os.getenv(\"MAX_TURNS\", \"8\"))\n\n# Adaptive sampling\nSTAGE1_BATCH = int(os.getenv(\"STAGE1_BATCH\", \"4\"))\nSTAGE2_BATCH = int(os.getenv(\"STAGE2_BATCH\", \"4\"))\nCONFIDENT_RATIO = float(os.getenv(\"CONFIDENT_RATIO\", \"0.78\"))\nVERIFY_RATIO = float(os.getenv(\"VERIFY_RATIO\", \"0.66\"))\nVERIFY_TOP_N = int(os.getenv(\"VERIFY_TOP_N\", \"3\"))\n\n# Generation\nMAX_MODEL_LEN = int(os.getenv(\"MAX_MODEL_LEN\", \"16384\"))\nDTYPE = os.getenv(\"DTYPE\", \"bfloat16\")\nGPU_MEM_UTIL = float(os.getenv(\"GPU_MEM_UTIL\", \"0.92\"))\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:52:32.877709Z","iopub.execute_input":"2025-12-31T09:52:32.878137Z","iopub.status.idle":"2025-12-31T09:52:32.886283Z","shell.execute_reply.started":"2025-12-31T09:52:32.878117Z","shell.execute_reply":"2025-12-31T09:52:32.885879Z"}},"outputs":[],"execution_count":1},{"id":"89793e30-e0e4-48c3-902f-5e396f10298e","cell_type":"code","source":"# =========================\n# CELL 2/13 — WARMUP HELPERS\n# =========================\ndef warmup_usr_lib() -> None:\n    import subprocess\n    cmd = \"find /kaggle/usr/lib -type f -print0 | xargs -0 -P 32 -n 500 cat > /dev/null\"\n    subprocess.run(cmd, shell=True, check=False)\n\ndef cache_model(path: str, exts=CACHE_EXTS, num_workers: int = 8, chunk_mb: int = 256) -> None:\n    import os, multiprocessing\n\n    def warmup_file(fpath: str) -> int:\n        chunk = chunk_mb * 1024 * 1024\n        total = 0\n        with open(fpath, \"rb\") as f:\n            while True:\n                b = f.read(chunk)\n                if not b:\n                    break\n                total += len(b)\n        return total\n\n    if not os.path.isdir(path):\n        return\n\n    files = [\n        os.path.join(root, name)\n        for root, _, names in os.walk(path)\n        for name in names\n        if name.endswith(exts)\n    ]\n    if not files:\n        return\n\n    try:\n        cpu = multiprocessing.cpu_count()\n    except Exception:\n        cpu = 4\n    num_workers = max(1, min(num_workers, cpu, 16))\n    files.sort(key=lambda f: os.path.getsize(f), reverse=True)\n\n    t0 = time.time()\n    total = 0\n    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n        futs = [ex.submit(warmup_file, f) for f in files]\n        for fut in as_completed(futs):\n            total += fut.result()\n\n    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n        gb = total / 1024**3\n        print(f\"[cache_model] warmed ~{gb:.2f} GB in {time.time()-t0:.1f}s\")\n\nif WARMUP_USR_LIB:\n    warmup_usr_lib()\n\nif CACHE_MODEL_FILES:\n    cache_model(MODEL_PATH, num_workers=CACHE_WORKERS, chunk_mb=CACHE_CHUNK_MB)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:52:32.887073Z","iopub.execute_input":"2025-12-31T09:52:32.887209Z","iopub.status.idle":"2025-12-31T09:53:37.817230Z","shell.execute_reply.started":"2025-12-31T09:52:32.887194Z","shell.execute_reply":"2025-12-31T09:53:37.816718Z"}},"outputs":[{"name":"stdout","text":"[cache_model] warmed ~29.03 GB in 64.9s\n","output_type":"stream"}],"execution_count":2},{"id":"afcaa544-a8a5-47c2-a339-6207b1caffa8","cell_type":"code","source":"# =========================\n# CELL 3/13 — IMPORTS (FIX vLLM missing: no crash)\n# =========================\nimport numpy as np\nimport pandas as pd\n\ntry:\n    import polars as pl\nexcept Exception:\n    pl = None\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\nfrom jupyter_client import KernelManager\n\nset_seed(SEED)\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:53:37.817863Z","iopub.execute_input":"2025-12-31T09:53:37.818009Z","iopub.status.idle":"2025-12-31T09:53:43.340950Z","shell.execute_reply.started":"2025-12-31T09:53:37.817994Z","shell.execute_reply":"2025-12-31T09:53:43.340208Z"}},"outputs":[{"name":"stderr","text":"2025-12-31 09:53:41.647063: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767174821.662449     635 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767174821.667005     635 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767174821.678832     635 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767174821.678850     635 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767174821.678852     635 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767174821.678853     635 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":3},{"id":"a9f03c2f-c367-4daf-a08d-5b5523fe44b8","cell_type":"code","source":"# =========================\n# CELL 4/13 — PARSERS + UTILS\n# =========================\n_THINK_RE = re.compile(r\"<think>.*?</think>\", re.DOTALL)\n_BOXED_RE = re.compile(r\"\\\\boxed\\{([^}]*)\\}\")\n_TOOL_RE = re.compile(r\"<tool:python>\\s*(.*?)\\s*</tool:python>\", re.DOTALL)\n\ndef strip_think(text: str) -> str:\n    text = _THINK_RE.sub(\"\", text)\n    if \"</think>\" in text:\n        text = text.split(\"</think>\", 1)[-1]\n    return text.strip()\n\ndef parse_boxed_int(text: str) -> Optional[int]:\n    text = strip_think(text)\n    m = _BOXED_RE.search(text)\n    if not m:\n        return None\n    raw = m.group(1).strip()\n    if not re.fullmatch(r\"[+-]?\\d+\", raw):\n        return None\n    v = int(raw)\n    return v if 0 <= v <= 99999 else None\n\ndef parse_tool_code(text: str) -> Optional[str]:\n    text = strip_think(text)\n    m = _TOOL_RE.search(text)\n    return m.group(1).strip() if m else None\n\ndef fallback_last_int(text: str) -> Optional[int]:\n    text = strip_think(text)\n    nums = re.findall(r\"[-+]?\\d+\", text)\n    if not nums:\n        return None\n    try:\n        v = int(nums[-1])\n    except Exception:\n        return None\n    return v if 0 <= v <= 99999 else None\n\ndef mod100000(x: int) -> int:\n    return int(x) % 100000\n\ndef clamp(x: float, lo: float, hi: float) -> float:\n    return float(min(hi, max(lo, x)))\n\ndef trim_history(hist: List[Tuple[str, str]], max_items: int = 10) -> List[Tuple[str, str]]:\n    return hist if len(hist) <= max_items else hist[-max_items:]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:53:43.341710Z","iopub.execute_input":"2025-12-31T09:53:43.342089Z","iopub.status.idle":"2025-12-31T09:53:43.348324Z","shell.execute_reply.started":"2025-12-31T09:53:43.342070Z","shell.execute_reply":"2025-12-31T09:53:43.347928Z"}},"outputs":[],"execution_count":4},{"id":"8c267031-9d65-409e-91b8-8cf16cb17b64","cell_type":"code","source":"# =========================\n# CELL 5/13 — DIFFICULTY ROUTER\n# =========================\n@dataclass(frozen=True)\nclass Plan:\n    tag: str\n    budget_weight: float\n    stage1_max_k: int\n    stage2_max_k: int\n    stage1_max_tokens: int\n    stage2_max_tokens: int\n    temp1: float\n    temp2: float\n    top_p1: float\n    top_p2: float\n\ndef route_problem(problem: str) -> Plan:\n    p = (problem or \"\").lower()\n\n    # Geometry\n    if any(k in p for k in [\"triangle\", \"circle\", \"radius\", \"angle\", \"tangent\", \"perpendicular\", \"circum\", \"inscribed\"]):\n        return Plan(\"GEO\", 1.15, 6, 18, 900, 1700, 0.55, 0.75, 0.92, 0.90)\n\n    # Number theory\n    if any(k in p for k in [\"mod\", \"congruen\", \"prime\", \"gcd\", \"lcm\", \"divis\", \"remainder\", \"coprime\", \"valuation\", \"phi(\"]):\n        return Plan(\"NT\", 1.25, 6, 22, 950, 1900, 0.55, 0.78, 0.92, 0.90)\n\n    # Functional equations\n    if any(k in p for k in [\"f(\", \"functional\", \"for all real\", \"for all integers\", \"for all x\", \"for all n\", \"satisfies\"]):\n        return Plan(\"FUNC\", 1.20, 6, 20, 950, 1900, 0.55, 0.78, 0.92, 0.90)\n\n    # Probability / expected value\n    if any(k in p for k in [\"probability\", \"expected\", \"random\", \"uniform\", \"dice\", \"coin\", \"distribution\"]):\n        return Plan(\"PROB\", 1.10, 6, 18, 900, 1700, 0.55, 0.75, 0.92, 0.90)\n\n    # Combinatorics\n    if any(k in p for k in [\"ways\", \"choose\", \"arrangements\", \"permutation\", \"combination\", \"graph\", \"color\", \"pigeonhole\", \"invariant\"]):\n        return Plan(\"COMB\", 1.15, 6, 20, 900, 1800, 0.55, 0.78, 0.92, 0.90)\n\n    # Default algebra\n    return Plan(\"ALG\", 1.00, 6, 18, 850, 1600, 0.50, 0.72, 0.92, 0.90)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:53:43.349361Z","iopub.execute_input":"2025-12-31T09:53:43.349718Z","iopub.status.idle":"2025-12-31T09:53:43.367376Z","shell.execute_reply.started":"2025-12-31T09:53:43.349701Z","shell.execute_reply":"2025-12-31T09:53:43.366997Z"}},"outputs":[],"execution_count":5},{"id":"f99e8fb4-d121-4a8d-812a-247845667a23","cell_type":"code","source":"# =========================\n# CELL 6/13 — PROMPTS\n# =========================\nSYSTEM_TIR = (\n    \"You are an olympiad math solver.\\n\"\n    \"You MUST follow this protocol:\\n\\n\"\n    \"If you need computation, output exactly:\\n\"\n    \"<tool:python>\\n\"\n    \"# python code\\n\"\n    \"</tool:python>\\n\\n\"\n    \"If you are ready to answer, output exactly ONE line:\\n\"\n    \"\\\\boxed{NONNEGATIVE_INTEGER}\\n\\n\"\n    \"Rules:\\n\"\n    \"- Output NOTHING else outside the tool block.\\n\"\n    \"- Final answer must be an integer in [0, 99999].\\n\"\n    \"- Prefer verifying with python when possible.\\n\"\n)\n\nSYSTEM_VERIFY = (\n    \"You are a strict verifier.\\n\"\n    \"Given a problem and proposed integer answer A, DISPROVE it quickly.\\n\"\n    \"Use python checks when possible:\\n\"\n    \"- parity constraints\\n\"\n    \"- modular constraints (including mod 100000 if relevant)\\n\"\n    \"- substitution / brute force small cases / random tests\\n\\n\"\n    \"Protocol:\\n\"\n    \"- You may output <tool:python>...</tool:python> blocks.\\n\"\n    \"- Then output EXACTLY one final line: PASS or FAIL or UNKNOWN\\n\"\n    \"- No extra text.\\n\"\n)\n\nSYSTEM_SELECT = (\n    \"You are a selector.\\n\"\n    \"Pick the most reliable candidate answer based on evidence.\\n\"\n    \"Output EXACTLY one line: \\\\boxed{NONNEGATIVE_INTEGER}\\n\"\n    \"No extra text.\\n\"\n)\n\nHINTS = [\n    \"Tool-first: explore small cases in python, infer pattern, verify, then output boxed.\",\n    \"Proof-first: derive symbolic structure, then minimal python verification, output boxed.\",\n    \"Number-theory: use modular constraints/parity/gcd; python to test; output boxed.\",\n    \"Comb/Prob: use invariants or counting; python to validate small n; output boxed.\",\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:53:43.367948Z","iopub.execute_input":"2025-12-31T09:53:43.368081Z","iopub.status.idle":"2025-12-31T09:53:43.380795Z","shell.execute_reply.started":"2025-12-31T09:53:43.368067Z","shell.execute_reply":"2025-12-31T09:53:43.380408Z"}},"outputs":[],"execution_count":6},{"id":"bf0cc30c-3186-4f1c-9032-e7212119b5db","cell_type":"code","source":"# =========================\n# CELL 7/13 — PROMPT BUILDER (chat template)\n# =========================\nclass PromptBuilder:\n    def __init__(self, model_path: str):\n        self.tok = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n    def render(self, system: str, user: str, history: List[Tuple[str, str]]) -> str:\n        msgs = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n        for r, c in history:\n            msgs.append({\"role\": r, \"content\": c})\n        try:\n            return self.tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n        except Exception:\n            out = [f\"[SYSTEM]\\n{system}\\n\", f\"[USER]\\n{user}\\n\"]\n            for r, c in history:\n                out.append(f\"[{r.upper()}]\\n{c}\\n\")\n            out.append(\"[ASSISTANT]\\n\")\n            return \"\\n\".join(out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:53:43.381294Z","iopub.execute_input":"2025-12-31T09:53:43.381426Z","iopub.status.idle":"2025-12-31T09:53:43.394274Z","shell.execute_reply.started":"2025-12-31T09:53:43.381412Z","shell.execute_reply":"2025-12-31T09:53:43.393901Z"}},"outputs":[],"execution_count":7},{"id":"7a4fb51f-d303-4559-9ff3-cddb99f44dd2","cell_type":"code","source":"# =========================\n# CELL 8/13 — PYTHON TOOL (KernelPool) [FIXED for Kaggle]\n# - Remove/avoid colab_kernel_launcher arg parsing issues\n# - Force child kernels to run via ipykernel_launcher\n# =========================\nimport sys\nimport queue\nimport threading\nfrom contextlib import contextmanager\nfrom typing import List, Tuple\n\nfrom jupyter_client import KernelManager\n\nclass PythonKernel:\n    def __init__(self) -> None:\n        # Do NOT rely on the notebook's kernelspec (may route via colab_kernel_launcher)\n        self.km = KernelManager()\n        # Force a standard ipykernel process\n        self.km.kernel_cmd = [sys.executable, \"-m\", \"ipykernel_launcher\", \"-f\", \"{connection_file}\"]\n\n        self.kc = None\n        self._lock = threading.Lock()\n\n    def start(self) -> None:\n        self.km.start_kernel()  # <- NO extra_arguments like \"-q\"\n        self.kc = self.km.client()\n        self.kc.start_channels()\n\n        # Wait until kernel is ready (important for stability)\n        try:\n            self.kc.wait_for_ready(timeout=5)\n        except Exception:\n            pass\n\n        # Lightweight prelude (sympy optional but useful)\n        self.execute(\n            \"import math, itertools, random\\n\"\n            \"try:\\n\"\n            \"    import sympy as sp\\n\"\n            \"except Exception:\\n\"\n            \"    sp = None\\n\",\n            timeout_s=2.0\n        )\n\n    def is_alive(self) -> bool:\n        try:\n            return self.km.is_alive()\n        except Exception:\n            return False\n\n    def interrupt(self) -> None:\n        try:\n            self.km.interrupt_kernel()\n        except Exception:\n            pass\n\n    def shutdown(self) -> None:\n        try:\n            if self.kc:\n                self.kc.stop_channels()\n        finally:\n            try:\n                self.km.shutdown_kernel(now=True)\n            except Exception:\n                pass\n\n    def execute(self, code: str, timeout_s: float) -> str:\n        assert self.kc is not None, \"Kernel not started\"\n        with self._lock:\n            msg_id = self.kc.execute(code, store_history=False, allow_stdin=False)\n            start = time.time()\n            out_lines: List[str] = []\n\n            while True:\n                if time.time() - start >= timeout_s:\n                    self.interrupt()\n                    out_lines.append(\"[PYTHON_TIMEOUT]\")\n                    break\n                try:\n                    msg = self.kc.get_iopub_msg(timeout=0.2)\n                except Exception:\n                    continue\n\n                if msg.get(\"parent_header\", {}).get(\"msg_id\") != msg_id:\n                    continue\n\n                msg_type = msg.get(\"msg_type\", \"\")\n                content = msg.get(\"content\", {})\n\n                if msg_type == \"stream\":\n                    t = content.get(\"text\", \"\")\n                    if t:\n                        out_lines.append(t.rstrip(\"\\n\"))\n                elif msg_type in (\"display_data\", \"execute_result\"):\n                    data = content.get(\"data\", {})\n                    if \"text/plain\" in data:\n                        out_lines.append(str(data[\"text/plain\"]))\n                elif msg_type == \"error\":\n                    tb = content.get(\"traceback\", [])\n                    out_lines.append(\"\\n\".join(tb[-3:]) if tb else \"[PYTHON_ERROR]\")\n                elif msg_type == \"status\" and content.get(\"execution_state\") == \"idle\":\n                    break\n\n            text = \"\\n\".join([x for x in out_lines if x])\n            if len(text) > 2000:\n                text = text[:2000] + \"\\n[...TRUNCATED...]\"\n            return text.strip() if text.strip() else \"[WARN] No output. Use print().\"\n\nclass KernelPool:\n    def __init__(self, size: int):\n        self.q: \"queue.Queue[PythonKernel]\" = queue.Queue()\n        self.all: List[PythonKernel] = []\n        size = max(1, int(size))\n\n        for _ in range(size):\n            k = PythonKernel()\n            try:\n                k.start()\n            except Exception:\n                # If a kernel fails to start, shutdown and retry once\n                try:\n                    k.shutdown()\n                except Exception:\n                    pass\n                k = PythonKernel()\n                k.start()\n\n            self.q.put(k)\n            self.all.append(k)\n\n    @contextmanager\n    def acquire(self) -> PythonKernel:\n        k = self.q.get()\n        try:\n            if not k.is_alive():\n                k.shutdown()\n                k = PythonKernel()\n                k.start()\n            yield k\n        finally:\n            self.q.put(k)\n\n    def close(self) -> None:\n        while not self.q.empty():\n            try:\n                self.q.get_nowait()\n            except Exception:\n                break\n        for k in self.all:\n            k.shutdown()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:53:43.394883Z","iopub.execute_input":"2025-12-31T09:53:43.395023Z","iopub.status.idle":"2025-12-31T09:53:43.412896Z","shell.execute_reply.started":"2025-12-31T09:53:43.395007Z","shell.execute_reply":"2025-12-31T09:53:43.412500Z"}},"outputs":[],"execution_count":8},{"id":"e9b8e83a-cd10-4dbe-9c84-a5ca7bf17100","cell_type":"code","source":"# =========================\n# CELL 9/13 — BACKEND (vLLM if available else HF) ✅ robust on Kaggle\n# - vLLM optional\n# - HF uses sdpa by default; only enable flash_attention_2 if flash_attn exists\n# =========================\nimport importlib.util\n\nclass BackendBase:\n    def generate(self, prompts: List[str], *, temperature: float, top_p: float, max_tokens: int) -> List[str]:\n        raise NotImplementedError\n\nclass HFBackend(BackendBase):\n    def __init__(self, model_path: str, max_model_len: int):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n        # Prefer PyTorch SDPA (native, no extra package). Only use FA2 if flash_attn is installed.\n        has_flash_attn = importlib.util.find_spec(\"flash_attn\") is not None\n        attn_impl = \"flash_attention_2\" if has_flash_attn else \"sdpa\"\n\n        base_kwargs = dict(\n            dtype=torch.bfloat16,          # use dtype (torch_dtype is deprecated)\n            device_map=\"auto\",\n            trust_remote_code=True,\n        )\n\n        # Try with chosen attention impl; if anything blows up, retry safely without forcing it.\n        try:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                attn_implementation=attn_impl,\n                **base_kwargs,\n            )\n        except Exception as e:\n            # FA2 often fails when flash_attn isn't present; fallback to default attention.\n            self.model = AutoModelForCausalLM.from_pretrained(model_path, **base_kwargs)\n\n        self.model.eval()\n        self.max_model_len = int(max_model_len)\n\n    @torch.inference_mode()\n    def generate(self, prompts: List[str], *, temperature: float, top_p: float, max_tokens: int) -> List[str]:\n        outs: List[str] = []\n        bs = 8\n        do_sample = temperature > 1e-6\n\n        for i in range(0, len(prompts), bs):\n            batch = prompts[i : i + bs]\n            enc = self.tokenizer(\n                batch,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=self.max_model_len,\n            ).to(self.model.device)\n\n            gen = self.model.generate(\n                **enc,\n                do_sample=do_sample,\n                temperature=float(max(1e-6, temperature)),\n                top_p=float(top_p),\n                max_new_tokens=int(max_tokens),\n                use_cache=True,\n                pad_token_id=self.tokenizer.eos_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n            )\n\n            for j in range(gen.shape[0]):\n                prompt_len = int(enc[\"attention_mask\"][j].sum().item())\n                tail = gen[j, prompt_len:]\n                outs.append(self.tokenizer.decode(tail, skip_special_tokens=True))\n\n        return outs\n\n_backend_name = \"hf\"\n_backend: BackendBase\n\ntry:\n    from vllm import LLM, SamplingParams  # type: ignore\n\n    class VLLMBackend(BackendBase):\n        def __init__(self, model_path: str):\n            self.llm = LLM(\n                model=model_path,\n                dtype=DTYPE,\n                max_model_len=MAX_MODEL_LEN,\n                gpu_memory_utilization=GPU_MEM_UTIL,\n                tensor_parallel_size=1,\n                trust_remote_code=True,\n            )\n\n        def generate(self, prompts: List[str], *, temperature: float, top_p: float, max_tokens: int) -> List[str]:\n            sp = SamplingParams(\n                temperature=float(temperature),\n                top_p=float(top_p),\n                max_tokens=int(max_tokens),\n            )\n            out = self.llm.generate(prompts, sp)\n            return [o.outputs[0].text for o in out]\n\n    _backend = VLLMBackend(MODEL_PATH)\n    _backend_name = \"vllm\"\n\nexcept Exception:\n    _backend = HFBackend(MODEL_PATH, max_model_len=MAX_MODEL_LEN)\n    _backend_name = \"hf\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:53:43.413451Z","iopub.execute_input":"2025-12-31T09:53:43.413630Z","iopub.status.idle":"2025-12-31T09:56:26.122155Z","shell.execute_reply.started":"2025-12-31T09:53:43.413614Z","shell.execute_reply":"2025-12-31T09:56:26.121660Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70749de3c7fb465c9c75d2f6dd235faf"}},"metadata":{}}],"execution_count":9},{"id":"a827edba-525a-4673-8d48-be8ad1ebd703","cell_type":"code","source":"# =========================\n# CELL 10/13 — TIME MANAGER (difficulty-weighted)\n# =========================\nclass TimeManager:\n    def __init__(self, hard_wall_s: int, total_questions: int):\n        self.start = time.time()\n        self.deadline = self.start + int(hard_wall_s)\n        self.total = max(1, int(total_questions))\n        self.done = 0\n\n    def remaining(self) -> float:\n        return max(0.0, self.deadline - time.time())\n\n    def budget(self, weight: float) -> float:\n        rem = self.remaining()\n        left = max(1, self.total - self.done)\n        base = rem / left\n        return clamp(base * float(weight), MIN_BUDGET_S, MAX_BUDGET_S)\n\n    def mark_done(self) -> None:\n        self.done += 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:56:26.122833Z","iopub.execute_input":"2025-12-31T09:56:26.123272Z","iopub.status.idle":"2025-12-31T09:56:26.127244Z","shell.execute_reply.started":"2025-12-31T09:56:26.123255Z","shell.execute_reply":"2025-12-31T09:56:26.126850Z"}},"outputs":[],"execution_count":10},{"id":"2b6e2062-2695-48bb-8a02-bc9381c249bf","cell_type":"code","source":"# =========================\n# CELL 11/13 — CANDIDATES + WEIGHTED VOTE\n# =========================\n@dataclass\nclass Candidate:\n    answer: Optional[int]\n    raw: str\n    tool_calls: int\n    tool_errors: int\n    elapsed: float\n    stage: int\n    verified: Optional[bool] = None  # PASS->True, FAIL->False, UNKNOWN->None\n\ndef cand_weight(c: Candidate) -> float:\n    if c.answer is None:\n        return 0.0\n    w = 1.0\n    if c.tool_calls > 0 and c.tool_errors == 0:\n        w += 0.9\n    w -= 0.75 * c.tool_errors\n    w += max(0.0, 0.35 - 0.015 * c.elapsed)\n    if c.verified is True:\n        w += 2.25\n    if c.verified is False:\n        w -= 2.25\n    return max(0.0, w)\n\ndef weighted_vote(cands: List[Candidate]) -> Tuple[Optional[int], float, Dict[int, float]]:\n    scores: Dict[int, float] = {}\n    total = 0.0\n    for c in cands:\n        if c.answer is None:\n            continue\n        w = cand_weight(c)\n        total += w\n        scores[c.answer] = scores.get(c.answer, 0.0) + w\n    if not scores or total <= 1e-9:\n        return None, 0.0, {}\n    best = max(scores.items(), key=lambda kv: kv[1])[0]\n    ratio = scores[best] / total\n    return best, ratio, scores\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:56:26.127821Z","iopub.execute_input":"2025-12-31T09:56:26.127958Z","iopub.status.idle":"2025-12-31T09:56:26.144796Z","shell.execute_reply.started":"2025-12-31T09:56:26.127943Z","shell.execute_reply":"2025-12-31T09:56:26.144365Z"}},"outputs":[],"execution_count":11},{"id":"70263c63-b6a2-4376-a1f1-22bb61b75e81","cell_type":"code","source":"# =========================\n# CELL 12/13 — TIR ENGINE (Adaptive K) + VERIFIER + SELECTOR\n# =========================\n@dataclass\nclass TIRState:\n    history: List[Tuple[str, str]]\n    hint: str\n    done: bool = False\n    answer: Optional[int] = None\n    tool_calls: int = 0\n    tool_errors: int = 0\n    raw_last: str = \"\"\n\nclass TIRBatchEngine:\n    def __init__(self, backend: BackendBase, pb: PromptBuilder, pool: KernelPool):\n        self.backend = backend\n        self.pb = pb\n        self.pool = pool\n\n    def _turn(self, problem: str, states: List[TIRState], *, temperature: float, top_p: float, max_tokens: int, time_left_s: float) -> None:\n        active = [i for i, s in enumerate(states) if not s.done]\n        if not active:\n            return\n\n        prompts: List[str] = []\n        for i in active:\n            s = states[i]\n            user = f\"{problem}\\n\\nHint: {s.hint}\"\n            prompts.append(self.pb.render(SYSTEM_TIR, user, s.history))\n\n        outs = self.backend.generate(prompts, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n\n        tool_jobs: List[Tuple[int, str]] = []\n        for idx, out in zip(active, outs):\n            out = strip_think(out)\n            st = states[idx]\n            st.raw_last = out\n\n            ans = parse_boxed_int(out)\n            if ans is not None:\n                st.done = True\n                st.answer = ans\n                continue\n\n            code = parse_tool_code(out)\n            if code:\n                st.tool_calls += 1\n                tool_jobs.append((idx, code))\n                continue\n\n            st.history.append((\"assistant\", out[:800]))\n            st.history.append((\"user\", \"Output ONLY either a <tool:python> block OR one line \\\\boxed{integer}.\"))\n            st.history = trim_history(st.history)\n\n        if not tool_jobs:\n            return\n\n        def run_one(job: Tuple[int, str]) -> Tuple[int, str, str]:\n            i, code = job\n            with self.pool.acquire() as k:\n                py_out = k.execute(code, timeout_s=min(TOOL_TIMEOUT_S, max(1.0, time_left_s)))\n            return i, code, py_out\n\n        with ThreadPoolExecutor(max_workers=min(len(tool_jobs), KERNEL_POOL_SIZE)) as ex:\n            futs = [ex.submit(run_one, j) for j in tool_jobs]\n            for fut in as_completed(futs):\n                i, code, py_out = fut.result()\n                st = states[i]\n                if (\"PYTHON_TIMEOUT\" in py_out) or (\"Traceback\" in py_out) or (\"Error\" in py_out):\n                    st.tool_errors += 1\n                st.history.append((\"assistant\", f\"<tool:python>\\n{code}\\n</tool:python>\"))\n                st.history.append((\"user\", f\"Python output:\\n{py_out}\"))\n                st.history = trim_history(st.history)\n\n    def run_progressive(\n        self,\n        problem: str,\n        *,\n        max_k: int,\n        batch_k: int,\n        budget_s: float,\n        stage: int,\n        max_tokens: int,\n        temperature: float,\n        top_p: float,\n        early_stop_ratio: float,\n    ) -> List[Candidate]:\n        t0 = time.time()\n        cands: List[Candidate] = []\n        created = 0\n\n        while created < max_k and (time.time() - t0) < budget_s:\n            add = min(batch_k, max_k - created)\n            states = [\n                TIRState(history=[], hint=HINTS[(created + i) % len(HINTS)])\n                for i in range(add)\n            ]\n            created += add\n\n            for _ in range(MAX_TURNS):\n                if time.time() - t0 >= budget_s:\n                    break\n                self._turn(\n                    problem, states,\n                    temperature=temperature, top_p=top_p,\n                    max_tokens=max_tokens,\n                    time_left_s=budget_s - (time.time() - t0),\n                )\n                if all(s.done for s in states):\n                    break\n\n            elapsed = time.time() - t0\n            for s in states:\n                ans = s.answer\n                if ans is None:\n                    ans = parse_boxed_int(s.raw_last) or fallback_last_int(s.raw_last)\n                cands.append(Candidate(\n                    answer=ans,\n                    raw=s.raw_last,\n                    tool_calls=s.tool_calls,\n                    tool_errors=s.tool_errors,\n                    elapsed=elapsed,\n                    stage=stage,\n                ))\n\n            best, ratio, _ = weighted_vote(cands)\n            if best is not None and ratio >= early_stop_ratio:\n                break\n\n        return cands\n\nclass Verifier:\n    def __init__(self, backend: BackendBase, pb: PromptBuilder, pool: KernelPool):\n        self.backend = backend\n        self.pb = pb\n        self.pool = pool\n\n    def verify(self, problem: str, answer: int, budget_s: float) -> Optional[bool]:\n        t0 = time.time()\n        history: List[Tuple[str, str]] = []\n        user = f\"Problem:\\n{problem}\\n\\nProposed answer A = {answer}\\n\"\n\n        for _ in range(6):\n            if time.time() - t0 >= budget_s:\n                return None\n\n            prompt = self.pb.render(SYSTEM_VERIFY, user, history)\n            out = strip_think(self.backend.generate([prompt], temperature=0.0, top_p=1.0, max_tokens=650)[0])\n\n            code = parse_tool_code(out)\n            if code:\n                with self.pool.acquire() as k:\n                    py_out = k.execute(code, timeout_s=min(TOOL_TIMEOUT_S, max(1.0, budget_s - (time.time() - t0))))\n                history.append((\"assistant\", f\"<tool:python>\\n{code}\\n</tool:python>\"))\n                history.append((\"user\", f\"Python output:\\n{py_out}\"))\n                history = trim_history(history, 10)\n                continue\n\n            last = out.strip().splitlines()[-1].strip().upper() if out.strip() else \"\"\n            if last == \"PASS\":\n                return True\n            if last == \"FAIL\":\n                return False\n            if last == \"UNKNOWN\":\n                return None\n\n            history.append((\"assistant\", out[:800]))\n            history.append((\"user\", \"Return ONLY one final line: PASS or FAIL or UNKNOWN.\"))\n            history = trim_history(history, 10)\n\n        return None\n\nclass Selector:\n    def __init__(self, backend: BackendBase, pb: PromptBuilder):\n        self.backend = backend\n        self.pb = pb\n\n    def select(self, problem: str, scores: Dict[int, float]) -> Optional[int]:\n        if not scores:\n            return None\n        items = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[:8]\n        evidence = \"\\n\".join([f\"- {a}: score={s:.3f}\" for a, s in items])\n        prompt = self.pb.render(SYSTEM_SELECT, f\"Problem:\\n{problem}\\n\\nCandidate scores:\\n{evidence}\\n\", [])\n        out = strip_think(self.backend.generate([prompt], temperature=0.0, top_p=1.0, max_tokens=220)[0])\n        return parse_boxed_int(out) or fallback_last_int(out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:56:26.145334Z","iopub.execute_input":"2025-12-31T09:56:26.145476Z","iopub.status.idle":"2025-12-31T09:56:26.161677Z","shell.execute_reply.started":"2025-12-31T09:56:26.145460Z","shell.execute_reply":"2025-12-31T09:56:26.161272Z"}},"outputs":[],"execution_count":12},{"id":"c0faf565-0b7e-49fd-b2f0-8c017dd87d9b","cell_type":"code","source":"# =========================\n# CELL 13/13 — SOLVER + KAGGLE HOOK\n# =========================\nclass AIMO3Solver:\n    def __init__(self):\n        self.pb = PromptBuilder(MODEL_PATH)\n        self.pool = KernelPool(KERNEL_POOL_SIZE)\n        self.backend = _backend\n        self.tm = TimeManager(HARD_WALL_SECONDS, TOTAL_QUESTIONS)\n        self.engine = TIRBatchEngine(self.backend, self.pb, self.pool)\n        self.verifier = Verifier(self.backend, self.pb, self.pool)\n        self.selector = Selector(self.backend, self.pb)\n\n        if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n            print(f\"[solver] backend = {_backend_name}\")\n\n    def close(self):\n        self.pool.close()\n\n    def solve_problem(self, problem: str) -> int:\n        plan = route_problem(problem)\n        rem = self.tm.remaining()\n        if rem < 5.0:\n            return 0\n\n        budget = min(self.tm.budget(plan.budget_weight), rem)\n\n        # Stage 1 (adaptive K + early stop)\n        c1 = self.engine.run_progressive(\n            problem,\n            max_k=plan.stage1_max_k,\n            batch_k=STAGE1_BATCH,\n            budget_s=0.38 * budget,\n            stage=1,\n            max_tokens=plan.stage1_max_tokens,\n            temperature=plan.temp1,\n            top_p=plan.top_p1,\n            early_stop_ratio=CONFIDENT_RATIO,\n        )\n        best, ratio, scores = weighted_vote(c1)\n        if best is not None and ratio >= CONFIDENT_RATIO:\n            self.tm.mark_done()\n            return mod100000(best)\n\n        # Stage 2 (adaptive K + early stop)\n        c2 = self.engine.run_progressive(\n            problem,\n            max_k=plan.stage2_max_k,\n            batch_k=STAGE2_BATCH,\n            budget_s=0.50 * budget,\n            stage=2,\n            max_tokens=plan.stage2_max_tokens,\n            temperature=plan.temp2,\n            top_p=plan.top_p2,\n            early_stop_ratio=CONFIDENT_RATIO,\n        )\n\n        all_c = c1 + c2\n        best, ratio, scores = weighted_vote(all_c)\n\n        # Repair if nothing parsed\n        if best is None:\n            c3 = self.engine.run_progressive(\n                problem,\n                max_k=1,\n                batch_k=1,\n                budget_s=0.10 * budget,\n                stage=2,\n                max_tokens=420,\n                temperature=0.0,\n                top_p=1.0,\n                early_stop_ratio=1.0,\n            )\n            ans = c3[0].answer if c3 else None\n            self.tm.mark_done()\n            return mod100000(ans if ans is not None else 0)\n\n        # Verifier-on-uncertainty\n        if ratio < VERIFY_RATIO and budget >= 25.0:\n            top = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[:VERIFY_TOP_N]\n            per_verify_budget = 0.10 * budget / max(1, len(top))\n\n            for ans, _ in top:\n                verdict = self.verifier.verify(problem, ans, budget_s=per_verify_budget)\n                for c in all_c:\n                    if c.answer == ans:\n                        c.verified = verdict\n\n            best, ratio, scores = weighted_vote(all_c)\n\n        # Deterministic selector if still not confident\n        final = best\n        if ratio < 0.80 and (0.07 * budget) >= 3.0:\n            sel = self.selector.select(problem, scores)\n            if sel is not None:\n                final = sel\n\n        self.tm.mark_done()\n        return mod100000(final)\n\nsolver = AIMO3Solver()\n\ndef predict(id_: \"pl.Series\", problem: \"pl.Series\"):\n    if pl is not None and isinstance(id_, pl.Series):\n        pid = id_.item(0)\n        prob = problem.item(0)\n        ans = solver.solve_problem(prob)\n        return pl.DataFrame({\"id\": [pid], \"answer\": [ans]})\n    else:\n        pid = id_[0] if hasattr(id_, \"__len__\") else id_\n        prob = problem[0] if hasattr(problem, \"__len__\") else problem\n        ans = solver.solve_problem(prob)\n        return pd.DataFrame({\"id\": [pid], \"answer\": [ans]})\n\nimport kaggle_evaluation.aimo_3_inference_server as aimo3\ninference_server = aimo3.AIMO3InferenceServer(predict)\n\nif os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    inference_server.serve()\nelse:\n    print(\"Dev mode: solver loaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T09:56:26.162241Z","iopub.execute_input":"2025-12-31T09:56:26.162379Z","iopub.status.idle":"2025-12-31T09:56:35.898424Z","shell.execute_reply.started":"2025-12-31T09:56:26.162365Z","shell.execute_reply":"2025-12-31T09:56:35.897956Z"}},"outputs":[{"name":"stderr","text":"0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n","output_type":"stream"},{"name":"stdout","text":"[solver] backend = hf\nDev mode: solver loaded.\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n    ColabKernelApp.launch_instance()\n  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1984, in _run_once\n    handle = self._ready.popleft()\n             ^^^^^^^^^^^^^^^^^^^^^\nIndexError: pop from an empty deque\n","output_type":"stream"}],"execution_count":13}]}